,repo name,star,url,state,readme
0,mrgloom/awesome-semantic-segmentation,8.9k,https://github.com/mrgloom/awesome-semantic-segmentation,"Updated on May 8, 2021","Awesome Semantic Segmentation Networks by architecture Semantic segmentation Instance aware segmentation Weakly-supervised segmentation RNN GANS Graphical Models (CRF, MRF) Datasets: Benchmarks Evaluation code Starter code Annotation Tools: Results: Metrics Losses Other lists Medical image segmentation: Satellite images segmentation Video segmentation Autonomous driving Other Networks by framework (Older list) Papers and Code (Older list) To look at Blog posts, other:      README.md      Awesome Semantic Segmentation Networks by architecture Semantic segmentation  U-Net [https://arxiv.org/pdf/1505.04597.pdf] [2015]  https://github.com/zhixuhao/unet [Keras] https://github.com/jocicmarko/ultrasound-nerve-segmentation [Keras] https://github.com/EdwardTyantov/ultrasound-nerve-segmentation [Keras] https://github.com/ZFTurbo/ZF_UNET_224_Pretrained_Model [Keras] https://github.com/yihui-he/u-net [Keras] https://github.com/jakeret/tf_unet [Tensorflow] https://github.com/divamgupta/image-segmentation-keras [Keras] https://github.com/ZijunDeng/pytorch-semantic-segmentation [PyTorch] https://github.com/akirasosa/mobile-semantic-segmentation [Keras] https://github.com/orobix/retina-unet [Keras] https://github.com/qureai/ultrasound-nerve-segmentation-using-torchnet [Torch] https://github.com/ternaus/TernausNet [PyTorch] https://github.com/qubvel/segmentation_models [Keras] https://github.com/LeeJunHyun/Image_Segmentation#u-net [PyTorch] https://github.com/yassouali/pytorch_segmentation [PyTorch] https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/ [Caffe + Matlab]   SegNet [https://arxiv.org/pdf/1511.00561.pdf] [2016]  https://github.com/alexgkendall/caffe-segnet [Caffe] https://github.com/developmentseed/caffe/tree/segnet-multi-gpu [Caffe] https://github.com/preddy5/segnet [Keras] https://github.com/imlab-uiip/keras-segnet [Keras] https://github.com/andreaazzini/segnet [Tensorflow] https://github.com/fedor-chervinskii/segnet-torch [Torch] https://github.com/0bserver07/Keras-SegNet-Basic [Keras] https://github.com/tkuanlun350/Tensorflow-SegNet [Tensorflow] https://github.com/divamgupta/image-segmentation-keras [Keras] https://github.com/ZijunDeng/pytorch-semantic-segmentation [PyTorch] https://github.com/chainer/chainercv/tree/master/examples/segnet [Chainer] https://github.com/ykamikawa/keras-SegNet [Keras] https://github.com/ykamikawa/tf-keras-SegNet [Keras] https://github.com/yassouali/pytorch_segmentation [PyTorch]   DeepLab [https://arxiv.org/pdf/1606.00915.pdf] [2017]  https://bitbucket.org/deeplab/deeplab-public/ [Caffe] https://bitbucket.org/aquariusjay/deeplab-public-ver2 [Caffe] https://github.com/TheLegendAli/DeepLab-Context [Caffe] https://github.com/msracver/Deformable-ConvNets/tree/master/deeplab [MXNet] https://github.com/DrSleep/tensorflow-deeplab-resnet [Tensorflow] https://github.com/muyang0320/tensorflow-deeplab-resnet-crf [TensorFlow] https://github.com/isht7/pytorch-deeplab-resnet [PyTorch] https://github.com/bermanmaxim/jaccardSegment [PyTorch] https://github.com/martinkersner/train-DeepLab [Caffe] https://github.com/chenxi116/TF-deeplab [Tensorflow] https://github.com/bonlime/keras-deeplab-v3-plus [Keras] https://github.com/tensorflow/models/tree/master/research/deeplab [Tensorflow] https://github.com/speedinghzl/pytorch-segmentation-toolbox [PyTorch] https://github.com/kazuto1011/deeplab-pytorch [PyTorch] https://github.com/youansheng/torchcv [PyTorch] https://github.com/yassouali/pytorch_segmentation [PyTorch] https://github.com/hualin95/Deeplab-v3plus [PyTorch]   FCN [https://arxiv.org/pdf/1605.06211.pdf] [2016]  https://github.com/vlfeat/matconvnet-fcn [MatConvNet] https://github.com/shelhamer/fcn.berkeleyvision.org [Caffe] https://github.com/MarvinTeichmann/tensorflow-fcn [Tensorflow] https://github.com/aurora95/Keras-FCN [Keras] https://github.com/mzaradzki/neuralnets/tree/master/vgg_segmentation_keras [Keras] https://github.com/k3nt0w/FCN_via_keras [Keras] https://github.com/shekkizh/FCN.tensorflow [Tensorflow] https://github.com/seewalker/tf-pixelwise [Tensorflow] https://github.com/divamgupta/image-segmentation-keras [Keras] https://github.com/ZijunDeng/pytorch-semantic-segmentation [PyTorch] https://github.com/wkentaro/pytorch-fcn [PyTorch] https://github.com/wkentaro/fcn [Chainer] https://github.com/apache/incubator-mxnet/tree/master/example/fcn-xs [MxNet] https://github.com/muyang0320/tf-fcn [Tensorflow] https://github.com/ycszen/pytorch-seg [PyTorch] https://github.com/Kaixhin/FCN-semantic-segmentation [PyTorch] https://github.com/petrama/VGGSegmentation [Tensorflow] https://github.com/simonguist/testing-fcn-for-cityscapes [Caffe] https://github.com/hellochick/semantic-segmentation-tensorflow [Tensorflow] https://github.com/pierluigiferrari/fcn8s_tensorflow [Tensorflow] https://github.com/theduynguyen/Keras-FCN [Keras] https://github.com/JihongJu/keras-fcn [Keras] https://github.com/yassouali/pytorch_segmentation [PyTorch]   ENet [https://arxiv.org/pdf/1606.02147.pdf] [2016]  https://github.com/TimoSaemann/ENet [Caffe] https://github.com/e-lab/ENet-training [Torch] https://github.com/PavlosMelissinos/enet-keras [Keras] https://github.com/fregu856/segmentation [Tensorflow] https://github.com/kwotsin/TensorFlow-ENet [Tensorflow] https://github.com/davidtvs/PyTorch-ENet [PyTorch] https://github.com/yassouali/pytorch_segmentation [PyTorch]   LinkNet [https://arxiv.org/pdf/1707.03718.pdf] [2017]  https://github.com/e-lab/LinkNet [Torch] https://github.com/qubvel/segmentation_models [Keras]   DenseNet [https://arxiv.org/pdf/1611.09326.pdf] [2017]  https://github.com/SimJeg/FC-DenseNet [Lasagne] https://github.com/HasnainRaz/FC-DenseNet-TensorFlow [Tensorflow] https://github.com/0bserver07/One-Hundred-Layers-Tiramisu [Keras]   DilatedNet [https://arxiv.org/pdf/1511.07122.pdf] [2016]  https://github.com/nicolov/segmentation_keras [Keras] https://github.com/fyu/dilation [Caffe] https://github.com/fyu/drn#semantic-image-segmentataion [PyTorch] https://github.com/hangzhaomit/semantic-segmentation-pytorch [PyTorch]   PixelNet [https://arxiv.org/pdf/1609.06694.pdf] [2016]  https://github.com/aayushbansal/PixelNet [Caffe]   ICNet [https://arxiv.org/pdf/1704.08545.pdf] [2017]  https://github.com/hszhao/ICNet [Caffe] https://github.com/aitorzip/Keras-ICNet [Keras] https://github.com/hellochick/ICNet-tensorflow [Tensorflow] https://github.com/oandrienko/fast-semantic-segmentation [Tensorflow] https://github.com/supervisely/supervisely/tree/master/plugins/nn/icnet [PyTorch]   ERFNet [http://www.robesafe.uah.es/personal/eduardo.romera/pdfs/Romera17iv.pdf] [?]  https://github.com/Eromera/erfnet [Torch] https://github.com/Eromera/erfnet_pytorch [PyTorch]   RefineNet [https://arxiv.org/pdf/1611.06612.pdf] [2016]  https://github.com/guosheng/refinenet [MatConvNet]   PSPNet [https://arxiv.org/pdf/1612.01105.pdf,https://hszhao.github.io/projects/pspnet/] [2017]  https://github.com/hszhao/PSPNet [Caffe] https://github.com/ZijunDeng/pytorch-semantic-segmentation [PyTorch] https://github.com/mitmul/chainer-pspnet [Chainer] https://github.com/Vladkryvoruchko/PSPNet-Keras-tensorflow [Keras/Tensorflow] https://github.com/pudae/tensorflow-pspnet [Tensorflow] https://github.com/hellochick/PSPNet-tensorflow [Tensorflow] https://github.com/hellochick/semantic-segmentation-tensorflow [Tensorflow] https://github.com/qubvel/segmentation_models [Keras] https://github.com/oandrienko/fast-semantic-segmentation [Tensorflow] https://github.com/speedinghzl/pytorch-segmentation-toolbox [PyTorch] https://github.com/youansheng/torchcv [PyTorch] https://github.com/yassouali/pytorch_segmentation [PyTorch] https://github.com/holyseven/PSPNet-TF-Reproduce [Tensorflow] https://github.com/kazuto1011/pspnet-pytorch [PyTorch]   DeconvNet [https://arxiv.org/pdf/1505.04366.pdf] [2015]  http://cvlab.postech.ac.kr/research/deconvnet/ [Caffe] https://github.com/HyeonwooNoh/DeconvNet [Caffe] https://github.com/fabianbormann/Tensorflow-DeconvNet-Segmentation [Tensorflow]   FRRN [https://arxiv.org/pdf/1611.08323.pdf] [2016]  https://github.com/TobyPDE/FRRN [Lasagne]   GCN [https://arxiv.org/pdf/1703.02719.pdf] [2017]  https://github.com/ZijunDeng/pytorch-semantic-segmentation [PyTorch] https://github.com/ycszen/pytorch-seg [PyTorch] https://github.com/yassouali/pytorch_segmentation [PyTorch]   LRR [https://arxiv.org/pdf/1605.02264.pdf] [2016]  https://github.com/golnazghiasi/LRR [Matconvnet]   DUC, HDC [https://arxiv.org/pdf/1702.08502.pdf] [2017]  https://github.com/ZijunDeng/pytorch-semantic-segmentation [PyTorch] https://github.com/ycszen/pytorch-seg [PyTorch] https://github.com/yassouali/pytorch_segmentation [PyTorch]   MultiNet [https://arxiv.org/pdf/1612.07695.pdf] [2016]  https://github.com/MarvinTeichmann/MultiNet https://github.com/MarvinTeichmann/KittiSeg   Segaware [https://arxiv.org/pdf/1708.04607.pdf] [2017]  https://github.com/aharley/segaware [Caffe]   Semantic Segmentation using Adversarial Networks [https://arxiv.org/pdf/1611.08408.pdf] [2016]  https://github.com/oyam/Semantic-Segmentation-using-Adversarial-Networks [Chainer]   PixelDCN [https://arxiv.org/pdf/1705.06820.pdf] [2017]  https://github.com/HongyangGao/PixelDCN [Tensorflow]   ShuffleSeg [https://arxiv.org/pdf/1803.03816.pdf] [2018]  https://github.com/MSiam/TFSegmentation [TensorFlow]   AdaptSegNet [https://arxiv.org/pdf/1802.10349.pdf] [2018]  https://github.com/wasidennis/AdaptSegNet [PyTorch]   TuSimple-DUC [https://arxiv.org/pdf/1702.08502.pdf] [2018]  https://github.com/TuSimple/TuSimple-DUC [MxNet]   FPN [http://presentations.cocodataset.org/COCO17-Stuff-FAIR.pdf] [2017]  https://github.com/qubvel/segmentation_models [Keras]   R2U-Net [https://arxiv.org/ftp/arxiv/papers/1802/1802.06955.pdf] [2018]  https://github.com/LeeJunHyun/Image_Segmentation#r2u-net [PyTorch]   Attention U-Net [https://arxiv.org/pdf/1804.03999.pdf] [2018]  https://github.com/LeeJunHyun/Image_Segmentation#attention-u-net [PyTorch] https://github.com/ozan-oktay/Attention-Gated-Networks [PyTorch]   DANet [https://arxiv.org/pdf/1809.02983.pdf] [2018]  https://github.com/junfu1115/DANet [PyTorch]   ShelfNet [https://arxiv.org/pdf/1811.11254.pdf] [2018]  https://github.com/juntang-zhuang/ShelfNet [PyTorch]   LadderNet [https://arxiv.org/pdf/1810.07810.pdf] [2018]  https://github.com/juntang-zhuang/LadderNet [PyTorch]   BiSeNet [https://arxiv.org/pdf/1808.00897.pdf] [2018]  https://github.com/ooooverflow/BiSeNet [PyTorch] https://github.com/ycszen/TorchSeg [PyTorch] https://github.com/zllrunning/face-parsing.PyTorch [PyTorch]   ESPNet [https://arxiv.org/pdf/1803.06815.pdf] [2018]  https://github.com/sacmehta/ESPNet [PyTorch]   DFN [https://arxiv.org/pdf/1804.09337.pdf] [2018]  https://github.com/ycszen/TorchSeg [PyTorch]   CCNet [https://arxiv.org/pdf/1811.11721.pdf] [2018]  https://github.com/speedinghzl/CCNet [PyTorch]   DenseASPP [http://openaccess.thecvf.com/content_cvpr_2018/papers/Yang_DenseASPP_for_Semantic_CVPR_2018_paper.pdf] [2018]  https://github.com/youansheng/torchcv [PyTorch]   Fast-SCNN [https://arxiv.org/pdf/1902.04502.pdf] [2019]  https://github.com/DeepVoltaire/Fast-SCNN [PyTorch]   HRNet [https://arxiv.org/pdf/1904.04514.pdf] [2019]  https://github.com/HRNet/HRNet-Semantic-Segmentation [PyTorch]   PSANet [https://hszhao.github.io/papers/eccv18_psanet.pdf] [2018]  https://github.com/hszhao/PSANet [Caffe]   UPSNet [https://arxiv.org/pdf/1901.03784.pdf] [2019]  https://github.com/uber-research/UPSNet [PyTorch]   ConvCRF [https://arxiv.org/pdf/1805.04777.pdf] [2018]  https://github.com/MarvinTeichmann/ConvCRF [PyTorch]   Multi-scale Guided Attention for Medical Image Segmentation [https://arxiv.org/pdf/1906.02849.pdf] [2019]  https://github.com/sinAshish/Multi-Scale-Attention [PyTorch]   DFANet [https://arxiv.org/pdf/1904.02216.pdf] [2019]  https://github.com/huaifeng1993/DFANet [PyTorch]   ExtremeC3Net [https://arxiv.org/pdf/1908.03093.pdf] [2019]  https://github.com/HYOJINPARK/ExtPortraitSeg [PyTorch]   EncNet [https://arxiv.org/pdf/1803.08904.pdf] [2018]  https://github.com/zhanghang1989/PyTorch-Encoding [PyTorch]   Unet++ [https://arxiv.org/pdf/1807.10165.pdf] [2018]  https://github.com/MrGiovanni/UNetPlusPlus [Keras] https://github.com/4uiiurz1/pytorch-nested-unet [PyTorch]   FastFCN [https://arxiv.org/pdf/1903.11816.pdf] [2019]  https://github.com/wuhuikai/FastFCN [PyTorch]   PortraitNet [https://www.yongliangyang.net/docs/mobilePotrait_c&g19.pdf] [2019]  https://github.com/dong-x16/PortraitNet [PyTorch]   GSCNN [https://arxiv.org/pdf/1907.05740.pdf] [2019]  https://github.com/nv-tlabs/gscnn [PyTorch]    Instance aware segmentation  FCIS [https://arxiv.org/pdf/1611.07709.pdf]  https://github.com/msracver/FCIS [MxNet]   MNC [https://arxiv.org/pdf/1512.04412.pdf]  https://github.com/daijifeng001/MNC [Caffe]   DeepMask [https://arxiv.org/pdf/1506.06204.pdf]  https://github.com/facebookresearch/deepmask [Torch]   SharpMask [https://arxiv.org/pdf/1603.08695.pdf]  https://github.com/facebookresearch/deepmask [Torch]   Mask-RCNN [https://arxiv.org/pdf/1703.06870.pdf]  https://github.com/CharlesShang/FastMaskRCNN [Tensorflow] https://github.com/jasjeetIM/Mask-RCNN [Caffe] https://github.com/TuSimple/mx-maskrcnn [MxNet] https://github.com/matterport/Mask_RCNN [Keras] https://github.com/facebookresearch/maskrcnn-benchmark [PyTorch] https://github.com/open-mmlab/mmdetection [PyTorch] https://github.com/ZFTurbo/Keras-Mask-RCNN-for-Open-Images-2019-Instance-Segmentation [Keras]   RIS [https://arxiv.org/pdf/1511.08250.pdf]  https://github.com/bernard24/RIS [Torch]   FastMask [https://arxiv.org/pdf/1612.08843.pdf]  https://github.com/voidrank/FastMask [Caffe]   BlitzNet [https://arxiv.org/pdf/1708.02813.pdf]  https://github.com/dvornikita/blitznet [Tensorflow]   PANet [https://arxiv.org/pdf/1803.01534.pdf] [2018]  https://github.com/ShuLiu1993/PANet [Caffe]   PAN [https://arxiv.org/pdf/1805.10180.pdf] [2018]  https://github.com/JaveyWang/Pyramid-Attention-Networks-pytorch [PyTorch]   TernausNetV2 [https://arxiv.org/pdf/1806.00844.pdf] [2018]  https://github.com/ternaus/TernausNetV2 [PyTorch]   MS R-CNN [https://arxiv.org/pdf/1903.00241.pdf] [2019]  https://github.com/zjhuang22/maskscoring_rcnn [PyTorch]   AdaptIS [https://arxiv.org/pdf/1909.07829.pdf] [2019]  https://github.com/saic-vul/adaptis [MxNet][PyTorch]   Pose2Seg [https://arxiv.org/pdf/1803.10683.pdf] [2019]  https://github.com/liruilong940607/Pose2Seg [PyTorch]   YOLACT [https://arxiv.org/pdf/1904.02689.pdf] [2019]  https://github.com/dbolya/yolact [PyTorch]   CenterMask [https://arxiv.org/pdf/1911.06667.pdf] [2019]  https://github.com/youngwanLEE/CenterMask [PyTorch] https://github.com/youngwanLEE/centermask2 [PyTorch]   InstaBoost [https://arxiv.org/pdf/1908.07801.pdf] [2019]  https://github.com/GothicAi/Instaboost [PyTorch]   SOLO [https://arxiv.org/pdf/1912.04488.pdf] [2019]  https://github.com/WXinlong/SOLO [PyTorch]   SOLOv2 [https://arxiv.org/pdf/2003.10152.pdf] [2020]  https://github.com/WXinlong/SOLO [PyTorch]   D2Det [https://openaccess.thecvf.com/content_CVPR_2020/papers/Cao_D2Det_Towards_High_Quality_Object_Detection_and_Instance_Segmentation_CVPR_2020_paper.pdf] [2020] +https://github.com/JialeCao001/D2Det [PyTorch]  Weakly-supervised segmentation  SEC [https://arxiv.org/pdf/1603.06098.pdf]  https://github.com/kolesman/SEC [Caffe]    RNN  ReNet [https://arxiv.org/pdf/1505.00393.pdf]  https://github.com/fvisin/reseg [Lasagne]   ReSeg [https://arxiv.org/pdf/1511.07053.pdf]  https://github.com/Wizaron/reseg-pytorch [PyTorch] https://github.com/fvisin/reseg [Lasagne]   RIS [https://arxiv.org/pdf/1511.08250.pdf]  https://github.com/bernard24/RIS [Torch]   CRF-RNN [http://www.robots.ox.ac.uk/%7Eszheng/papers/CRFasRNN.pdf]  https://github.com/martinkersner/train-CRF-RNN [Caffe] https://github.com/torrvision/crfasrnn [Caffe] https://github.com/NP-coder/CLPS1520Project [Tensorflow] https://github.com/renmengye/rec-attend-public [Tensorflow] https://github.com/sadeepj/crfasrnn_keras [Keras]    GANS  pix2pix [https://arxiv.org/pdf/1611.07004.pdf] [2018]  https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix [Pytorch] https://github.com/affinelayer/pix2pix-tensorflow [Tensorflow]   pix2pixHD [https://arxiv.org/pdf/1711.11585.pdf] [2018]  https://github.com/NVIDIA/pix2pixHD   Probalistic Unet [https://arxiv.org/pdf/1806.05034.pdf] [2018]  https://github.com/SimonKohl/probabilistic_unet    Graphical Models (CRF, MRF)  https://github.com/cvlab-epfl/densecrf http://vladlen.info/publications/efficient-inference-in-fully-connected-crfs-with-gaussian-edge-potentials/ http://www.philkr.net/home/densecrf http://graphics.stanford.edu/projects/densecrf/ https://github.com/amiltonwong/segmentation/blob/master/segmentation.ipynb https://github.com/jliemansifry/super-simple-semantic-segmentation http://users.cecs.anu.edu.au/~jdomke/JGMT/ https://www.quora.com/How-can-one-train-and-test-conditional-random-field-CRF-in-Python-on-our-own-training-testing-dataset https://github.com/tpeng/python-crfsuite https://github.com/chokkan/crfsuite https://sites.google.com/site/zeppethefake/semantic-segmentation-crf-baseline https://github.com/lucasb-eyer/pydensecrf  Datasets:  Stanford Background Dataset Sift Flow Dataset Barcelona Dataset Microsoft COCO dataset MSRC Dataset LITS Liver Tumor Segmentation Dataset KITTI Pascal Context Data from Games dataset Human parsing dataset Mapillary Vistas Dataset Microsoft AirSim MIT Scene Parsing Benchmark COCO 2017 Stuff Segmentation Challenge ADE20K Dataset INRIA Annotations for Graz-02 Daimler dataset ISBI Challenge: Segmentation of neuronal structures in EM stacks INRIA Annotations for Graz-02 (IG02) Pratheepan Dataset Clothing Co-Parsing (CCP) Dataset ApolloScape UrbanMapper3D RoadDetector Cityscapes CamVid Inria Aerial Image Labeling  Benchmarks  https://github.com/openseg-group/openseg.pytorch [PyTorch] https://github.com/open-mmlab/mmsegmentation [PyTorch] https://github.com/ZijunDeng/pytorch-semantic-segmentation [PyTorch] https://github.com/meetshah1995/pytorch-semseg [PyTorch] https://github.com/GeorgeSeif/Semantic-Segmentation-Suite [Tensorflow] https://github.com/MSiam/TFSegmentation [Tensorflow] https://github.com/CSAILVision/sceneparsing [Caffe+Matlab] https://github.com/BloodAxe/segmentation-networks-benchmark [PyTorch] https://github.com/warmspringwinds/pytorch-segmentation-detection [PyTorch] https://github.com/ycszen/TorchSeg [PyTorch] https://github.com/qubvel/segmentation_models [Keras] https://github.com/qubvel/segmentation_models.pytorch [PyTorch] https://github.com/Tramac/awesome-semantic-segmentation-pytorch [PyTorch] https://github.com/hszhao/semseg [PyTorch] https://github.com/yassouali/pytorch_segmentation [PyTorch] https://github.com/divamgupta/image-segmentation-keras [Keras] https://github.com/CSAILVision/semantic-segmentation-pytorch [PyTorch] https://github.com/thuyngch/Human-Segmentation-PyTorch [PyTorch] https://github.com/PaddlePaddle/PaddleSeg [PaddlePaddle]  Evaluation code  [Cityscapes dataset] https://github.com/phillipi/pix2pix/tree/master/scripts/eval_cityscapes  Starter code  https://github.com/mrgloom/keras-semantic-segmentation-example  Annotation Tools:  https://github.com/AKSHAYUBHAT/ImageSegmentation https://github.com/kyamagu/js-segment-annotator https://github.com/CSAILVision/LabelMeAnnotationTool https://github.com/seanbell/opensurfaces-segmentation-ui https://github.com/lzx1413/labelImgPlus https://github.com/wkentaro/labelme https://github.com/labelbox/labelbox https://github.com/Deep-Magic/COCO-Style-Dataset-Generator-GUI https://github.com/Labelbox/Labelbox https://github.com/opencv/cvat https://github.com/saic-vul/fbrs_interactive_segmentation  Results:  MSRC-21 Cityscapes VOC2012 https://paperswithcode.com/task/semantic-segmentation  Metrics  https://github.com/martinkersner/py_img_seg_eval  Losses  https://github.com/JunMa11/SegLoss http://www.cs.umanitoba.ca/~ywang/papers/isvc16.pdf https://arxiv.org/pdf/1705.08790.pdf https://arxiv.org/pdf/1707.03237.pdf http://www.bmva.org/bmvc/2013/Papers/paper0032/paper0032.pdf  Other lists  https://paperswithcode.com/task/semantic-segmentation https://github.com/tangzhenyu/SemanticSegmentation_DL https://github.com/nightrome/really-awesome-semantic-segmentation https://github.com/JackieZhangdx/InstanceSegmentationList https://github.com/damminhtien/awesome-semantic-segmentation  Medical image segmentation:   DIGITS  https://github.com/NVIDIA/DIGITS/tree/master/examples/medical-imaging    U-Net: Convolutional Networks for Biomedical Image Segmentation  http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/ https://github.com/dmlc/mxnet/issues/1514 https://github.com/orobix/retina-unet https://github.com/fvisin/reseg https://github.com/yulequan/melanoma-recognition http://www.andrewjanowczyk.com/use-case-1-nuclei-segmentation/ https://github.com/junyanz/MCILBoost https://github.com/imlab-uiip/lung-segmentation-2d https://github.com/scottykwok/cervix-roi-segmentation-by-unet https://github.com/WeidiXie/cell_counting_v2 https://github.com/yandexdataschool/YSDA_deeplearning17/blob/master/Seminar6/Seminar%206%20-%20segmentation.ipynb    Cascaded-FCN  https://github.com/IBBM/Cascaded-FCN    Keras  https://github.com/jocicmarko/ultrasound-nerve-segmentation https://github.com/EdwardTyantov/ultrasound-nerve-segmentation https://github.com/intact-project/ild-cnn https://github.com/scottykwok/cervix-roi-segmentation-by-unet https://github.com/lishen/end2end-all-conv    Tensorflow  https://github.com/imatge-upc/liverseg-2017-nipsws https://github.com/DLTK/DLTK/tree/master/examples/applications/MRBrainS13_tissue_segmentation    Using Convolutional Neural Networks (CNN) for Semantic Segmentation of Breast Cancer Lesions (BRCA)  https://github.com/ecobost/cnn4brca    Papers:  https://www2.warwick.ac.uk/fac/sci/dcs/people/research/csrkbb/tmi2016_ks.pdf Sliding window approach  http://people.idsia.ch/~juergen/nips2012.pdf   https://github.com/albarqouni/Deep-Learning-for-Medical-Applications#segmentation    Data:  https://luna16.grand-challenge.org/ https://camelyon16.grand-challenge.org/ https://github.com/beamandrew/medical-data    Satellite images segmentation  https://github.com/mshivaprakash/sat-seg-thesis https://github.com/KGPML/Hyperspectral https://github.com/lopuhin/kaggle-dstl https://github.com/mitmul/ssai https://github.com/mitmul/ssai-cnn https://github.com/azavea/raster-vision https://github.com/nshaud/DeepNetsForEO https://github.com/trailbehind/DeepOSM https://github.com/mapbox/robosat https://github.com/datapink/robosat.pink   Data:  https://github.com/RSIA-LIESMARS-WHU/RSOD-Dataset- SpaceNet[https://spacenetchallenge.github.io/] https://github.com/chrieke/awesome-satellite-imagery-datasets    Video segmentation  https://github.com/shelhamer/clockwork-fcn https://github.com/JingchunCheng/Seg-with-SPN  Autonomous driving  https://github.com/MarvinTeichmann/MultiNet https://github.com/MarvinTeichmann/KittiSeg https://github.com/vxy10/p5_VehicleDetection_Unet [Keras] https://github.com/ndrplz/self-driving-car https://github.com/mvirgo/MLND-Capstone https://github.com/zhujun98/semantic_segmentation/tree/master/fcn8s_road https://github.com/MaybeShewill-CV/lanenet-lane-detection  Other Networks by framework (Older list)   Keras  https://github.com/gakarak/FCN_MSCOCO_Food_Segmentation https://github.com/abbypa/NNProject_DeepMask    TensorFlow  https://github.com/warmspringwinds/tf-image-segmentation    Caffe  https://github.com/xiaolonw/nips14_loc_seg_testonly https://github.com/naibaf7/caffe_neural_tool    torch  https://github.com/erogol/seg-torch https://github.com/phillipi/pix2pix    MXNet  https://github.com/itijyou/ademxapp    Papers and Code (Older list)   Simultaneous detection and segmentation  http://www.eecs.berkeley.edu/Research/Projects/CS/vision/shape/sds/ https://github.com/bharath272/sds_eccv2014    Decoupled Deep Neural Network for Semi-supervised Semantic Segmentation  https://github.com/HyeonwooNoh/DecoupledNet    Learning to Propose Objects  http://vladlen.info/publications/learning-to-propose-objects/ https://github.com/philkr/lpo    Nonparametric Scene Parsing via Label Transfer  http://people.csail.mit.edu/celiu/LabelTransfer/code.html    Other  https://github.com/cvjena/cn24 http://lmb.informatik.uni-freiburg.de/resources/software.php https://github.com/NVIDIA/DIGITS/tree/master/examples/semantic-segmentation http://jamie.shotton.org/work/code.html https://github.com/amueller/textonboost    To look at  https://github.com/fchollet/keras/issues/6538 https://github.com/warmspringwinds/tensorflow_notes https://github.com/kjw0612/awesome-deep-vision#semantic-segmentation https://github.com/desimone/segmentation-models https://github.com/nightrome/really-awesome-semantic-segmentation https://github.com/kjw0612/awesome-deep-vision#semantic-segmentation http://www.it-caesar.com/list-of-contemporary-semantic-segmentation-datasets/ https://github.com/MichaelXin/Awesome-Caffe#23-image-segmentation https://github.com/warmspringwinds/pytorch-segmentation-detection https://github.com/neuropoly/axondeepseg https://github.com/petrochenko-pavel-a/segmentation_training_pipeline  Blog posts, other:  https://handong1587.github.io/deep_learning/2015/10/09/segmentation.html http://www.andrewjanowczyk.com/efficient-pixel-wise-deep-learning-on-large-images/ https://devblogs.nvidia.com/parallelforall/image-segmentation-using-digits-5/ https://github.com/NVIDIA/DIGITS/tree/master/examples/binary-segmentation https://github.com/NVIDIA/DIGITS/tree/master/examples/semantic-segmentation http://blog.qure.ai/notes/semantic-segmentation-deep-learning-review https://medium.com/@barvinograd1/instance-embedding-instance-segmentation-without-proposals-31946a7c53e1 "
1,CSAILVision/semantic-segmentation-pytorch,4.2k,https://github.com/CSAILVision/semantic-segmentation-pytorch,"Updated on Sep 13, 2021","Semantic Segmentation on MIT ADE20K dataset in PyTorch Updates Highlights Syncronized Batch Normalization on PyTorch Dynamic scales of input for training with multiple GPUs State-of-the-Art models Supported models Performance: Environment Quick start: Test on an image using our trained model Training Evaluation Integration with other projects Reference      README.md           Semantic Segmentation on MIT ADE20K dataset in PyTorch This is a PyTorch implementation of semantic segmentation models on MIT ADE20K scene parsing dataset (http://sceneparsing.csail.mit.edu/). ADE20K is the largest open source dataset for semantic segmentation and scene parsing, released by MIT Computer Vision team. Follow the link below to find the repository for our dataset and implementations on Caffe and Torch7: https://github.com/CSAILVision/sceneparsing If you simply want to play with our demo, please try this link: http://scenesegmentation.csail.mit.edu You can upload your own photo and parse it! You can also use this colab notebook playground here to tinker with the code for segmenting an image. All pretrained models can be found at: http://sceneparsing.csail.mit.edu/model/pytorch   [From left to right: Test Image, Ground Truth, Predicted Result] Color encoding of semantic categories can be found here: https://docs.google.com/spreadsheets/d/1se8YEtb2detS7OuPE86fXGyD269pMycAWe2mtKUj2W8/edit?usp=sharing Updates  HRNet model is now supported. We use configuration files to store most options which were in argument parser. The definitions of options are detailed in config/defaults.py. We conform to Pytorch practice in data preprocessing (RGB [0, 1], substract mean, divide std).  Highlights Syncronized Batch Normalization on PyTorch This module computes the mean and standard-deviation across all devices during training. We empirically find that a reasonable large batch size is important for segmentation. We thank Jiayuan Mao for his kind contributions, please refer to Synchronized-BatchNorm-PyTorch for details. The implementation is easy to use as:  It is pure-python, no C++ extra extension libs. It is completely compatible with PyTorch's implementation. Specifically, it uses unbiased variance to update the moving average, and use sqrt(max(var, eps)) instead of sqrt(var + eps). It is efficient, only 20% to 30% slower than UnsyncBN.  Dynamic scales of input for training with multiple GPUs For the task of semantic segmentation, it is good to keep aspect ratio of images during training. So we re-implement the DataParallel module, and make it support distributing data to multiple GPUs in python dict, so that each gpu can process images of different sizes. At the same time, the dataloader also operates differently. Now the batch size of a dataloader always equals to the number of GPUs, each element will be sent to a GPU. It is also compatible with multi-processing. Note that the file index for the multi-processing dataloader is stored on the master process, which is in contradict to our goal that each worker maintains its own file list. So we use a trick that although the master process still gives dataloader an index for __getitem__ function, we just ignore such request and send a random batch dict. Also, the multiple workers forked by the dataloader all have the same seed, you will find that multiple workers will yield exactly the same data, if we use the above-mentioned trick directly. Therefore, we add one line of code which sets the defaut seed for numpy.random before activating multiple worker in dataloader. State-of-the-Art models  PSPNet is scene parsing network that aggregates global representation with Pyramid Pooling Module (PPM). It is the winner model of ILSVRC'16 MIT Scene Parsing Challenge. Please refer to https://arxiv.org/abs/1612.01105 for details. UPerNet is a model based on Feature Pyramid Network (FPN) and Pyramid Pooling Module (PPM). It doesn't need dilated convolution, an operator that is time-and-memory consuming. Without bells and whistles, it is comparable or even better compared with PSPNet, while requiring much shorter training time and less GPU memory. Please refer to https://arxiv.org/abs/1807.10221 for details. HRNet is a recently proposed model that retains high resolution representations throughout the model, without the traditional bottleneck design. It achieves the SOTA performance on a series of pixel labeling tasks. Please refer to https://arxiv.org/abs/1904.04514 for details.  Supported models We split our models into encoder and decoder, where encoders are usually modified directly from classification networks, and decoders consist of final convolutions and upsampling. We have provided some pre-configured models in the config folder. Encoder:  MobileNetV2dilated ResNet18/ResNet18dilated ResNet50/ResNet50dilated ResNet101/ResNet101dilated HRNetV2 (W48)  Decoder:  C1 (one convolution module) C1_deepsup (C1 + deep supervision trick) PPM (Pyramid Pooling Module, see PSPNet paper for details.) PPM_deepsup (PPM + deep supervision trick) UPerNet (Pyramid Pooling + FPN head, see UperNet for details.)  Performance: IMPORTANT: The base ResNet in our repository is a customized (different from the one in torchvision). The base models will be automatically downloaded when needed.  Architecture MultiScale Testing Mean IoU Pixel Accuracy(%) Overall Score Inference Speed(fps)  MobileNetV2dilated + C1_deepsup No34.8475.7554.07 17.2   Yes33.8476.8055.32 10.3   MobileNetV2dilated + PPM_deepsup No35.7677.7756.27 14.9   Yes36.2878.2657.27 6.7   ResNet18dilated + C1_deepsup No33.8276.0554.94 13.9   Yes35.3477.4156.38 5.8   ResNet18dilated + PPM_deepsup No38.0078.6458.32 11.7   Yes38.8179.2959.05 4.2   ResNet50dilated + PPM_deepsup No41.2679.7360.50 8.3   Yes42.1480.1361.14 2.6   ResNet101dilated + PPM_deepsup No42.1980.5961.39 6.8   Yes42.5380.9161.72 2.0   UperNet50 No40.4479.8060.12 8.4   Yes41.5580.2360.89 2.9   UperNet101 No42.0080.7961.40 7.8   Yes42.6681.0161.84 2.3   HRNetV2 No42.0380.7761.40 5.8   Yes43.2081.4762.34 1.9   The training is benchmarked on a server with 8 NVIDIA Pascal Titan Xp GPUs (12GB GPU memory), the inference speed is benchmarked a single NVIDIA Pascal Titan Xp GPU, without visualization. Environment The code is developed under the following configurations.  Hardware: >=4 GPUs for training, >=1 GPU for testing (set [--gpus GPUS] accordingly) Software: Ubuntu 16.04.3 LTS, CUDA>=8.0, Python>=3.5, PyTorch>=0.4.0 Dependencies: numpy, scipy, opencv, yacs, tqdm  Quick start: Test on an image using our trained model  Here is a simple demo to do inference on a single image:  chmod +x demo_test.sh ./demo_test.sh          This script downloads a trained model (ResNet50dilated + PPM_deepsup) and a test image, runs the test script, and saves predicted segmentation (.png) to the working directory.  To test on an image or a folder of images ($PATH_IMG), you can simply do the following:  python3 -u test.py --imgs $PATH_IMG --gpu $GPU --cfg $CFG           Training  Download the ADE20K scene parsing dataset:  chmod +x download_ADE20K.sh ./download_ADE20K.sh           Train a model by selecting the GPUs ($GPUS) and configuration file ($CFG) to use. During training, checkpoints by default are saved in folder ckpt.  python3 train.py --gpus $GPUS --cfg $CFG           To choose which gpus to use, you can either do --gpus 0-7, or --gpus 0,2,4,6.  For example, you can start with our provided configurations:  Train MobileNetV2dilated + C1_deepsup  python3 train.py --gpus GPUS --cfg config/ade20k-mobilenetv2dilated-c1_deepsup.yaml           Train ResNet50dilated + PPM_deepsup  python3 train.py --gpus GPUS --cfg config/ade20k-resnet50dilated-ppm_deepsup.yaml           Train UPerNet101  python3 train.py --gpus GPUS --cfg config/ade20k-resnet101-upernet.yaml           You can also override options in commandline, for example  python3 train.py TRAIN.num_epoch 10 .  Evaluation  Evaluate a trained model on the validation set. Add VAL.visualize True in argument to output visualizations as shown in teaser.  For example:  Evaluate MobileNetV2dilated + C1_deepsup  python3 eval_multipro.py --gpus GPUS --cfg config/ade20k-mobilenetv2dilated-c1_deepsup.yaml           Evaluate ResNet50dilated + PPM_deepsup  python3 eval_multipro.py --gpus GPUS --cfg config/ade20k-resnet50dilated-ppm_deepsup.yaml           Evaluate UPerNet101  python3 eval_multipro.py --gpus GPUS --cfg config/ade20k-resnet101-upernet.yaml          Integration with other projects This library can be installed via pip to easily integrate with another codebase pip install git+https://github.com/CSAILVision/semantic-segmentation-pytorch.git@master          Now this library can easily be consumed programmatically. For example from mit_semseg.config import cfg from mit_semseg.dataset import TestDataset from mit_semseg.models import ModelBuilder, SegmentationModule          Reference If you find the code or pre-trained models useful, please cite the following papers: Semantic Understanding of Scenes through ADE20K Dataset. B. Zhou, H. Zhao, X. Puig, T. Xiao, S. Fidler, A. Barriuso and A. Torralba. International Journal on Computer Vision (IJCV), 2018. (https://arxiv.org/pdf/1608.05442.pdf) @article{zhou2018semantic,   title={Semantic understanding of scenes through the ade20k dataset},   author={Zhou, Bolei and Zhao, Hang and Puig, Xavier and Xiao, Tete and Fidler, Sanja and Barriuso, Adela and Torralba, Antonio},   journal={International Journal on Computer Vision},   year={2018} }           Scene Parsing through ADE20K Dataset. B. Zhou, H. Zhao, X. Puig, S. Fidler, A. Barriuso and A. Torralba. Computer Vision and Pattern Recognition (CVPR), 2017. (http://people.csail.mit.edu/bzhou/publication/scene-parse-camera-ready.pdf) @inproceedings{zhou2017scene,     title={Scene Parsing through ADE20K Dataset},     author={Zhou, Bolei and Zhao, Hang and Puig, Xavier and Fidler, Sanja and Barriuso, Adela and Torralba, Antonio},     booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},     year={2017} } "
2,GeorgeSeif/Semantic-Segmentation-Suite,2.4k,https://github.com/GeorgeSeif/Semantic-Segmentation-Suite,"Updated on Apr 22, 2021","Semantic Segmentation Suite in TensorFlow News What's New Description Citing Frontends Models Files and Directories Installation Usage Results      README.md           Semantic Segmentation Suite in TensorFlow  News What's New  This repo has been depricated and will no longer be handling issues. Feel free to use as is :)  Description This repository serves as a Semantic Segmentation Suite. The goal is to easily be able to implement, train, and test new Semantic Segmentation models! Complete with the following:  Training and testing modes Data augmentation Several state-of-the-art models. Easily plug and play with different models Able to use any dataset Evaluation including precision, recall, f1 score, average accuracy, per-class accuracy, and mean IoU Plotting of loss function and accuracy over epochs  Any suggestions to improve this repository, including any new segmentation models you would like to see are welcome! You can also check out my Transfer Learning Suite. Citing If you find this repository useful, please consider citing it using a link to the repo :) Frontends The following feature extraction models are currently made available:  MobileNetV2, ResNet50/101/152, and InceptionV4  Models The following segmentation models are currently made available:   Encoder-Decoder based on SegNet. This network uses a VGG-style encoder-decoder, where the upsampling in the decoder is done using transposed convolutions.   Encoder-Decoder with skip connections based on SegNet. This network uses a VGG-style encoder-decoder, where the upsampling in the decoder is done using transposed convolutions. In addition, it employs additive skip connections from the encoder to the decoder.   Mobile UNet for Semantic Segmentation. Combining the ideas of MobileNets Depthwise Separable Convolutions with UNet to build a high speed, low parameter Semantic Segmentation model.   Pyramid Scene Parsing Network. In this paper, the capability of global context information by different-region based context aggregation is applied through a pyramid pooling module together with the proposed pyramid scene parsing network (PSPNet). Note that the original PSPNet uses a ResNet with dilated convolutions, but the one is this respository has only a regular ResNet.   The One Hundred Layers Tiramisu: Fully Convolutional DenseNets for Semantic Segmentation. Uses a downsampling-upsampling style encoder-decoder network. Each stage i.e between the pooling layers uses dense blocks. In addition, it concatenated skip connections from the encoder to the decoder. In the code, this is the FC-DenseNet model.   Rethinking Atrous Convolution for Semantic Image Segmentation. This is the DeepLabV3 network. Uses Atrous Spatial Pyramid Pooling to capture multi-scale context by using multiple atrous rates. This creates a large receptive field.   RefineNet: Multi-Path Refinement Networks for High-Resolution Semantic Segmentation. A multi-path refinement network that explicitly exploits all the information available along the down-sampling process to enable high-resolution prediction using long-range residual connections. In this way, the deeper layers that capture high-level semantic features can be directly refined using fine-grained features from earlier convolutions.   Full-Resolution Residual Networks for Semantic Segmentation in Street Scenes. Combines multi-scale context with pixel-level accuracy by using two processing streams within the network. The residual stream carries information at the full image resolution, enabling precise adherence to segment boundaries. The pooling stream undergoes a sequence of pooling operations to obtain robust features for recognition. The two streams are coupled at the full image resolution using residuals. In the code, this is the FRRN model.   Large Kernel Matters -- Improve Semantic Segmentation by Global Convolutional Network. Proposes a Global Convolutional Network to address both the classification and localization issues for the semantic segmentation. Uses large separable kernals to expand the receptive field, plus a boundary refinement block to further improve localization performance near boundaries.   AdapNet: Adaptive Semantic Segmentation in Adverse Environmental Conditions Modifies the ResNet50 architecture by performing the lower resolution processing using a multi-scale strategy with atrous convolutions. This is a slightly modified version using bilinear upscaling instead of transposed convolutions as I found it gave better results.   ICNet for Real-Time Semantic Segmentation on High-Resolution Images. Proposes a compressed-PSPNet-based image cascade network (ICNet) that incorporates multi-resolution branches under proper label guidance to address this challenge. Most of the processing is done at low resolution for high speed and the multi-scale auxillary loss helps get an accurate model. Note that for this model, I have implemented the network but have not integrated its training yet   Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation. This is the DeepLabV3+ network which adds a Decoder module on top of the regular DeepLabV3 model.   DenseASPP for Semantic Segmentation in Street Scenes. Combines many different scales using dilated convolution but with dense connections   Dense Decoder Shortcut Connections for Single-Pass Semantic Segmentation. Dense Decoder Shorcut Connections using dense connectivity in the decoder stage of the segmentation model. Note: this network takes a bit of extra time to load due to the construction of the ResNeXt blocks   BiSeNet: Bilateral Segmentation Network for Real-time Semantic Segmentation. BiSeNet use a Spatial Path with a small stride to preserve the spatial information and generate high-resolution features while having a parallel Context Path with a fast downsampling strategy to obtain sufficient receptive field.   Or make your own and plug and play!   Files and Directories   train.py: Training on the dataset of your choice. Default is CamVid   test.py: Testing on the dataset of your choice. Default is CamVid   predict.py: Use your newly trained model to run a prediction on a single image   helper.py: Quick helper functions for data preparation and visualization   utils.py: Utilities for printing, debugging, testing, and evaluation   models: Folder containing all model files. Use this to build your models, or use a pre-built one   CamVid: The CamVid datatset for Semantic Segmentation as a test bed. This is the 32 class version   checkpoints: Checkpoint files for each epoch during training   Test: Test results including images, per-class accuracies, precision, recall, and f1 score   Installation This project has the following dependencies:   Numpy sudo pip install numpy   OpenCV Python sudo apt-get install python-opencv   TensorFlow sudo pip install --upgrade tensorflow-gpu   Usage The only thing you have to do to get started is set up the folders in the following structure: ├── ""dataset_name"" |   ├── train |   ├── train_labels |   ├── val |   ├── val_labels |   ├── test |   ├── test_labels           Put a text file under the dataset directory called ""class_dict.csv"" which contains the list of classes along with the R, G, B colour labels to visualize the segmentation results. This kind of dictionairy is usually supplied with the dataset. Here is an example for the CamVid dataset: name,r,g,b Animal,64,128,64 Archway,192,0,128 Bicyclist,0,128, 192 Bridge,0, 128, 64 Building,128, 0, 0 Car,64, 0, 128 CartLuggagePram,64, 0, 192 Child,192, 128, 64 Column_Pole,192, 192, 128 Fence,64, 64, 128 LaneMkgsDriv,128, 0, 192 LaneMkgsNonDriv,192, 0, 64 Misc_Text,128, 128, 64 MotorcycleScooter,192, 0, 192 OtherMoving,128, 64, 64 ParkingBlock,64, 192, 128 Pedestrian,64, 64, 0 Road,128, 64, 128 RoadShoulder,128, 128, 192 Sidewalk,0, 0, 192 SignSymbol,192, 128, 128 Sky,128, 128, 128 SUVPickupTruck,64, 128,192 TrafficCone,0, 0, 64 TrafficLight,0, 64, 64 Train,192, 64, 128 Tree,128, 128, 0 Truck_Bus,192, 128, 192 Tunnel,64, 0, 64 VegetationMisc,192, 192, 0 Void,0, 0, 0 Wall,64, 192, 0           Note: If you are using any of the networks that rely on a pre-trained ResNet, then you will need to download the pre-trained weights using the provided script. These are currently: PSPNet, RefineNet, DeepLabV3, DeepLabV3+, GCN. Then you can simply run train.py! Check out the optional command line arguments: usage: train.py [-h] [--num_epochs NUM_EPOCHS]                 [--checkpoint_step CHECKPOINT_STEP]                 [--validation_step VALIDATION_STEP] [--image IMAGE]                 [--continue_training CONTINUE_TRAINING] [--dataset DATASET]                 [--crop_height CROP_HEIGHT] [--crop_width CROP_WIDTH]                 [--batch_size BATCH_SIZE] [--num_val_images NUM_VAL_IMAGES]                 [--h_flip H_FLIP] [--v_flip V_FLIP] [--brightness BRIGHTNESS]                 [--rotation ROTATION] [--model MODEL] [--frontend FRONTEND]  optional arguments:   -h, --help            show this help message and exit   --num_epochs NUM_EPOCHS                         Number of epochs to train for   --checkpoint_step CHECKPOINT_STEP                         How often to save checkpoints (epochs)   --validation_step VALIDATION_STEP                         How often to perform validation (epochs)   --image IMAGE         The image you want to predict on. Only valid in                         ""predict"" mode.   --continue_training CONTINUE_TRAINING                         Whether to continue training from a checkpoint   --dataset DATASET     Dataset you are using.   --crop_height CROP_HEIGHT                         Height of cropped input image to network   --crop_width CROP_WIDTH                         Width of cropped input image to network   --batch_size BATCH_SIZE                         Number of images in each batch   --num_val_images NUM_VAL_IMAGES                         The number of images to used for validations   --h_flip H_FLIP       Whether to randomly flip the image horizontally for                         data augmentation   --v_flip V_FLIP       Whether to randomly flip the image vertically for data                         augmentation   --brightness BRIGHTNESS                         Whether to randomly change the image brightness for                         data augmentation. Specifies the max bightness change                         as a factor between 0.0 and 1.0. For example, 0.1                         represents a max brightness change of 10% (+-).   --rotation ROTATION   Whether to randomly rotate the image for data                         augmentation. Specifies the max rotation angle in                         degrees.   --model MODEL         The model you are using. See model_builder.py for                         supported models   --frontend FRONTEND   The frontend you are using. See frontend_builder.py                         for supported models            Results These are some sample results for the CamVid dataset with 11 classes (previous research version). In training, I used a batch size of 1 and image size of 352x480. The following results are for the FC-DenseNet103 model trained for 300 epochs. I used RMSProp with learning rate 0.001 and decay 0.995. I did not use any data augmentation like in the paper. I also didn't use any class balancing. These are just some quick and dirty example results. Note that the checkpoint files are not uploaded to this repository since they are too big for GitHub (greater than 100 MB)    Class Original Accuracy My Accuracy     Sky 93.0 94.1   Building 83.0 81.2   Pole 37.8 38.3   Road 94.5 97.5   Pavement 82.2 87.9   Tree 77.3 75.5   SignSymbol 43.9 49.7   Fence 37.1 69.0   Car 77.3 87.0   Pedestrian 59.6 60.3   Bicyclist 50.5 75.3   Unlabelled N/A 40.9   Global 91.5 89.6       Loss vs Epochs Val. Acc. vs Epochs             Original GT Result "
3,tangzhenyu/SemanticSegmentation_DL,1.1k,https://github.com/tangzhenyu/SemanticSegmentation_DL,"Updated on Mar 9, 2021","Semantic-Segmentation Dataset importance SemanticSegmentation_DL Dataset Resources Survey papers Online demos 2D Semantic Segmentation Papers: 3D Semantic Segmentation Papers Instance Segmentation Robotics Adversarial Training Scene Understanding Papers Dataset & Resources Weakly-Supervised-Segmentation && Interactive Segmentation && Transferable Semantic Segmentation Video Semantic Segmentation Multi-Task Learning Papers: Road Segmentation && Real Time Segmentation Papers: Codes Medical Image Semantic Segmentation Papers Codes Part Semantic Segmentation Clothes Parsing Popular Methods and Implementations Annotation Tools: Distinguished Researchers & Teams: Results: Reference      README.md           Semantic-Segmentation A list of all papers and resoureces on Semantic Segmentation. Dataset importance  SemanticSegmentation_DL Some implementation of semantic segmantation for DL model Dataset  voc2012 CitySpaces Mapillary ADE20K PASCAL Context COCO-Stuff 10K dataset v1.1 2D-3D-S dataset Mapillary Vistas Stanford Background Dataset Sift Flow Dataset Barcelona Dataset Microsoft COCO dataset MSRC Dataset LITS Liver Tumor Segmentation Dataset KITTI Pascal Context Data from Games dataset Human parsing dataset Mapillary Vistas Dataset Microsoft AirSim MIT Scene Parsing Benchmark COCO 2017 Stuff Segmentation Challenge ADE20K Dataset INRIA Annotations for Graz-02 Daimler dataset ISBI Challenge: Segmentation of neuronal structures in EM stacks INRIA Annotations for Graz-02 (IG02) Pratheepan Dataset Clothing Co-Parsing (CCP) Dataset  Resources Survey papers  A 2017 Guide to Semantic Segmentation with Deep Learning by Qure AI [Blog about different sem. segm. methods] A Review on Deep Learning Techniques Applied to Semantic Segmentation [Survey paper with a special focus on datasets and the highest performing methods] Computer Vision for Autonomous Vehicles: Problems, Datasets and State-of-the-Art [Survey paper about all aspects of autonomous vehicles, including sem. segm.] [Webpage with a summary of all relevant publications] A Survey on Deep Learning in Medical Image Analysis [[Paper]](https://arxiv.org/pdf/1702.05747）  Online demos  CRF as RNN SegNet  2D Semantic Segmentation Papers:   [2019-CVPR oral] CLAN: Category-level Adversaries for Semantics Consistent [paper] [code]   [2019-CVPR] BRS: Interactive Image Segmentation via Backpropagating Refinement Scheme(***) [paper] [code]   [2019-CVPR] DFANet：Deep Feature Aggregation for Real-Time Semantic Segmentation(used in camera) [paper] [code]   [2019-CVPR] DeepCO3: Deep Instance Co-segmentation by Co-peak Search and Co-saliency [paper] [code]   [2019-CVPR] Domain Adaptation(reducing the domain shif) [paper]   [2019-CVPR] ELKPPNet: An Edge-aware Neural Network with Large Kernel Pyramid Pooling for Learning Discriminative Features in Semantic- Segmentation [paper] [code]   [2019-CVPR oral] GLNet: Collaborative Global-Local Networks for Memory-Efficient Segmentation of Ultra-High Resolution Images[paper] [code]   [2019-CVPR] Instance Segmentation by Jointly Optimizing Spatial Embeddings and Clustering Bandwidth(***SOTA) [paper] [code]   [2019-ECCV] ICNet: Real-Time Semantic Segmentation on High-Resolution Images [paper] [code]   [2019-CVPR] LEDNet: A Lightweight Encoder-Decoder Network for Real-Time Semantic Segmentation(***SOTA) [paper] [code]   [2019-arXiv] LightNet++: Boosted Light-weighted Networks for Real-time Semantic Segmentation [paper] [code]   [2019-CVPR] PTSNet: A Cascaded Network for Video Object Segmentation [paper] [code]   [2019-CVPR] PPGNet: Learning Point-Pair Graph for Line Segment Detection [paper] [code]   [2019-CVPR] Show, Match and Segment: Joint Learning of Semantic Matching and Object Co-segmentation [paper] [code]   [2019-CVPR] Video Instance Segmentation [paper] [code]    Arxiv-2018 ExFuse: Enhancing Feature Fusion for Semantic Segmentation 87.9% mean Iou->voc2012 [Paper] CVPR-2018 spotlight Learning to Adapt Structured Output Space for Semantic Segmentation  [Paper] [Code] Arfix-2018 Adversarial Learning for Semi-supervised Semantic Segmentation [Paper] [Code] Arxiv-2018 Context Encoding for Semantic Segmentation [Paper] [Code] CVPR-2018 Learning to Adapt Structured Output Space for Semantic Segmentation [Paper][Code] CVPR-2018 Dynamic-structured Semantic Propagation Network [Paper] Deeplab v4: Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation [Paper] [Code] Deep Value Networks Learn to Evaluate and Iteratively Refine Structured Outputs [Paper][Code] ICCV-2017 Semantic Line Detection and Its Applications [Paper] ICCV-2017 Attentive Semantic Video Generation Using Captions [Paper] ICCV-2017 BlitzNet: A Real-Time Deep Network for Scene Understanding [Paper] [Code] ICCV-2017 SCNet: Learning Semantic Correspondence   [Code] CVPR-2017 End-to-End Instance Segmentation with Recurrent Attention [Code] CVPR-2017 Deep Watershed Transform for Instance Segmentation [Code] Piecewise Flat Embedding for Image Segmentation [Paper] ICCV-2017 Curriculum Domain Adaptation for Semantic Segmentation of Urban Scenes [Paper][Code] CVPR-2017 Not All Pixels Are Equal: Difficulty-Aware Semantic Segmentation via Deep Layer Cascade-2017 [Paper] CVPR-2017 Annotating Object Instances with a Polygon-RNN-2017 [Project] [Paper] CVPR-2017  Loss maxpooling for semantic image segmentation [Paper] ICCV-2017 Scale-adaptive convolutions for scene parsing [Paper] Towards End-to-End Lane Detection: an Instance Segmentation Approach [Paper]arxiv-1802 AAAI-2018 Mix-and-Match Tuning for Self-Supervised Semantic Segmentation [Paper] arxiv-1712 NIPS-2017-Learning Affinity via Spatial Propagation Networks [Paper] AAAI-2018-Spatial As Deep: Spatial CNN for Traffic Scene Understanding [Paper] Stacked Deconvolutional Network for Semantic Segmentation-2017 [Paper] Deeplab v3: Rethinking Atrous Convolution for Semantic Image Segmentation-2017(DeeplabV3) [Paper] CVPR-2017 Learning Object Interactions and Descriptions for Semantic Image Segmentation-2017 [Paper] Pixel Deconvolutional Networks-2017 [Code-Tensorflow] [Paper] Dilated Residual Networks-2017 [Paper] A Review on Deep Learning Techniques Applied to Semantic Segmentation-2017 [Paper] BiSeg: Simultaneous Instance Segmentation and Semantic Segmentation with Fully Convolutional Networks [Paper] ICNet for Real-Time Semantic Segmentation on High-Resolution Images-2017 [Project] [Code] [Paper] [Video]   Feature Forwarding: Exploiting Encoder Representations for Efficient Semantic Segmentation-2017 [Project] [Code-Torch7] Reformulating Level Sets as Deep Recurrent Neural Network Approach to Semantic Segmentation-2017 [Paper] Adversarial Examples for Semantic Image Segmentation-2017 [Paper] Large Kernel Matters - Improve Semantic Segmentation by Global Convolutional Network-2017 [Paper] HyperNet: Towards Accurate Region Proposal Generation and Joint Object Detection [Paper] Hypercolumns for Object Segmentation and Fine-grained Localization [Paper] Matching-CNN meets KNN: Quasi-parametric human parsing[Paper] Deep Human Parsing with Active Template Regression [Paper] TPAMI-2012 Learning Hierarchical Features for Scene Labeling The first paper for applying dl on semantic segmentation !!! [Paper] Label Refinement Network for Coarse-to-Fine Semantic Segmentation-2017 [Paper] Laplacian Pyramid Reconstruction and Refinement for Semantic Segmentation [Paper] ParseNet: Looking Wider to See Better [Paper] CVPR-2016 Recombinator Networks: Learning Coarse-to-Fine Feature Aggregation [Paper] PixelNet: Representation of the pixels, by the pixels, and for the pixels-2017 [Project] [Code-Caffe] [Paper] LabelBank: Revisiting Global Perspectives for Semantic Segmentation-2017 [Paper] Progressively Diffused Networks for Semantic Image Segmentation-2017 [Paper] Understanding Convolution for Semantic Segmentation-2017 [Model-Mxnet] [Paper] [Code] ICCV-2017 Predicting Deeper into the Future of Semantic Segmentation-2017 [Paper] CVPR-2017 Pyramid Scene Parsing Network-2017 [Project] [Code-Caffe] [Paper] [Slides] FCNs in the Wild: Pixel-level Adversarial and Constraint-based Adaptation-2016 [Paper] FusionNet: A deep fully residual convolutional neural network for image segmentation in connectomics-2016 [Code-PyTorch] [Paper] RefineNet: Multi-Path Refinement Networks for High-Resolution Semantic Segmentation-2016 [Code-MatConvNet] [Paper] CVPRW-2017 The One Hundred Layers Tiramisu: Fully Convolutional DenseNets for Semantic Segmentation [Code-Theano] [Code-Keras1] [Code-Keras2] [Paper] CVPR-2017 Full-Resolution Residual Networks for Semantic Segmentation in Street Scenes [Code-Theano] [Paper] PixelNet: Towards a General Pixel-level Architecture-2016 [Paper] Recalling Holistic Information for Semantic Segmentation-2016 [Paper] Semantic Segmentation using Adversarial Networks-2016 [Paper] [Code-Chainer] Region-based semantic segmentation with end-to-end training-2016 [Paper] Exploring Context with Deep Structured models for Semantic Segmentation-2016 [Paper]</ Multi-scale context aggregation by dilated convolutions [Paper] Better Image Segmentation by Exploiting Dense Semantic Predictions-2016 [Paper] Boundary-aware Instance Segmentation-2016 [Paper] Improving Fully Convolution Network for Semantic Segmentation-2016 [Paper] Deep Structured Features for Semantic Segmentation-2016 [Paper] DeepLab v2:Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs-2016** [Project] [Code-Caffe] [Code-Tensorflow] [Code-PyTorch] [Paper] DeepLab v1: Semantic Image Segmentation With Deep Convolutional Nets and Fully Connected CRFs-2014** [Code-Caffe1] [Code-Caffe2] [Paper] Deep Learning Markov Random Field for Semantic Segmentation-2016 [Project] [Paper] ECCV2016 Salient Deconvolutional Networks  [Code] Convolutional Random Walk Networks for Semantic Image Segmentation-2016 [Paper] ENet: A Deep Neural Network Architecture for Real-Time Semantic Segmentation-2016 [Code-Caffe1][Code-Caffe2] [Paper] [Blog] High-performance Semantic Segmentation Using Very Deep Fully Convolutional Networks-2016 [Paper] CVPR-2016-oral ScribbleSup: Scribble-Supervised Convolutional Networks for Semantic Segmentation-2016 [Paper] Object Boundary Guided Semantic Segmentation-2016 [Code-Caffe] [Paper] Segmentation from Natural Language Expressions-2016 [Project] [Code-Tensorflow] [Code-Caffe] [Paper] Seed, Expand and Constrain: Three Principles for Weakly-Supervised Image Segmentation-2016 [Code-Caffe] [Paper] Global Deconvolutional Networks for Semantic Segmentation-2016 [Paper] Learning Transferrable Knowledge for Semantic Segmentation with Deep Convolutional Neural Network-2015 [Project] [Code-Caffe] [Paper] Learning Dense Convolutional Embeddings for Semantic Segmentation-2015 [Paper] ParseNet: Looking Wider to See Better-2015 [Code-Caffe] [Model-Caffe] [Paper] Decoupled Deep Neural Network for Semi-supervised Semantic Segmentation-2015 [Project] [Code-Caffe] [Paper] Bayesian segnet: Model uncertainty in deep convolutional encoder-decoder architectures for scene understanding [Paper] SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation-2015 [Project] [Code-Caffe] [Paper] [Tutorial1] [Tutorial2] Semantic Image Segmentation with Task-Specific Edge Detection Using CNNs and a Discriminatively Trained Domain Transform-2015 [Paper] Semantic Segmentation with Boundary Neural Fields-2015 [Code] [Paper] Semantic Image Segmentation via Deep Parsing Network-2015 [Project] [Paper1] [Paper2] [Slides] What’s the Point: Semantic Segmentation with Point Supervision-2015 [Project] [Code-Caffe] [Model-Caffe] [Paper] U-Net: Convolutional Networks for Biomedical Image Segmentation-2015 [Project] [Code+Data] [Code-Keras] [Code-Tensorflow] [Paper] [Notes] Learning Deconvolution Network for Semantic Segmentation(DeconvNet)-2015 [Project] [Code-Caffe] [Paper] [Slides] Multi-scale Context Aggregation by Dilated Convolutions-2015 [Project] [Code-Caffe] [Code-Keras] [Paper] [Notes] ReSeg: A Recurrent Neural Network-based Model for Semantic Segmentation-2015 [Code-Theano] [Paper] ICCV-2015 BoxSup: Exploiting Bounding Boxes to Supervise Convolutional Networks for Semantic Segmentation-2015 [Paper] Feedforward semantic segmentation with zoom-out features-2015 [Code] [Paper] [Video] Conditional Random Fields as Recurrent Neural Networks-2015 [Project] [Code-Caffe1] [Code-Caffe2] [Demo] [Paper1] [Paper2] Efficient Piecewise Training of Deep Structured Models for Semantic Segmentation-2015 [Paper] Fully Convolutional Networks for Semantic Segmentation-2015 [Code-Caffe] [Model-Caffe] [Code-Tensorflow1] [Code-Tensorflow2] [Code-Chainer] [Code-PyTorch] [Paper1] [Paper2] [Slides1] [Slides2] Deep Joint Task Learning for Generic Object Extraction-2014 [Project] [Code-Caffe] [Dataset] [Paper] Highly Efficient Forward and Backward Propagation of Convolutional Neural Networks for Pixelwise Classification-2014 [Code-Caffe] [Paper] Wider or deeper: Revisiting the resnet model for visual recognition [Paper] Describing the Scene as a Whole: Joint Object Detection, Scene Classification and Semantic Segmentation[Paper] Analyzing Semantic Segmentation Using Hybrid Human-Machine CRFs[Paper] Convolutional Patch Networks with Spatial Prior for Road Detection and Urban Scene Understanding[Paper] Deep Deconvolutional Networks for Scene Parsing[Paper] FusionSeg: Learning to combine motion and appearance for fully automatic segmention of generic objects in videos[Paper][Poject] ICCV-2017 Deep Dual Learning for Semantic Image Segmentation [Paper] From image-level to pixel level labeling with convolutional networks [Paper] Scene Segmentation with DAG-Recurrent Neural Networks [Paper] Learning to Segment Every Thing [Paper] Panoptic Segmentation [Paper] The Devil is in the Decoder [Paper] Attention to Scale: Scale-aware Semantic Image Segmentation [Paper][Project] Convolutional Oriented Boundaries: From Image Segmentation to High-Level Tasks [Paper] [Project] Scale-Aware Alignment of Hierarchical Image Segmentation [Paper] [Project] ICCV-2017 Semi Supervised Semantic Segmentation Using Generative Adversarial Network[Paper] Object Region Mining with Adversarial Erasing: A Simple Classification to Semantic Segmentation Approach [Paper]   CVPR-2016 Convolutional Feature Masking for Joint Object and Stuff Segmentation [Paper] ECCV-2016 Laplacian Pyramid Reconstruction and Refinement for Semantic Segmentation [Paper]   FastMask: Segment Object Multi-scale Candidates in One Shot-2016 [Code-Caffe] [Paper] Pixel Objectness-2017 [Project] [Code-Caffe] [Paper]  3D Semantic Segmentation Papers  PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation [Paper] PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space (2017) [Paper] Learning 3D Mesh Segmentation and Labeling (2010) [Paper] Unsupervised Co-Segmentation of a Set of Shapes via Descriptor-Space Spectral Clustering (2011) [Paper] Single-View Reconstruction via Joint Analysis of Image and Shape Collections (2015) [Paper] 3D Shape Segmentation with Projective Convolutional Networks (2017) [Paper] Learning Hierarchical Shape Segmentation and Labeling from Online Repositories (2017) [Paper] 3D Graph Neural Networks for RGBD Semantic Segmentation (2017) [Paper] 3DCNN-DQN-RNN: A Deep Reinforcement Learning Framework for Semantic Parsing of Large-scale 3D Point Clouds (2017)[Paper] Multi-view deep learning for consistent semantic mapping with rgb-d cameras [Paper]   ICCV-2017 Large-scale 3D Shape Reconstruction and Segmentation from ShapeNet Core55 [Paper][Project]  Instance Segmentation  Mask Scoring R-CNN (MS R-CNN) [Code][Paper] Predicting Future Instance Segmentations by Forecasting Convolutional Features [Paper] CVPR-2018 Path Aggregation Network for Instance Segmentation [Paper] better than Mask-rcnn!！COCO-2017 1st! Pixelwise Instance Segmentation with a Dynamically Instantiated Network-2017 [Paper] Semantic Instance Segmentation via Deep Metric Learning-2017 [Paper] CVPR-2017 FastMask: Segment Multi-scale Object Candidates in One Shot [Code-Tensorflow] [Paper] Pose2Instance: Harnessing Keypoints for Person Instance Segmentation-2017 [Paper] Pixelwise Instance Segmentation with a Dynamically Instantiated Network-2017 [Paper] CVPR-2017-spotlight Fully Convolutional Instance-aware Semantic Segmentation-2016 [Code] [Paper] CVPR-2016-oral Instance-aware Semantic Segmentation via Multi-task Network Cascades-2015 [Code] [Paper] Recurrent Instance Segmentation-2015 [Project] [Code-Torch7] [Paper] [Poster] [Video] Annotating Object Instances with a Polygon-RNN [Paper] MaskLab: Instance Segmentation by Refining Object Detection with Semantic and Direction Features [Paper] FCIS:Fully Convolutional Instance-aware Semantic Segmentation [Paper]Code MNC:Instance-aware Semantic Segmentation via Multi-task Network Cascades [Paper]Code DeepMask:Learning to Segment Object Candidates [Paper] Code SharpMask:Learning to Refine Object Segments [Paper]Code RIS:Recurrent Instance Segmentation [Paper]Code FastMask: Segment Multi-scale Object Candidates in One Shot [Paper]Code Proposal-free network for instance-level object segmentation [Paper] ECCV-2016 Instance-sensitive Fully Convolutional Networks  [Paper] Pixel-level encoding and depth layering for instance-level semantic labeling [Paper]  Robotics  Virtual-to-Real: Learning to Control in Visual Semantic Segmentation [Paper] End-to-End Tracking and Semantic Segmentation Using Recurrent Neural Networks [Paper] Semantic Segmentation using Adversarial Networks [Paper]  Adversarial Training  CVPR-2017-Image-to-Image Translation with Conditional Adversarial Networks [Paper] ICCV-2017-Adversarial Examples for Semantic Segmentation and Object Detection [Paper]  Scene Understanding Papers 1.Spatial As Deep: Spatial CNN for Traffic Scene Understanding [Paper] Dataset & Resources  SUNRGB-D 3D Object Detection Challenge [Link] 19 object categories for predicting a 3D bounding box in real world dimension Training set: 10,355 RGB-D scene images, Testing set: 2860 RGB-D images SceneNN (2016) [Link] 100+ indoor scene meshes with per-vertex and per-pixel annotation. ScanNet (2017) [Link] An RGB-D video dataset containing 2.5 million views in more than 1500 scans, annotated with 3D camera poses, surface reconstructions, and instance-level semantic segmentations. Matterport3D: Learning from RGB-D Data in Indoor Environments (2017) [Link] 10,800 panoramic views (in both RGB and depth) from 194,400 RGB-D images of 90 building-scale scenes of private rooms. Instance-level semantic segmentations are provided for region (living room, kitchen) and object (sofa, TV) categories. SUNCG: A Large 3D Model Repository for Indoor Scenes (2017) [Link] The dataset contains over 45K different scenes with manually created realistic room and furniture layouts. All of the scenes are semantically annotated at the object level. MINOS: Multimodal Indoor Simulator (2017) [Link] MINOS is a simulator designed to support the development of multisensory models for goal-directed navigation in complex indoor environments. MINOS leverages large datasets of complex 3D environments and supports flexible configuration of multimodal sensor suites. MINOS supports SUNCG and Matterport3D scenes. Facebook House3D: A Rich and Realistic 3D Environment (2017) [Link] House3D is a virtual 3D environment which consists of 45K indoor scenes equipped with a diverse set of scene types, layouts and objects sourced from the SUNCG dataset. All 3D objects are fully annotated with category labels. Agents in the environment have access to observations of multiple modalities, including RGB images, depth, segmentation masks and top-down 2D map views. HoME: a Household Multimodal Environment (2017) [Link] HoME integrates over 45,000 diverse 3D house layouts based on the SUNCG dataset, a scale which may facilitate learning, generalization, and transfer. HoME is an open-source, OpenAI Gym-compatible platform extensible to tasks in reinforcement learning, language grounding, sound-based navigation, robotics, multi-agent learning. AI2-THOR: Photorealistic Interactive Environments for AI Agents [Link] AI2-THOR is a photo-realistic interactable framework for AI agents. There are a total 120 scenes in version 1.0 of the THOR environment covering four different room categories: kitchens, living rooms, bedrooms, and bathrooms. Each room has a number of actionable objects.  Weakly-Supervised-Segmentation && Interactive Segmentation && Transferable Semantic Segmentation  arxiv-2018 WebSeg: Learning Semantic Segmentation from Web Searches [Paper] Weakly Supervised Object Localization Using Things and Stuff Transfer [Paper] Semi and Weakly Supervised Semantic Segmentation Using Generative Adversarial Network [Paper] Weakly- and Semi-Supervised Learning of a Deep Convolutional Network for Semantic Image Segmentation [Paper]   Weakly Supervised Structured Output Learning for Semantic Segmentation [Paper] ICCV-2011 Weakly supervised semantic segmentation with a multi-image model [Paper] ScribbleSup: Scribble-Supervised Convolutional Networks for Semantic Segmentation. IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016[Paper] Constrained convolutional neural networks for weakly supervised segmentation. Proceedings of the IEEE International Conference on Computer Vision. 2015.[Paper] Weakly-and semi-supervised learning of a DCNN for semantic image segmentation. arXiv preprint arXiv:1502.02734 (2015).[Paper] Learning to segment under various forms of weak supervision. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2015.[Paper] STC: A Simple to Complex Framework for Weakly-supervised Semantic Segmentation 2017 TPAMI [Paper] [Project] [Paper] CVPR-2017-Simple Does It: Weakly Supervised Instance and Semantic Segmentation [Paper] [tensorflow] CVPR-2017-Weakly Supervised Semantic Segmentation using Web-Crawled Videos [Paper] AAAI-2017-Weakly Supervised Semantic Segmentation Using Superpixel Pooling Network [Paper] ICCV-2015-Weakly supervised graph based semantic segmentation by learning communities of image-parts [Paper] Towards Weakly Supervised Semantic Segmentation by Means of Multiple Instance and Multitask Learning [Paper] Weakly-Supervised Semantic Segmentation using Motion Cues [Paper] [Project] Weakly Supervised Semantic Segmentation Based on Web Image Co-segmentation [Paper] Learning to Rene Object Segments [Paper]   Weakly-Supervised Dual Clustering for Image Semantic Segmentation [Paper]   Interactive Video Object Segmentation in the Wild [Paper]  Video Semantic Segmentation  CVPR-2017 Video Object Segmentation Without Temporal Information One-Shot Video Object Segmentation [Project] Feature Space Optimization for Semantic Video Segmentation[Paper][Slides] The Basics of Video Object Segmentation [Blog] ICCV2017----SegFlow_Joint Learning for Video Object Segmentation and Optical Flow OSVOS:One-Shot Video Object Segmentation Surveillance Video Parsing with Single Frame Supervision The 2017 DAVIS Challenge on Video Object Segmentation Video Propagation Networks OnAVOS: Online Adaptation of Convolutional Neural Networks for Video Object Segmentation. P. Voigtlaender, B. Leibe, BMVC 2017. [Project Page] [Precomputed results] MSK: Learning Video Object Segmentation from Static Images. F. Perazzi*, A. Khoreva*, R. Benenson, B. Schiele, A. Sorkine-Hornung, CVPR 2017. [Project Page] [Precomputed results] SFL: SegFlow: Joint Learning for Video Object Segmentation and Optical Flow. J. Cheng, Y.-H. Tsai, S. Wang, M.-H. Yang, ICCV 2017. [Project Page] [Precomputed results] CTN: Online Video Object Segmentation via Convolutional Trident Network. W.-D. Jang, C.-S. Kim, CVPR 2017. [Project Page] [Precomputed results] VPN: Video Propagation Networks. V. Jampani, R. Gadde, P. V. Gehler, CVPR 2017. [Project Page] [Precomputed results] PLM: Pixel-level Matching for Video Object Segmentation using Convolutional Neural Networks. J. Shin Yoon, F. Rameau, J. Kim, S. Lee, S. Shin, I. So Kweon, ICCV 2017. [Project Page] [Precomputed results] OFL: Video Segmentation via Object Flow. Y.-H. Tsai, M.-H. Yang, M. Black, CVPR 2016. [Project Page] [Precomputed results] BVS: Bilateral Space Video Segmentation. N. Marki, F. Perazzi, O. Wang, A. Sorkine-Hornung, CVPR 2016. [Project Page] [Precomputed results] FCP: Fully Connected Object Proposals for Video Segmentation. F. Perazzi, O. Wang, M. Gross, A. Sorkine-Hornung, ICCV 2015. [Project Page] [Precomputed results] JMP: JumpCut: Non-Successive Mask Transfer and Interpolation for Video Cutout. Q. Fan, F. Zhong, D. Lischinski, D. Cohen-Or, B. Chen, SIGGRAPH 2015. [Project Page] [Precomputed results] HVS: Efficient hierarchical graph-based video segmentation. M. Grundmann, V. Kwatra, M. Han, I. A. Essa, CVPR 2010. [Project Page] [Precomputed results] SEA: SeamSeg: Video Object Segmentation Using Patch Seams. S. Avinash Ramakanth, R. Venkatesh Babu, CVPR 2014. [Project Page] [Precomputed results] ARP: Primary Object Segmentation in Videos Based on Region Augmentation and Reduction. Y.J. Koh, C.-S. Kim, CVPR 2017. [Project Page] [Precomputed results] LVO: Learning Video Object Segmentation with Visual Memory. P. Tokmakov, K. Alahari, C. Schmid, ICCV 2017. [Project Page] [Precomputed results] FSEG: FusionSeg: Learning to combine motion and appearance for fully automatic segmentation of generic objects in videos. S. Jain, B. Xiong, K. Grauman, CVPR 2017. [Project Page] [Precomputed results] LMP: Learning Motion Patterns in Videos. P. Tokmakov, K. Alahari, C. Schmid, CVPR 2017. [Project Page] [Precomputed results] SFL: SegFlow: Joint Learning for Video Object Segmentation and Optical Flow. J. Cheng, Y.-H. Tsai, S. Wang, M.-H. Yang, ICCV 2017. [Project Page] [Precomputed results] FST: Fast Object Segmentation in Unconstrained Video. A. Papazoglou, V. Ferrari, ICCV 2013. [Project Page] [Precomputed results] CUT: Motion Trajectory Segmentation via Minimum Cost Multicuts. M. Keuper, B. Andres, T. Brox, ICCV 2015. [Project Page] [Precomputed results] NLC: Video Segmentation by Non-Local Consensus voting. A. Faktor, M. Irani, BMVC 2014. [Project Page] [Precomputed results] MSG: Object segmentation in video: A hierarchical variational approach for turning point trajectories into dense regions. P. Ochs, T. Brox, ICCV 2011. [Project Page] [Precomputed results] KEY: Key-segments for video object segmentation. Y. Lee, J. Kim, K. Grauman, ICCV 2011. [Project Page] [Precomputed results] CVOS: Causal Video Object Segmentation from Persistence of Occlusions. B. Taylor, V. Karasev, S. Soatto, CVPR 2015. [Project Page] [Precomputed results] TRC: Video segmentation by tracing discontinuities in a trajectory embedding. K. Fragkiadaki, G. Zhang, J. Shi, CVPR 2012. [Project Page] [Precomputed results] Instance Embedding Transfer to Unsupervised Video Object Segmentation [Paper]   Result of DAVIS-Challenge 2017 Benchmark 2016----A Benchmark Dataset and Evaluation Methodology for Video Object Segmentation 2016----Clockwork Convnets for Video Semantic Segmentation 2016----MaskTrack ----Learning Video Object Segmentation from Static Images 2017----DAVIS-Challenge-1st----Video Object Segmentation with Re-identification 2017----DAVIS-Challenge-2nd----Lucid Data Dreaming for Multiple Object Tracking 2017----DAVIS-Challenge-3rd----Instance Re-Identification Flow for Video Object Segmentation 2017----DAVIS-Challenge-4th----Multiple-Instance Video Segmentation with Sequence-Specific Object Proposals 2017----DAVIS-Challenge-5th Online Adaptation of Convolutional Neural Networks for the 2017 DAVIS Challenge on Video Object Segmentation 2017----DAVIS-Challenge-6th ----Learning to Segment Instances in Videos with Spatial Propagation Network 2017----DAVIS-Challenge-7th----Some Promising Ideas about Multi-instance Video Segmentation 2017----DAVIS-Challenge-8th----One-Shot Video Object Segmentation with Iterative Online Fine-Tuning 2017----DAVIS-Challenge-9th----Video Object Segmentation using Tracked Object Proposals  Multi-Task Learning Papers:  Multi-Task Learning Using Uncertainty to Weigh Losses for Scene Geometry and Semantics [Paper] Multi-task Learning using Multi-modal Encoder-Decoder Networks with Shared Skip Connections [Paper]  Road Segmentation && Real Time Segmentation Papers:  Deep Semantic Segmentation for Automated Driving: Taxonomy, Roadmap and Challenges [Paper] 2018-arxiv Real-time Semantic Segmentation Comparative Study[Paper][Code] MultiNet: Real-time Joint Semantic Reasoning for Autonomous Driving [Paper] self-driving-car-road-segmentation [Link] Efficient Deep Models for Monocular Road Segmentation[Paper] Semantic Road Segmentation via Multi-scale Ensembles of Learned Features [Paper] Distantly Supervised Road Segmentation [Paper] Deep Fully Convolutional Networks with Random Data Augmentation for Enhanced Generalization in Road Detection [Paper] ICCV-2017 Real-time category-based and general obstacle detection for autonomous driving [Paper] ICCV-2017 FoveaNet: Perspective-aware Urban Scene Parsing [Paper] CVPR-2017 UberNet: Training a universal convolutional neural network for low-, mid-, and high-level vision using diverse datasets and limited memory [Paper]   LinkNet: Exploiting Encoder Representations for Efficient Semantic Segmentation [Paper] ENet: A Deep Neural Network Architecture for Real-Time Semantic Segmentation-2016 [Code-Caffe1][Code-Caffe2] [Paper] [Blog] Efficient Deep Models for Monocular Road Segmentation[Paper] Real-Time Coarse-to-fine Topologically Preserving Segmentation[Paper] ICNet for Real-Time Semantic Segmentation on High-Resolution Images [Paper] Efficient and robust deep networks for semantic segmentation [Paper] NIPSW-2017 Speeding up semantic segmentation for autonomous driving [Paper]   ECCV-2012 Road Scene Segmentation from a Single Image [Paper]  Codes  https://github.com/MarvinTeichmann/MultiNet https://github.com/MarvinTeichmann/KittiSeg https://github.com/vxy10/p5_VehicleDetection_Unet [Keras] https://github.com/ndrplz/self-driving-car https://github.com/mvirgo/MLND-Capstone https://github.com/zhujun98/semantic_segmentation/tree/master/fcn8s_road  Medical Image Semantic Segmentation Papers  Arxiv-2018 Deep learning and its application to medical image segmentation [Paper]   Deep neural networks segment neuronal membranes in electron microscopy images Semantic Image  Segmentation with Deep Learning [Paper] Automatic Liver and Tumor Segmentation of CT and MRI Volumes Using Cascaded Fully Convolutional Neural Networks [Paper] DeepNAT: Deep Convolutional Neural Network for Segmenting Neuroanatomy [Paper] CNN-based Segmentation of Medical Imaging Data [Paper] Deep Retinal Image Understanding (http://www.vision.ee.ethz.ch/~cvlsegmentation/driu/data/paper/DRIU_MICCAI2016.pdf) Model-based segmentation of vertebral bodies from MR images with 3D CNNs Efficient multi-scale 3D CNN with fully connected CRF for accurate brain lesion segmentation U-net: Convolutional networks for biomedical image segmentation 3D U-Net: Learning dense volumetric segmentation from sparse annotation. V-Net: Fully convolutional neural networks for volumetric medical image segmentation.arXiv:1606.04797 The importance of skip connections in biomedical image segmentation Spatial clockwork recurrent neural network for muscle perimysium segmentation NPIS-2015 Parallel multi-dimensional LSTM, with application to fast biomedical volumetric image segmentation Multi-dimensional gated recurrent units for the segmentation of biomedical 3D-data Combining fully convolutional and recurrent neural networks for 3D biomedical image segmentation Recurrent fully convolutional neural networks for multi-slice MRI cardiac segmentation. arXiv:1608.03974 Automatic detection and classification of colorectal polyps by transferring low-level CNN features from nonmedical domain Deep learning for multi-task medical image segmentation in multiple modalities Sub-cortical brain structure segmentation using F-CNNs Segmentation label propagation using deep convolutional neural networks and dense conditional random field Fast fully automatic segmentation of the human placenta from motion corrupted MRI Automatic detection of cerebral microbleeds from MR images via 3D convolutional neural networks Non-uniform patch sampling with deep convolutional neural networks for white matter hyperintensity segmentation A unified framework for automatic wound segmentation and analysis with deep convolutional neural networks Deep 3D convolutional encoder networks with shortcuts for multiscale feature integration applied to Multiple Sclerosis lesion segmentation Brain tumor segmentation using convolutional neural networks in MRI images Deep feature learning for knee cartilage segmentation using a triplanar convolutional neural network Automatic Coronary Calcium Scoring in Cardiac CT Angiography Using Convolutional Neural Networks [Paper] Improving computer-aided detection using convolutional neural networks and random view aggregation [Paper] Pulmonary nodule detection in CT images: false positive reduction using multi-view convolutional networks [Paper]  Codes Part Semantic Segmentation  Look into Person: Self-supervised Structure-sensitive Learning and A New Benchmark for Human Parsing-2017 [Project] [Code-Caffe] [Paper] Deep Learning for Human Part Discovery in Images-2016 [Code-Chainer] [Paper] A CNN Cascade for Landmark Guided Semantic Part Segmentation-2016 [Project] [Paper] Deep Learning for Semantic Part Segmentation With High-level Guidance-2015 [Paper] Neural Activation Constellations-Unsupervised Part Model Discovery with Convolutional Networks-2015 [Paper] Human Parsing with Contextualized Convolutional Neural Network-2015 [Paper] Part detector discovery in deep convolutional neural networks-2014 [Code] [Paper] Hypercolumns for object segmentation and fine-grained localization [Paper]  Clothes Parsing  Looking at Outfit to Parse Clothing-2017 [Paper] Semantic Object Parsing with Local-Global Long Short-Term Memory-2015 [Paper] -A High Performance CRF Model for Clothes Parsing-2014 [Project] [Code] [Dataset] [Paper] Clothing co-parsing by joint image segmentation and labeling-2013 [Project] [Dataset] [Paper] Parsing clothing in fashion photographs-2012 [Project] [Paper]  Popular Methods and Implementations  U-Net [https://arxiv.org/pdf/1505.04597.pdf]Pytorch SegNet [https://arxiv.org/pdf/1511.00561.pdf]Caffe DeepLab [https://arxiv.org/pdf/1606.00915.pdf]Caffe FCN [https://arxiv.org/pdf/1605.06211.pdf]tensorflow ENet [https://arxiv.org/pdf/1606.02147.pdf]Caffe LinkNet [https://arxiv.org/pdf/1707.03718.pdf]Torch DenseNet [https://arxiv.org/pdf/1608.06993.pdf] Tiramisu [https://arxiv.org/pdf/1611.09326.pdf] DilatedNet [https://arxiv.org/pdf/1511.07122.pdf] PixelNet [https://arxiv.org/pdf/1609.06694.pdf]Caffe ICNet [https://arxiv.org/pdf/1704.08545.pdf]Caffe ERFNet [http://www.robesafe.uah.es/personal/eduardo.romera/pdfs/Romera17iv.pdf]Torch RefineNet [https://arxiv.org/pdf/1611.06612.pdf]tensorflow PSPNet [https://arxiv.org/pdf/1612.01105.pdf,https://hszhao.github.io/projects/pspnet/]Caffe Dilated convolution [https://arxiv.org/pdf/1511.07122.pdf]Caffe DeconvNet [https://arxiv.org/pdf/1505.04366.pdf]Caffe FRRN [https://arxiv.org/pdf/1611.08323.pdf]Lasagne GCN [https://arxiv.org/pdf/1703.02719.pdf]PyTorch LRR [https://arxiv.org/pdf/1605.02264.pdf]Matconvnet DUC, HDC [https://arxiv.org/pdf/1702.08502.pdf]PyTorch MultiNet [https://arxiv.org/pdf/1612.07695.pdf] tensorflow1tensorflow2 Segaware [https://arxiv.org/pdf/1708.04607.pdf]Caffe Semantic Segmentation using Adversarial Networks [https://arxiv.org/pdf/1611.08408.pdf] [Chainer](+ https://github.com/oyam/Semantic-Segmentation-using-Adversarial-Networks ) In-Place Activated BatchNorm:obtain #1 positions [https://arxiv.org/abs/1712.02616] Pytorch  Annotation Tools:  https://github.com/AKSHAYUBHAT/ImageSegmentation https://github.com/kyamagu/js-segment-annotator https://github.com/CSAILVision/LabelMeAnnotationTool https://github.com/seanbell/opensurfaces-segmentation-ui https://github.com/lzx1413/labelImgPlus https://github.com/wkentaro/labelme  Distinguished Researchers & Teams:  Liang-Chieh (Jay) Chen Deeplab-Google Jianping Shi PSPNet Kaiming He Mask-RCNN Ming-Ming Cheng Joachim M. Buhmann Jifeng Dai FCIS-MSRA Alex Kendall SegNet  Results:  MSRC-21 Cityscapes VOC2012  Reference https://github.com/nightrome/really-awesome-semantic-segmentation https://github.com/mrgloom/awesome-semantic-segmentation "
4,HRNet/HRNet-Semantic-Segmentation,2.4k,https://github.com/HRNet/HRNet-Semantic-Segmentation,"Updated on Jul 20, 2021","High-resolution networks and Segmentation Transformer for Semantic Segmentation Branches News Introduction Segmentation models Quick start Install Data preparation Train and Test PyTorch Version Differences Training Testing Other applications of HRNet Citation Reference Acknowledgement      README.md           High-resolution networks and Segmentation Transformer for Semantic Segmentation Branches  This is the implementation for HRNet + OCR. The PyTroch 1.1 version ia available here. The PyTroch 0.4.1 version is available here.  News   [2021/05/04] We rephrase the OCR approach as Segmentation Transformer pdf. We will provide the updated implementation soon.   [2021/02/16] Based on the PaddleClas ImageNet pretrained weights, we achieve 83.22% on Cityscapes val, 59.62% on PASCAL-Context val (new SOTA), 45.20% on COCO-Stuff val (new SOTA), 58.21% on LIP val and  47.98% on ADE20K val. Please checkout openseg.pytorch for more details.   [2020/08/16] MMSegmentation has supported our HRNet + OCR.   [2020/07/20] The researchers from AInnovation have achieved Rank#1 on ADE20K Leaderboard via training our HRNet + OCR with a semi-supervised learning scheme. More details are in their Technical Report.   [2020/07/09] Our paper is accepted by ECCV 2020: Object-Contextual Representations for Semantic Segmentation. Notably, the reseachers from Nvidia set a new state-of-the-art performance on Cityscapes leaderboard: 85.4% via combining our HRNet + OCR with a new hierarchical mult-scale attention scheme.   [2020/03/13] Our paper is accepted by TPAMI: Deep High-Resolution Representation Learning for Visual Recognition.   HRNet + OCR + SegFix: Rank #1 (84.5) in Cityscapes leaderboard. OCR: object contextual represenations pdf. HRNet + OCR is reproduced here.   Thanks Google and UIUC researchers. A modified HRNet combined with semantic and instance multi-scale context achieves SOTA panoptic segmentation result on the Mapillary Vista challenge. See the paper.   Small HRNet models for Cityscapes segmentation. Superior to MobileNetV2Plus ....   Rank #1 (83.7) in Cityscapes leaderboard. HRNet combined with an extension of object context   Pytorch-v1.1 and the official Sync-BN supported. We have reproduced the cityscapes results on the new codebase. Please check the pytorch-v1.1 branch.   Introduction This is the official code of high-resolution representations for Semantic Segmentation. We augment the HRNet with a very simple segmentation head shown in the figure below. We aggregate the output representations at four different resolutions, and then use a 1x1 convolutions to fuse these representations. The output representations is fed into the classifier. We evaluate our methods on three datasets, Cityscapes, PASCAL-Context and LIP.  Besides, we further combine HRNet with Object Contextual Representation and achieve higher performance on the three datasets. The code of HRNet+OCR is contained in this branch. We illustrate the overall framework of OCR in the Figure and the equivalent Transformer pipelines:   Segmentation models The models are initialized by the weights pretrained on the ImageNet. ''Paddle'' means the results are based on PaddleCls pretrained HRNet models. You can download the pretrained models from  https://github.com/HRNet/HRNet-Image-Classification. Slightly different, we use align_corners = True for upsampling in HRNet.  Performance on the Cityscapes dataset. The models are trained and tested with the input size of 512x1024 and 1024x2048 respectively. If multi-scale testing is used, we adopt scales: 0.5,0.75,1.0,1.25,1.5,1.75.     model Train Set Test Set OHEM Multi-scale Flip mIoU Link     HRNetV2-W48 Train Val No No No 80.9 Github/BaiduYun(Access Code:pmix)   HRNetV2-W48 + OCR Train Val No No No 81.6 Github/BaiduYun(Access Code:fa6i)   HRNetV2-W48 + OCR Train + Val Test No Yes Yes 82.3 Github/BaiduYun(Access Code:ycrk)   HRNetV2-W48 (Paddle) Train Val No No No 81.6 ---   HRNetV2-W48 + OCR (Paddle) Train Val No No No --- ---   HRNetV2-W48 + OCR (Paddle) Train + Val Test No Yes Yes --- ---     Performance on the LIP dataset. The models are trained and tested with the input size of 473x473.     model OHEM Multi-scale Flip mIoU Link     HRNetV2-W48 No No Yes 55.83 Github/BaiduYun(Access Code:fahi)   HRNetV2-W48 + OCR No No Yes 56.48 Github/BaiduYun(Access Code:xex2)   HRNetV2-W48 (Paddle) No No Yes --- ---   HRNetV2-W48 + OCR (Paddle) No No Yes --- ---    Note Currently we could only reproduce HRNet+OCR results on LIP dataset with PyTorch 0.4.1.  Performance on the PASCAL-Context dataset. The models are trained and tested with the input size of 520x520. If multi-scale testing is used, we adopt scales: 0.5,0.75,1.0,1.25,1.5,1.75,2.0 (the same as EncNet, DANet etc.).     model num classes OHEM Multi-scale Flip mIoU Link     HRNetV2-W48 59 classes No Yes Yes 54.1 Github/BaiduYun(Access Code:wz6v)   HRNetV2-W48 + OCR 59 classes No Yes Yes 56.2 Github/BaiduYun(Access Code:yyxh)   HRNetV2-W48 60 classes No Yes Yes 48.3 OneDrive/BaiduYun(Access Code:9uf8)   HRNetV2-W48 + OCR 60 classes No Yes Yes 50.1 Github/BaiduYun(Access Code:gtkb)   HRNetV2-W48 (Paddle) 59 classes No Yes Yes --- ---   HRNetV2-W48 (Paddle) 60 classes No Yes Yes --- ---   HRNetV2-W48 + OCR (Paddle) 59 classes No Yes Yes --- ---   HRNetV2-W48 + OCR (Paddle) 60 classes No Yes Yes --- ---     Performance on the COCO-Stuff dataset. The models are trained and tested with the input size of 520x520. If multi-scale testing is used, we adopt scales: 0.5,0.75,1.0,1.25,1.5,1.75,2.0 (the same as EncNet, DANet etc.).     model OHEM Multi-scale Flip mIoU Link     HRNetV2-W48 Yes No No 36.2 Github/BaiduYun(Access Code:92gw)   HRNetV2-W48 + OCR Yes No No 39.7 Github/BaiduYun(Access Code:sjc4)   HRNetV2-W48 Yes Yes Yes 37.9 Github/BaiduYun(Access Code:92gw)   HRNetV2-W48 + OCR Yes Yes Yes 40.6 Github/BaiduYun(Access Code:sjc4)   HRNetV2-W48 (Paddle) Yes No No --- ---   HRNetV2-W48 + OCR (Paddle) Yes No No --- ---   HRNetV2-W48 (Paddle) Yes Yes Yes --- ---   HRNetV2-W48 + OCR (Paddle) Yes Yes Yes --- ---     Performance on the ADE20K dataset. The models are trained and tested with the input size of 520x520. If multi-scale testing is used, we adopt scales: 0.5,0.75,1.0,1.25,1.5,1.75,2.0 (the same as EncNet, DANet etc.).     model OHEM Multi-scale Flip mIoU Link     HRNetV2-W48 Yes No No 43.1 Github/BaiduYun(Access Code:f6xf)   HRNetV2-W48 + OCR Yes No No 44.5 Github/BaiduYun(Access Code:peg4)   HRNetV2-W48 Yes Yes Yes 44.2 Github/BaiduYun(Access Code:f6xf)   HRNetV2-W48 + OCR Yes Yes Yes 45.5 Github/BaiduYun(Access Code:peg4)   HRNetV2-W48 (Paddle) Yes No No --- ---   HRNetV2-W48 + OCR (Paddle) Yes No No --- ---   HRNetV2-W48 (Paddle) Yes Yes Yes --- ---   HRNetV2-W48 + OCR (Paddle) Yes Yes Yes --- ---    Quick start Install  For LIP dataset, install PyTorch=0.4.1 following the official instructions. For Cityscapes and PASCAL-Context, we use PyTorch=1.1.0. git clone https://github.com/HRNet/HRNet-Semantic-Segmentation $SEG_ROOT Install dependencies: pip install -r requirements.txt  If you want to train and evaluate our models on PASCAL-Context, you need to install details. pip install git+https://github.com/zhanghang1989/detail-api.git#subdirectory=PythonAPI          Data preparation You need to download the Cityscapes, LIP and PASCAL-Context datasets. Your directory tree should be look like this: $SEG_ROOT/data ├── cityscapes │   ├── gtFine │   │   ├── test │   │   ├── train │   │   └── val │   └── leftImg8bit │       ├── test │       ├── train │       └── val ├── lip │   ├── TrainVal_images │   │   ├── train_images │   │   └── val_images │   └── TrainVal_parsing_annotations │       ├── train_segmentations │       ├── train_segmentations_reversed │       └── val_segmentations ├── pascal_ctx │   ├── common │   ├── PythonAPI │   ├── res │   └── VOCdevkit │       └── VOC2010 ├── cocostuff │   ├── train │   │   ├── image │   │   └── label │   └── val │       ├── image │       └── label ├── ade20k │   ├── train │   │   ├── image │   │   └── label │   └── val │       ├── image │       └── label ├── list │   ├── cityscapes │   │   ├── test.lst │   │   ├── trainval.lst │   │   └── val.lst │   ├── lip │   │   ├── testvalList.txt │   │   ├── trainList.txt │   │   └── valList.txt          Train and Test PyTorch Version Differences Note that the codebase supports both PyTorch 0.4.1 and 1.1.0, and they use different command for training. In the following context, we use $PY_CMD to denote different startup command. # For PyTorch 0.4.1 PY_CMD=""python"" # For PyTorch 1.1.0 PY_CMD=""python -m torch.distributed.launch --nproc_per_node=4""          e.g., when training on Cityscapes, we use PyTorch 1.1.0. So the command $PY_CMD tools/train.py --cfg experiments/cityscapes/seg_hrnet_ocr_w48_train_512x1024_sgd_lr1e-2_wd5e-4_bs_12_epoch484.yaml          indicates python -m torch.distributed.launch --nproc_per_node=4 tools/train.py --cfg experiments/cityscapes/seg_hrnet_ocr_w48_train_512x1024_sgd_lr1e-2_wd5e-4_bs_12_epoch484.yaml          Training Just specify the configuration file for tools/train.py. For example, train the HRNet-W48 on Cityscapes with a batch size of 12 on 4 GPUs: $PY_CMD tools/train.py --cfg experiments/cityscapes/seg_hrnet_w48_train_512x1024_sgd_lr1e-2_wd5e-4_bs_12_epoch484.yaml          For example, train the HRNet-W48 + OCR on Cityscapes with a batch size of 12 on 4 GPUs: $PY_CMD tools/train.py --cfg experiments/cityscapes/seg_hrnet_ocr_w48_train_512x1024_sgd_lr1e-2_wd5e-4_bs_12_epoch484.yaml          Note that we only reproduce HRNet+OCR on LIP dataset using PyTorch 0.4.1. So we recommend to use PyTorch 0.4.1 if you want to train on LIP dataset. Testing For example, evaluating HRNet+OCR on the Cityscapes validation set with multi-scale and flip testing: python tools/test.py --cfg experiments/cityscapes/seg_hrnet_ocr_w48_train_512x1024_sgd_lr1e-2_wd5e-4_bs_12_epoch484.yaml \                      TEST.MODEL_FILE hrnet_ocr_cs_8162_torch11.pth \                      TEST.SCALE_LIST 0.5,0.75,1.0,1.25,1.5,1.75 \                      TEST.FLIP_TEST True          Evaluating HRNet+OCR on the Cityscapes test set with multi-scale and flip testing: python tools/test.py --cfg experiments/cityscapes/seg_hrnet_ocr_w48_train_512x1024_sgd_lr1e-2_wd5e-4_bs_12_epoch484.yaml \                      DATASET.TEST_SET list/cityscapes/test.lst \                      TEST.MODEL_FILE hrnet_ocr_trainval_cs_8227_torch11.pth \                      TEST.SCALE_LIST 0.5,0.75,1.0,1.25,1.5,1.75 \                      TEST.FLIP_TEST True          Evaluating HRNet+OCR on the PASCAL-Context validation set with multi-scale and flip testing: python tools/test.py --cfg experiments/pascal_ctx/seg_hrnet_ocr_w48_cls59_520x520_sgd_lr1e-3_wd1e-4_bs_16_epoch200.yaml \                      DATASET.TEST_SET testval \                      TEST.MODEL_FILE hrnet_ocr_pascal_ctx_5618_torch11.pth \                      TEST.SCALE_LIST 0.5,0.75,1.0,1.25,1.5,1.75,2.0 \                      TEST.FLIP_TEST True          Evaluating HRNet+OCR on the LIP validation set with flip testing: python tools/test.py --cfg experiments/lip/seg_hrnet_w48_473x473_sgd_lr7e-3_wd5e-4_bs_40_epoch150.yaml \                      DATASET.TEST_SET list/lip/testvalList.txt \                      TEST.MODEL_FILE hrnet_ocr_lip_5648_torch04.pth \                      TEST.FLIP_TEST True \                      TEST.NUM_SAMPLES 0          Evaluating HRNet+OCR on the COCO-Stuff validation set with multi-scale and flip testing: python tools/test.py --cfg experiments/cocostuff/seg_hrnet_ocr_w48_520x520_ohem_sgd_lr1e-3_wd1e-4_bs_16_epoch110.yaml \                      DATASET.TEST_SET list/cocostuff/testval.lst \                      TEST.MODEL_FILE hrnet_ocr_cocostuff_3965_torch04.pth \                      TEST.SCALE_LIST 0.5,0.75,1.0,1.25,1.5,1.75,2.0 \                      TEST.MULTI_SCALE True TEST.FLIP_TEST True          Evaluating HRNet+OCR on the ADE20K validation set with multi-scale and flip testing: python tools/test.py --cfg experiments/ade20k/seg_hrnet_ocr_w48_520x520_ohem_sgd_lr2e-2_wd1e-4_bs_16_epoch120.yaml \                      DATASET.TEST_SET list/ade20k/testval.lst \                      TEST.MODEL_FILE hrnet_ocr_ade20k_4451_torch04.pth \                      TEST.SCALE_LIST 0.5,0.75,1.0,1.25,1.5,1.75,2.0 \                      TEST.MULTI_SCALE True TEST.FLIP_TEST True          Other applications of HRNet  Human pose estimation Image Classification Object detection Facial landmark detection  Citation If you find this work or code is helpful in your research, please cite: @inproceedings{SunXLW19,   title={Deep High-Resolution Representation Learning for Human Pose Estimation},   author={Ke Sun and Bin Xiao and Dong Liu and Jingdong Wang},   booktitle={CVPR},   year={2019} }  @article{WangSCJDZLMTWLX19,   title={Deep High-Resolution Representation Learning for Visual Recognition},   author={Jingdong Wang and Ke Sun and Tianheng Cheng and           Borui Jiang and Chaorui Deng and Yang Zhao and Dong Liu and Yadong Mu and           Mingkui Tan and Xinggang Wang and Wenyu Liu and Bin Xiao},   journal={TPAMI},   year={2019} }  @article{YuanCW19,   title={Object-Contextual Representations for Semantic Segmentation},   author={Yuhui Yuan and Xilin Chen and Jingdong Wang},   booktitle={ECCV},   year={2020} }           Reference [1] Deep High-Resolution Representation Learning for Visual Recognition. Jingdong Wang, Ke Sun, Tianheng Cheng, Borui Jiang, Chaorui Deng, Yang Zhao, Dong Liu, Yadong Mu, Mingkui Tan, Xinggang Wang, Wenyu Liu, Bin Xiao. Accepted by TPAMI.  download [2] Object-Contextual Representations for Semantic Segmentation. Yuhui Yuan, Xilin Chen, Jingdong Wang. download Acknowledgement We adopt sync-bn implemented by InplaceABN for PyTorch 0.4.1 experiments and the official sync-bn provided by PyTorch for PyTorch 1.10 experiments. We adopt data precosessing on the PASCAL-Context dataset, implemented by PASCAL API. "
5,NVIDIA/semantic-segmentation,1.5k,https://github.com/NVIDIA/semantic-segmentation,"Updated on Jul 26, 2021","Paper | YouTube  | Cityscapes Score Installation Download Weights Download/Prepare Data Running the code Run inference on Cityscapes Run inference on Mapillary Dump images for Cityscapes Run inference and dump images on a folder of images Train a model Train SOTA default train-val split      README.md           Paper | YouTube  | Cityscapes Score Pytorch implementation of our paper Hierarchical Multi-Scale Attention for Semantic Segmentation. Please refer to the sdcnet branch if you are looking for the code corresponding to Improving Semantic Segmentation via Video Prediction and Label Relaxation. Installation  The code is tested with pytorch 1.3 and python 3.6 You can use ./Dockerfile to build an image.  Download Weights  Create a directory where you can keep large files. Ideally, not in this directory.    > mkdir <large_asset_dir>            Update __C.ASSETS_PATH in config.py to point at that directory __C.ASSETS_PATH=<large_asset_dir>   Download pretrained weights from google drive and put into <large_asset_dir>/seg_weights   Download/Prepare Data If using Cityscapes, download Cityscapes data, then update config.py to set the path: __C.DATASET.CITYSCAPES_DIR=<path_to_cityscapes>           Download Autolabelled-Data from google drive  If using Cityscapes Autolabelled Images, download Cityscapes data, then update config.py to set the path: __C.DATASET.CITYSCAPES_CUSTOMCOARSE=<path_to_cityscapes>          If using Mapillary, download Mapillary data, then update config.py to set the path: __C.DATASET.MAPILLARY_DIR=<path_to_mapillary>          Running the code The instructions below make use of a tool called runx, which we find useful to help automate experiment running and summarization. For more information about this tool, please see runx. In general, you can either use the runx-style commandlines shown below. Or you can call python train.py <args ...> directly if you like. Run inference on Cityscapes Dry run: > python -m runx.runx scripts/eval_cityscapes.yml -i -n          This will just print out the command but not run. It's a good way to inspect the commandline. Real run: > python -m runx.runx scripts/eval_cityscapes.yml -i          The reported IOU should be 86.92. This evaluates with scales of 0.5, 1.0. and 2.0. You will find evaluation results in ./logs/eval_cityscapes/... Run inference on Mapillary > python -m runx.runx scripts/eval_mapillary.yml -i          The reported IOU should be 61.05. Note that this must be run on a 32GB node and the use of 'O3' mode for amp is critical in order to avoid GPU out of memory. Results in logs/eval_mapillary/... Dump images for Cityscapes > python -m runx.runx scripts/dump_cityscapes.yml -i          This will dump network output and composited images from running evaluation with the Cityscapes validation set. Run inference and dump images on a folder of images > python -m runx.runx scripts/dump_folder.yml -i          You should end up seeing images that look like the following:  Train a model Train cityscapes, using HRNet + OCR + multi-scale attention with fine data and mapillary-pretrained model > python -m runx.runx scripts/train_cityscapes.yml -i          The first time this command is run, a centroid file has to be built for the dataset. It'll take about 10 minutes. The centroid file is used during training to know how to sample from the dataset in a class-uniform way. This training run should deliver a model that achieves 84.7 IOU. Train SOTA default train-val split > python -m runx.runx  scripts/train_cityscapes_sota.yml -i          Again, use -n to do a dry run and just print out the command. This should result in a model with 86.8 IOU. If you run out of memory, try to lower the crop size or turn off rmi_loss. "
6,Tramac/awesome-semantic-segmentation-pytorch,2k,https://github.com/Tramac/awesome-semantic-segmentation-pytorch,"Updated on Sep 15, 2021","Semantic Segmentation on PyTorch Installation Usage Train Evaluation Demo Support Model Dataset Result Overfitting Test To Do References      README.md           Semantic Segmentation on PyTorch English | 简体中文    This project aims at providing a concise, easy-to-use, modifiable reference implementation for semantic segmentation models using PyTorch.  Installation # semantic-segmentation-pytorch dependencies pip install ninja tqdm  # follow PyTorch installation in https://pytorch.org/get-started/locally/ conda install pytorch torchvision -c pytorch  # install PyTorch Segmentation git clone https://github.com/Tramac/awesome-semantic-segmentation-pytorch.git           Usage Train   Single GPU training  # for example, train fcn32_vgg16_pascal_voc: python train.py --model fcn32s --backbone vgg16 --dataset pascal_voc --lr 0.0001 --epochs 50            Multi-GPU training  # for example, train fcn32_vgg16_pascal_voc with 4 GPUs: export NGPUS=4 python -m torch.distributed.launch --nproc_per_node=$NGPUS train.py --model fcn32s --backbone vgg16 --dataset pascal_voc --lr 0.0001 --epochs 50           Evaluation   Single GPU evaluating  # for example, evaluate fcn32_vgg16_pascal_voc python eval.py --model fcn32s --backbone vgg16 --dataset pascal_voc            Multi-GPU evaluating  # for example, evaluate fcn32_vgg16_pascal_voc with 4 GPUs: export NGPUS=4 python -m torch.distributed.launch --nproc_per_node=$NGPUS eval.py --model fcn32s --backbone vgg16 --dataset pascal_voc           Demo cd ./scripts #for new users: python demo.py --model fcn32s_vgg16_voc --input-pic ../tests/test_img.jpg #you should add 'test.jpg' by yourself python demo.py --model fcn32s_vgg16_voc --input-pic ../datasets/test.jpg           .{SEG_ROOT} ├── scripts │   ├── demo.py │   ├── eval.py │   └── train.py           Support Model  FCN ENet PSPNet ICNet DeepLabv3 DeepLabv3+ DenseASPP EncNet BiSeNet PSANet DANet OCNet CGNet ESPNetv2 DUNet(DUpsampling) FastFCN(JPU) LEDNet Fast-SCNN LightSeg DFANet  DETAILS for model & backbone. .{SEG_ROOT} ├── core │   ├── models │   │   ├── bisenet.py │   │   ├── danet.py │   │   ├── deeplabv3.py │   │   ├── deeplabv3+.py │   │   ├── denseaspp.py │   │   ├── dunet.py │   │   ├── encnet.py │   │   ├── fcn.py │   │   ├── pspnet.py │   │   ├── icnet.py │   │   ├── enet.py │   │   ├── ocnet.py │   │   ├── psanet.py │   │   ├── cgnet.py │   │   ├── espnet.py │   │   ├── lednet.py │   │   ├── dfanet.py │   │   ├── ......           Dataset You can run script to download dataset, such as: cd ./core/data/downloader python ade20k.py --download-dir ../datasets/ade              Dataset training set validation set testing set     VOC2012 1464 1449 ✘   VOCAug 11355 2857 ✘   ADK20K 20210 2000 ✘   Cityscapes 2975 500 ✘   COCO      SBU-shadow 4085 638 ✘   LIP(Look into Person) 30462 10000 10000    .{SEG_ROOT} ├── core │   ├── data │   │   ├── dataloader │   │   │   ├── ade.py │   │   │   ├── cityscapes.py │   │   │   ├── mscoco.py │   │   │   ├── pascal_aug.py │   │   │   ├── pascal_voc.py │   │   │   ├── sbu_shadow.py │   │   └── downloader │   │       ├── ade20k.py │   │       ├── cityscapes.py │   │       ├── mscoco.py │   │       ├── pascal_voc.py │   │       └── sbu_shadow.py           Result  PASCAL VOC 2012     Methods Backbone TrainSet EvalSet crops_size epochs JPU Mean IoU pixAcc     FCN32s vgg16 train val 480 60 ✘ 47.50 85.39   FCN16s vgg16 train val 480 60 ✘ 49.16 85.98   FCN8s vgg16 train val 480 60 ✘ 48.87 85.02   FCN32s resnet50 train val 480 50 ✘ 54.60 88.57   PSPNet resnet50 train val 480 60 ✘ 63.44 89.78   DeepLabv3 resnet50 train val 480 60 ✘ 60.15 88.36    Note: lr=1e-4, batch_size=4, epochs=80. Overfitting Test See TEST for details. .{SEG_ROOT} ├── tests │   └── test_model.py           To Do   add train script  remove syncbn  train & evaluate  test distributed training  fix syncbn (Why SyncBN?)  add distributed (How DIST?)  References  PyTorch-Encoding maskrcnn-benchmark gloun-cv imagenet "
7,zijundeng/pytorch-semantic-segmentation,1.6k,https://github.com/zijundeng/pytorch-semantic-segmentation,"Updated on Oct 25, 2019","PyTorch for Semantic Segmentation Models Requirement Preparation TODO      README.md           PyTorch for Semantic Segmentation This repository contains some models for semantic segmentation and the pipeline of training and testing models, implemented in PyTorch Models  Vanilla FCN: FCN32, FCN16, FCN8, in the versions of VGG, ResNet and DenseNet respectively (Fully convolutional networks for semantic segmentation) U-Net (U-net: Convolutional networks for biomedical image segmentation) SegNet (Segnet: A deep convolutional encoder-decoder architecture for image segmentation) PSPNet (Pyramid scene parsing network) GCN (Large Kernel Matters) DUC, HDC (understanding convolution for semantic segmentation)  Requirement  PyTorch 0.2.0 TensorBoard for PyTorch. Here  to install Some other libraries (find what you miss when running the code :-P)  Preparation  Go to models directory and set the path of pretrained models in config.py Go to datasets directory and do following the README  TODO  DeepLab v3 RefineNet More dataset (e.g. ADE) "
8,udacity/CarND-Semantic-Segmentation,177,https://github.com/udacity/CarND-Semantic-Segmentation,"Updated on Nov 25, 2021","Deprecated Repository Semantic Segmentation Introduction Setup GPU Frameworks and Packages Dataset Start Implement Run Example Outputs Submission Tips Why Layer 3, 4 and 7? Optional sections Using GitHub and Creating Effective READMEs      README.md           Deprecated Repository This repository is deprecated. Currently enrolled learners, if any, can:  Utilize the https://knowledge.udacity.com/ forum to seek help on content-specific issues. Submit a support ticket if (learners are) blocked due to other reasons.  Semantic Segmentation Introduction In this project, you'll label the pixels of a road in images using a Fully Convolutional Network (FCN). Setup GPU main.py will check to make sure you are using GPU - if you don't have a GPU on your system, you can use AWS or another cloud computing platform. Frameworks and Packages Make sure you have the following is installed:  Python 3 TensorFlow NumPy SciPy  You may also need Python Image Library (PIL) for SciPy's imresize function. Dataset Download the Kitti Road dataset from here.  Extract the dataset in the data folder.  This will create the folder data_road with all the training a test images. Start Implement Implement the code in the main.py module indicated by the ""TODO"" comments. The comments indicated with ""OPTIONAL"" tag are not required to complete. Run Run the following command to run the project: python main.py           Note: If running this in Jupyter Notebook system messages, such as those regarding test status, may appear in the terminal rather than the notebook. Example Outputs Here are examples of a sufficient vs. insufficient output from a trained network:    Sufficient Result Insufficient Result          Submission  Ensure you've passed all the unit tests. Ensure you pass all points on the rubric. Submit the following in a zip file.   helper.py main.py project_tests.py Newest inference images from runs folder  (all images from the most recent run)  Tips  The link for the frozen VGG16 model is hardcoded into helper.py.  The model can be found here. The model is not vanilla VGG16, but a fully convolutional version, which already contains the 1x1 convolutions to replace the fully connected layers. Please see this post for more information.  A summary of additional points, follow. The original FCN-8s was trained in stages. The authors later uploaded a version that was trained all at once to their GitHub repo.  The version in the GitHub repo has one important difference: The outputs of pooling layers 3 and 4 are scaled before they are fed into the 1x1 convolutions.  As a result, some students have found that the model learns much better with the scaling layers included. The model may not converge substantially faster, but may reach a higher IoU and accuracy. When adding l2-regularization, setting a regularizer in the arguments of the tf.layers is not enough. Regularization loss terms must be manually added to your loss function. otherwise regularization is not implemented.  Why Layer 3, 4 and 7? In main.py, you'll notice that layers 3, 4 and 7 of VGG16 are utilized in creating skip layers for a fully convolutional network. The reasons for this are contained in the paper Fully Convolutional Networks for Semantic Segmentation. In section 4.3, and further under header ""Skip Architectures for Segmentation"" and Figure 3, they note these provided for 8x, 16x and 32x upsampling, respectively. Using each of these in their FCN-8s was the most effective architecture they found. Optional sections Within main.py, there are a few optional sections you can also choose to implement, but are not required for the project.  Train and perform inference on the Cityscapes Dataset. Note that the project_tests.py is not currently set up to also unit test for this alternate dataset, and helper.py will also need alterations, along with changing num_classes and input_shape in main.py. Cityscapes is a much more extensive dataset, with segmentation of 30 different classes (compared to road vs. not road on KITTI) on either 5,000 finely annotated images or 20,000 coarsely annotated images. Add image augmentation. You can use some of the augmentation techniques you may have used on Traffic Sign Classification or Behavioral Cloning, or look into additional methods for more robust training! Apply the trained model to a video. This project only involves performing inference on a set of test images, but you can also try to utilize it on a full video.  Using GitHub and Creating Effective READMEs If you are unfamiliar with GitHub , Udacity has a brief GitHub tutorial to get you started. Udacity also provides a more detailed free course on git and GitHub. To learn about REAMDE files and Markdown, Udacity provides a free course on READMEs, as well. GitHub also provides a tutorial about creating Markdown files. "
9,open-mmlab/mmsegmentation,2.9k,https://github.com/open-mmlab/mmsegmentation,Updated 2 days ago,"Introduction Major features License Changelog Benchmark and model zoo Installation Get Started Citation Contributing Acknowledgement Projects in OpenMMLab      README.md                       Documentation: https://mmsegmentation.readthedocs.io/ English | 简体中文 Introduction MMSegmentation is an open source semantic segmentation toolbox based on PyTorch. It is a part of the OpenMMLab project. The master branch works with PyTorch 1.5+.  Major features   Unified Benchmark We provide a unified benchmark toolbox for various semantic segmentation methods.   Modular Design We decompose the semantic segmentation framework into different components and one can easily construct a customized semantic segmentation framework by combining different modules.   Support of multiple methods out of box The toolbox directly supports popular and contemporary semantic segmentation frameworks, e.g. PSPNet, DeepLabV3, PSANet, DeepLabV3+, etc.   High efficiency The training speed is faster than or comparable to other codebases.   License This project is released under the Apache 2.0 license. Changelog v0.20.2 was released in 12/15/2021. Please refer to changelog.md for details and release history. Benchmark and model zoo Results and models are available in the model zoo. Supported backbones:   ResNet (CVPR'2016)  ResNeXt (CVPR'2017)  HRNet (CVPR'2019)  ResNeSt (ArXiv'2020)  MobileNetV2 (CVPR'2018)  MobileNetV3 (ICCV'2019)  Vision Transformer (ICLR'2021)  Swin Transformer (ICCV'2021)  Twins (NeurIPS'2021)  Supported methods:   FCN (CVPR'2015/TPAMI'2017)  ERFNet (T-ITS'2017)  UNet (MICCAI'2016/Nat. Methods'2019)  PSPNet (CVPR'2017)  DeepLabV3 (ArXiv'2017)  BiSeNetV1 (ECCV'2018)  PSANet (ECCV'2018)  DeepLabV3+ (CVPR'2018)  UPerNet (ECCV'2018)  ICNet (ECCV'2018)  NonLocal Net (CVPR'2018)  EncNet (CVPR'2018)  Semantic FPN (CVPR'2019)  DANet (CVPR'2019)  APCNet (CVPR'2019)  EMANet (ICCV'2019)  CCNet (ICCV'2019)  DMNet (ICCV'2019)  ANN (ICCV'2019)  GCNet (ICCVW'2019/TPAMI'2020)  FastFCN (ArXiv'2019)  Fast-SCNN (ArXiv'2019)  ISANet (ArXiv'2019/IJCV'2021)  OCRNet (ECCV'2020)  DNLNet (ECCV'2020)  PointRend (CVPR'2020)  CGNet (TIP'2020)  BiSeNetV2 (IJCV'2021)  STDC (CVPR'2021)  SETR (CVPR'2021)  DPT (ArXiv'2021)  SegFormer (NeurIPS'2021)  Supported datasets:   Cityscapes  PASCAL VOC  ADE20K  Pascal Context  COCO-Stuff 10k  COCO-Stuff 164k  CHASE_DB1  DRIVE  HRF  STARE  Dark Zurich  Nighttime Driving  LoveDA  Installation Please refer to get_started.md for installation and dataset_prepare.md for dataset preparation. Get Started Please see train.md and inference.md for the basic usage of MMSegmentation. There are also tutorials for customizing dataset, designing data pipeline, customizing modules, and customizing runtime. We also provide many training tricks for better training and useful tools for deployment. A Colab tutorial is also provided. You may preview the notebook here or directly run on Colab. Citation If you find this project useful in your research, please consider cite: @misc{mmseg2020,     title={{MMSegmentation}: OpenMMLab Semantic Segmentation Toolbox and Benchmark},     author={MMSegmentation Contributors},     howpublished = {\url{https://github.com/open-mmlab/mmsegmentation}},     year={2020} }          Contributing We appreciate all contributions to improve MMSegmentation. Please refer to CONTRIBUTING.md for the contributing guideline. Acknowledgement MMSegmentation is an open source project that welcome any contribution and feedback. We wish that the toolbox and benchmark could serve the growing research community by providing a flexible as well as standardized toolkit to reimplement existing methods and develop their own new semantic segmentation methods. Projects in OpenMMLab  MMCV: OpenMMLab foundational library for computer vision. MMClassification: OpenMMLab image classification toolbox and benchmark. MMDetection: OpenMMLab detection toolbox and benchmark. MMDetection3D: OpenMMLab's next-generation platform for general 3D object detection. MMSegmentation: OpenMMLab semantic segmentation toolbox and benchmark. MMAction2: OpenMMLab's next-generation action understanding toolbox and benchmark. MMTracking: OpenMMLab video perception toolbox and benchmark. MMPose: OpenMMLab pose estimation toolbox and benchmark. MMEditing: OpenMMLab image and video editing toolbox. MMOCR: A Comprehensive Toolbox for Text Detection, Recognition and Understanding. MMGeneration: A powerful toolkit for generative models. MIM: MIM Installs OpenMMLab Packages. MMFlow: OpenMMLab optical flow toolbox and benchmark. MMFewShot: OpenMMLab few shot learning toolbox and benchmark. MMHuman3D: OpenMMLab 3D human parametric model toolbox and benchmark. MMSelfSup: OpenMMLab self-supervised learning toolbox and benchmark. MMRazor: OpenMMLab Model Compression Toolbox and Benchmark. MMDeploy: OpenMMLab Model Deployment Framework. "
10,Hitachi-Automotive-And-Industry-Lab/semantic-segmentation-editor,1.1k,https://github.com/Hitachi-Automotive-And-Industry-Lab/semantic-segmentation-editor,Updated 5 days ago,"Semantic Segmentation Editor Bitmap Image Editor PCD Point Cloud Editor How to run Using Docker Compose Running from source Install Meteor (OSX or Linux) Download and unzip latest version from here Start the application Configuration File: settings.json How to use Using the bitmap image editor Polygon Drawing Tool (P) Magic Tool (A) Manipulation Tool (Alt) Cutting/Expanding Tool (C) Contiguous Polygon Tool (F) Using the point cloud editor PCD support API Endpoints      README.md           Semantic Segmentation Editor A web based labeling tool for creating AI training data sets (2D and 3D). The tool has been developed in the context of autonomous driving research. It supports images (.jpg or .png) and point clouds (.pcd). It is a Meteor app developed with React, Paper.js and three.js. Latest changes  Version 1.5: Provide a Docker image and update to Meteor 1.10 Version 1.4: Support for RGB pointclouds (thanks @Gekk0r) Version 1.3: Improve pointcloud labeling: bug fixes and performance improvement (labeling a 1M pointcloud is now possible) Version 1.2.2: Breaking change: exported point cloud coordinates are no longer translated (thanks @hetzge) Version 1.2.0: Support for binary and binary compressed point clouds (thanks @CecilHarvey)  Bitmap Image Editor  VIDEO: Bitmap labeling overview  DEMO: Bitmap editor   PCD Point Cloud Editor  VIDEO: Point cloud labeling overview  DEMO: Point cloud editor   How to run Using Docker Compose  Download the docker compose stack file (sse-docker-stack.yml) Set the folder that contains bitmap and point cloud files (YOUR_IMAGES_PATH) and run the tool using docker-compose The tool runs by default on port 80, you can change the mapping in sse-docker-stack.yml  wget https://raw.githubusercontent.com/Hitachi-Automotive-And-Industry-Lab/semantic-segmentation-editor/master/sse-docker-stack.yml wget https://raw.githubusercontent.com/Hitachi-Automotive-And-Industry-Lab/semantic-segmentation-editor/master/settings.json METEOR_SETTINGS=$(cat ./settings.json) SSE_IMAGES=YOUR_IMAGES_PATH docker-compose -f stack.yml up           (Optional) You can modify settings.json to customize classes data. Running from source Install Meteor (OSX or Linux) curl https://install.meteor.com/ | sh          or download Meteor Windows Installer Download and unzip latest version from here Start the application cd semantic-segmentation-editor-x.x.x meteor npm install meteor npm start          The editor will run by default on http://localhost:3000 (Optional) Edit settings.json By default, images are served from your_home_dir/sse-images and pointcloud binary segmentation data are stored in your_home_dir/sse-internal. You can configure these folders in settings.json by modifying images-folder and internal-folder properties. On Windows, use '/' separators, example c:/Users/john/images Check Meteor Environment Variables to configure your app (MONGO_URL, DISABLE_WEBSOCKETS, etc...) Configuration File: settings.json {   ""configuration"": {     ""images-folder"": ""/mnt/images"", // The root folder containing images and PCD files     ""internal-folder"": ""/mnt/pointcloud_data"" // Segmentation data (only 3D) will be stored in this folder   },   // The different sets of classes available in the tool   // For object classes, only the 'label' field is mandatory   // The icon field can be set with an icon from the mdi-material-ui package   ""sets-of-classes"": [     {       ""name"": ""Cityscapes"", ""objects"": [       {""label"": ""VOID"", ""color"": ""#CFCFCF""},       {""label"": ""Road"", ""color"": ""#804080"", ""icon"": ""Road""},       {""label"": ""Sidewalk"", ""color"": ""#F423E8"", ""icon"": ""NaturePeople""},       {""label"": ""Parking"", ""color"": ""#FAAAA0"", ""icon"": ""Parking""},       {""label"": ""Rail Track"", ""color"": ""#E6968C"", ""icon"": ""Train""},       {""label"": ""Person"", ""color"": ""#DC143C"", ""icon"": ""Walk""},       {""label"": ""Rider"", ""color"": ""#FF0000"", ""icon"": ""Motorbike""},       {""label"": ""Car"", ""color"": ""#0000E8"", ""icon"": ""Car""}     },     { ... }   ] }           How to use The editor is built around 3 different screens: The file navigator let's you browse available files to select a bitmap images or a point cloud for labeling  The bitmap image editor is dedicated to the labeling of jpg and png files by drawing polygons  The point cloud editor is dedicated to the labeling of point clouds by creating objects made of subsets of 3D points  Using the bitmap image editor There are several tools to create labeling polygons: Polygon Drawing Tool (P)  Click and/or drag to create points Type ESC to remove last created points in reverse order Drag the mouse pointer or hold Shift to create a complex polygon without having to click for each point Type ENTER or double click the first point to close the polygon  Magic Tool (A)  Create a polygon automatically using contrast threshold detection This tool is only useful to draw the outline of objects that have sharp contrasted edges (examples: sky, lane marking) Click inside the area you want to outline, then adjusts any sliders on the right to adjust the result Type ENTER to validate the result  Manipulation Tool (Alt)  Select, move and add point(s) to existing polygons Click inside a polygon to select it Click a point to select it Draw a lasso around multiple points to select them Drag a point with the mouse to move it Hold Shift to separate points that belongs to more than one polygon Click the line of a polygon to create a new point and drag the newly created point to place it  Cutting/Expanding Tool (C)  Modify the shape of an existing polygon Select the polygon you want to modify Draw a line starting and ending on the outline of a polygon The new line replace the existing path between starting and ending points The resulting shape is always the largest one  Contiguous Polygon Tool (F)  Create contiguous polygons easily Start a new polygon with the Polygon Drawing Tool Create the starting point by snapping to the outline of the polygon you want to workaround Create the ending point by snapping to another outline, at this point you must have a straight line crossing one or more existing polygons Hit F one or several times to choose what workaround path to use  Using the point cloud editor  Mouse left button: Rotate the point cloud around the current focused point (the center of the point cloud by default), click on a single point to add it to the current selection Mouse wheel: Zoom in/out Mouse middle button (or Ctrl+Click): Change the target of the camera Mouse right button: Used to select multiple points at the same time depending on the current Selection Tool and Selection Mode. Arrow keys: Move through the scene  PCD support  Supported input PCD format: ASCII, Binary and Binary compressed Supported input fields: x, y, z, label (optional integer), rgb (optional integer) Output PCD format is ASCII with fields x, y, z, label, object  and rgb (if available)  API Endpoints  /api/listing: List all annotated images /api/json/[PATH_TO_FILE]: (2D only) Get the polygons and other data for that file /api/pcdtext/[PATH_TO_FILE]: (3D only) Get the labeling of a pcd file using 2 addditional columns: label and object /api/pcdfile/[PATH_TO_FILE]: (3D only) The same but returned as ""plain/text"" attachment file download "
11,meetps/pytorch-semseg,3.2k,https://github.com/meetps/pytorch-semseg,Updated 5 days ago,"pytorch-semseg Semantic Segmentation Algorithms Implemented in PyTorch Networks implemented Upcoming DataLoaders implemented Requirements One-line installation Data Usage      README.md           pytorch-semseg    Semantic Segmentation Algorithms Implemented in PyTorch This repository aims at mirroring popular semantic segmentation architectures in PyTorch.     Networks implemented  PSPNet - With support for loading pretrained models w/o caffe dependency ICNet - With optional batchnorm and pretrained models FRRN - Model A and B FCN - All 1 (FCN32s), 2 (FCN16s) and 3 (FCN8s) stream variants U-Net - With optional deconvolution and batchnorm Link-Net - With multiple resnet backends Segnet - With Unpooling using Maxpool indices  Upcoming  E-Net RefineNet  DataLoaders implemented  CamVid Pascal VOC ADE20K MIT Scene Parsing Benchmark Cityscapes NYUDv2 Sun-RGBD  Requirements  pytorch >=0.4.0 torchvision ==0.2.0 scipy tqdm tensorboardX  One-line installation pip install -r requirements.txt Data  Download data for desired dataset(s) from list of URLs here. Extract the zip / tar and modify the path appropriately in your config.yaml  Usage Setup config file # Model Configuration model:     arch: <name> [options: 'fcn[8,16,32]s, unet, segnet, pspnet, icnet, icnetBN, linknet, frrn[A,B]'     <model_keyarg_1>:<value>  # Data Configuration data:     dataset: <name> [options: 'pascal, camvid, ade20k, mit_sceneparsing_benchmark, cityscapes, nyuv2, sunrgbd, vistas']     train_split: <split_to_train_on>     val_split: <spit_to_validate_on>     img_rows: 512     img_cols: 1024     path: <path/to/data>     <dataset_keyarg1>:<value>  # Training Configuration training:     n_workers: 64     train_iters: 35000     batch_size: 16     val_interval: 500     print_interval: 25     loss:         name: <loss_type> [options: 'cross_entropy, bootstrapped_cross_entropy, multi_scale_crossentropy']         <loss_keyarg1>:<value>      # Optmizer Configuration     optimizer:         name: <optimizer_name> [options: 'sgd, adam, adamax, asgd, adadelta, adagrad, rmsprop']         lr: 1.0e-3         <optimizer_keyarg1>:<value>          # Warmup LR Configuration         warmup_iters: <iters for lr warmup>         mode: <'constant' or 'linear' for warmup'>         gamma: <gamma for warm up>      # Augmentations Configuration     augmentations:         gamma: x                                     #[gamma varied in 1 to 1+x]         hue: x                                       #[hue varied in -x to x]         brightness: x                                #[brightness varied in 1-x to 1+x]         saturation: x                                #[saturation varied in 1-x to 1+x]         contrast: x                                  #[contrast varied in 1-x to 1+x]         rcrop: [h, w]                                #[crop of size (h,w)]         translate: [dh, dw]                          #[reflective translation by (dh, dw)]         rotate: d                                    #[rotate -d to d degrees]         scale: [h,w]                                 #[scale to size (h,w)]         ccrop: [h,w]                                 #[center crop of (h,w)]         hflip: p                                     #[flip horizontally with chance p]         vflip: p                                     #[flip vertically with chance p]      # LR Schedule Configuration     lr_schedule:         name: <schedule_type> [options: 'constant_lr, poly_lr, multi_step, cosine_annealing, exp_lr']         <scheduler_keyarg1>:<value>      # Resume from checkpoint     resume: <path_to_checkpoint>          To train the model : python train.py [-h] [--config [CONFIG]]  --config                Configuration file to use           To validate the model : usage: validate.py [-h] [--config [CONFIG]] [--model_path [MODEL_PATH]]                        [--eval_flip] [--measure_time]    --config              Config file to be used   --model_path          Path to the saved model   --eval_flip           Enable evaluation with flipped image | True by default   --measure_time        Enable evaluation with time (fps) measurement | True                         by default           To test the model w.r.t. a dataset on custom images(s): python test.py [-h] [--model_path [MODEL_PATH]] [--dataset [DATASET]]                [--dcrf [DCRF]] [--img_path [IMG_PATH]] [--out_path [OUT_PATH]]    --model_path          Path to the saved model   --dataset             Dataset to use ['pascal, camvid, ade20k etc']   --dcrf                Enable DenseCRF based post-processing   --img_path            Path of the input image   --out_path            Path of the output segmap           If you find this code useful in your research, please consider citing: @article{mshahsemseg,     Author = {Meet P Shah},     Title = {Semantic Segmentation Architectures Implemented in PyTorch.},     Journal = {https://github.com/meetshah1995/pytorch-semseg},     Year = {2017} } "
12,yaksoy/SemanticSoftSegmentation,552,https://github.com/yaksoy/SemanticSoftSegmentation,"Updated on Sep 7, 2020","Semantic Soft Segmentation License and citation Credit      readme.md     Semantic Soft Segmentation This repository includes the spectral segmentation approach presented in Yagiz Aksoy, Tae-Hyun Oh, Sylvain Paris, Marc Pollefeys and Wojciech Matusik, ""Semantic Soft Segmentation"", ACM Transactions on Graphics (Proc. SIGGRAPH), 2018           The network for semantic feature generation can be found [here]. Please refer to the [project page] for more information and example data. The Superpixels class requires [ImageGraphs]. License and citation This toolbox is provided for academic use only. If you use this code, please cite our paper: @ARTICLE{sss, author={Ya\u{g}{\i}z Aksoy and Tae-Hyun Oh and Sylvain Paris and Marc Pollefeys and Wojciech Matusik}, title={Semantic Soft Segmentation}, journal={ACM Transactions on Graphics (Proc. SIGGRAPH)}, year={2018}, pages = {72:1-72:13}, volume = {37}, number = {4} }           Credit Parts of this implementation are taken from Anat Levin, Alex Rav-Acha, Dani Lischinski, ""Spectral Matting"", IEEE TPAMI, 2008           The original source code for Spectral Matting can be found [here]. "
13,hszhao/semseg,1k,https://github.com/hszhao/semseg,"Updated on Mar 31, 2021","PyTorch Semantic Segmentation Introduction Update Usage Performance Citation Question      README.md           PyTorch Semantic Segmentation Introduction This repository is a PyTorch implementation for semantic segmentation / scene parsing. The code is easy to use for training and testing on various datasets. The codebase mainly uses ResNet50/101/152 as backbone and can be easily adapted to other basic classification structures. Implemented networks including PSPNet and PSANet, which ranked 1st places in ImageNet Scene Parsing Challenge 2016 @ECCV16, LSUN Semantic Segmentation Challenge 2017 @CVPR17 and WAD Drivable Area Segmentation Challenge 2018 @CVPR18. Sample experimented datasets are ADE20K, PASCAL VOC 2012 and Cityscapes.  Update  2020.05.15: Branch master, use official nn.SyncBatchNorm, only multiprocessing training is supported, tested with pytorch 1.4.0. 2019.05.29: Branch 1.0.0, both multithreading training (nn.DataParallel) and multiprocessing training (nn.parallel.DistributedDataParallel) (recommended) are supported. And the later one is much faster. Use syncbn from EncNet and apex, tested with pytorch 1.0.0.  Usage   Highlight:  Fast multiprocessing training (nn.parallel.DistributedDataParallel) with official nn.SyncBatchNorm. Better reimplementation results with well designed code structures. All initialization models, trained models and predictions are available.    Requirement:  Hardware: 4-8 GPUs (better with >=11G GPU memory) Software: PyTorch>=1.1.0, Python3, tensorboardX,    Clone the repository: git clone https://github.com/hszhao/semseg.git            Train:   Download related datasets and symlink the paths to them as follows (you can alternatively modify the relevant paths specified in folder config): cd semseg mkdir -p dataset ln -s /path_to_ade20k_dataset dataset/ade20k             Download ImageNet pre-trained models and put them under folder initmodel for weight initialization. Remember to use the right dataset format detailed in FAQ.md.   Specify the gpu used in config then do training: sh tool/train.sh ade20k pspnet50            If you are using SLURM for nodes manager, uncomment lines in train.sh and then do training: sbatch tool/train.sh ade20k pspnet50              Test:   Download trained segmentation models and put them under folder specified in config or modify the specified paths.   For full testing (get listed performance): sh tool/test.sh ade20k pspnet50            Quick demo on one image: PYTHONPATH=./ python tool/demo.py --config=config/ade20k/ade20k_pspnet50.yaml --image=figure/demo/ADE_val_00001515.jpg TEST.scales '[1.0]'              Visualization: tensorboardX incorporated for better visualization. tensorboard --logdir=exp/ade20k            Other:  Resources: GoogleDrive LINK contains shared models, visual predictions and data lists. Models: ImageNet pre-trained models and trained segmentation models can be accessed. Note that our ImageNet pretrained models are slightly different from original ResNet implementation in the beginning part. Predictions: Visual predictions of several models can be accessed. Datasets: attributes (names and colors) are in folder dataset and some sample lists can be accessed. Some FAQs: FAQ.md. Former video predictions: high accuracy -- PSPNet, PSANet; high efficiency -- ICNet.    Performance Description: mIoU/mAcc/aAcc stands for mean IoU, mean accuracy of each class and all pixel accuracy respectively. ss denotes single scale testing and ms indicates multi-scale testing. Training time is measured on a sever with 8 GeForce RTX 2080 Ti. General parameters cross different datasets are listed below:  Train Parameters: sync_bn(True), scale_min(0.5), scale_max(2.0), rotate_min(-10), rotate_max(10), zoom_factor(8), ignore_label(255), aux_weight(0.4), batch_size(16), base_lr(1e-2), power(0.9), momentum(0.9), weight_decay(1e-4). Test Parameters: ignore_label(255), scales(single: [1.0], multiple: [0.5 0.75 1.0 1.25 1.5 1.75]).    ADE20K: Train Parameters: classes(150), train_h(473/465-PSP/A), train_w(473/465-PSP/A), epochs(100). Test Parameters: classes(150), test_h(473/465-PSP/A), test_w(473/465-PSP/A), base_size(512).  Setting: train on train (20210 images) set and test on val (2000 images) set.     Network mIoU/mAcc/aAcc(ss) mIoU/mAcc/pAcc(ms) Training Time     PSPNet50 0.4189/0.5227/0.8039. 0.4284/0.5266/0.8106. 14h   PSANet50 0.4229/0.5307/0.8032. 0.4305/0.5312/0.8101. 14h   PSPNet101 0.4310/0.5375/0.8107. 0.4415/0.5426/0.8172. 20h   PSANet101 0.4337/0.5385/0.8102. 0.4414/0.5392/0.8170. 20h      PSACAL VOC 2012: Train Parameters: classes(21), train_h(473/465-PSP/A), train_w(473/465-PSP/A), epochs(50). Test Parameters: classes(21), test_h(473/465-PSP/A), test_w(473/465-PSP/A), base_size(512).  Setting: train on train_aug (10582 images) set and test on val (1449 images) set.     Network mIoU/mAcc/aAcc(ss) mIoU/mAcc/pAcc(ms) Training Time     PSPNet50 0.7705/0.8513/0.9489. 0.7802/0.8580/0.9513. 3.3h   PSANet50 0.7725/0.8569/0.9491. 0.7787/0.8606/0.9508. 3.3h   PSPNet101 0.7907/0.8636/0.9534. 0.7963/0.8677/0.9550. 5h   PSANet101 0.7870/0.8642/0.9528. 0.7966/0.8696/0.9549. 5h      Cityscapes: Train Parameters: classes(19), train_h(713/709-PSP/A), train_w(713/709-PSP/A), epochs(200). Test Parameters: classes(19), test_h(713/709-PSP/A), test_w(713/709-PSP/A), base_size(2048).  Setting: train on fine_train (2975 images) set and test on fine_val (500 images) set.     Network mIoU/mAcc/aAcc(ss) mIoU/mAcc/pAcc(ms) Training Time     PSPNet50 0.7730/0.8431/0.9597. 0.7838/0.8486/0.9617. 7h   PSANet50 0.7745/0.8461/0.9600. 0.7818/0.8487/0.9622. 7.5h   PSPNet101 0.7863/0.8577/0.9614. 0.7929/0.8591/0.9638. 10h   PSANet101 0.7842/0.8599/0.9621. 0.7940/0.8631/0.9644. 10.5h      Citation If you find the code or trained models useful, please consider citing: @misc{semseg2019,   author={Zhao, Hengshuang},   title={semseg},   howpublished={\url{https://github.com/hszhao/semseg}},   year={2019} } @inproceedings{zhao2017pspnet,   title={Pyramid Scene Parsing Network},   author={Zhao, Hengshuang and Shi, Jianping and Qi, Xiaojuan and Wang, Xiaogang and Jia, Jiaya},   booktitle={CVPR},   year={2017} } @inproceedings{zhao2018psanet,   title={{PSANet}: Point-wise Spatial Attention Network for Scene Parsing},   author={Zhao, Hengshuang and Zhang, Yi and Liu, Shu and Shi, Jianping and Loy, Chen Change and Lin, Dahua and Jia, Jiaya},   booktitle={ECCV},   year={2018} }           Question Some FAQ.md collected. You are welcome to send pull requests or give some advices. Contact information: hengshuangzhao at gmail.com. "
14,bubbliiiing/Semantic-Segmentation,331,https://github.com/bubbliiiing/Semantic-Segmentation,"Updated on Jun 29, 2021",Semantic-Segmentation语义分割模型在Keras当中的实现 大通知！ 目录 所需环境 注意事项 数据集下载 训练步骤 预测步骤 Reference      README.md           Semantic-Segmentation语义分割模型在Keras当中的实现  大通知！ PSPnet-Keras重制版如下： 源码路径：https://github.com/bubbliiiing/pspnet-keras 视频地址：https://www.bilibili.com/video/BV1bz4y1f77C PSPnet-Pytorch重制版如下： 源码路径：https://github.com/bubbliiiing/pspnet-pytorch 视频地址：https://www.bilibili.com/video/BV1zt4y1q7HH PSPnet-Tensorflow2重制版如下： 源码路径：https://github.com/bubbliiiing/pspnet-tf2 视频地址：https://www.bilibili.com/video/BV1Wh411f7NU Unet-Keras重制版如下： 源码路径：https://github.com/bubbliiiing/unet-keras 视频地址：https://www.bilibili.com/video/BV1St4y1r7hE Unet-Pytorch重制版如下： 源码路径：https://github.com/bubbliiiing/unet-pytorch 视频地址：https://www.bilibili.com/video/BV1rz4y117rR Unet-Tensorflow2重制版如下： 源码路径：https://github.com/bubbliiiing/unet-tf2 Deeplab-Keras重制版如下： 源码路径：https://github.com/bubbliiiing/deeplabv3-plus-keras 目录  所需环境 Environment 注意事项 Attention 数据集下载 Download 训练步骤 How2train 预测步骤 How2predict 参考资料 Reference  所需环境 tensorflow-gpu==1.13.1 keras==2.1.5 注意事项 该代码是我早期整理的语义分割代码，尽管可以使用，但是存在许多缺点。大家尽量可以使用重制版的代码，因为重制版的代码里面增加了很多新内容，比如添加了Dice-loss，增加了更多参数的选择，提供了VOC预训练权重等。 在2021年1月28重新上传了该库，给代码添加了非常详细的注释，该库仍然可以作为一个语义分割的入门库进行使用。 在使用前一定要注意根目录与相对目录的选取，这样才能正常进行训练。 数据集下载 斑马线数据集： 链接：https://pan.baidu.com/s/1uzwqLaCXcWe06xEXk1ROWw 提取码：pp6w VOC数据集： 链接: https://pan.baidu.com/s/1Urh9W7XPNMF8yR67SDjAAQ 提取码: cvy2 训练步骤  准备好训练数据集，如果想要进行简单尝试，可以通过如上的斑马线数据集进行尝试；如果想要进行自己的数据集训练，可以参考制作自己的数据集的视频，进行制作 在完成数据集的准备后，利用pycharm或者vscode打开对应模型的文件夹，将数据集及其对应的train.txt文件复制到datasets2文件夹中 然后运行train.py进行训练。 大家关心的多分类的代码在Muiti_Class_deeplab_Mobile里。  预测步骤  除去Muiti_Class_deeplab_Mobile可以直接运行predict.py进行预测外，其它的模型均需要先完成训练才可以预测。 在完成训练后，将predict.py里面模型载入的权重更换成logs文件夹内的权值。 将想要预测的图片放入img文件夹。 运行predict.py即可开始预测。  Reference image-segmentation-keras 
15,msracver/FCIS,1.6k,https://github.com/msracver/FCIS,"Updated on Sep 26, 2021","Fully Convolutional Instance-aware Semantic Segmentation Introduction Resources Disclaimer License Citing FCIS Main Results Requirements: Software Requirements: Hardware Installation Demo Preparation for Training & Testing Usage Misc.      README.md           Fully Convolutional Instance-aware Semantic Segmentation The major contributors of this repository include Haozhi Qi, Yi Li, Guodong Zhang, Haochen Zhang, Jifeng Dai, and Yichen Wei. Introduction FCIS is a fully convolutional end-to-end solution for instance segmentation, which won the first place in COCO segmentation challenge 2016. FCIS is initially described in a CVPR 2017 spotlight paper. It is worth noticing that:  FCIS provides a simple, fast and accurate framework for instance segmentation. Different from MNC, FCIS performs instance mask estimation and categorization jointly and simultanously, and estimates class-specific masks. We did not exploit the various techniques & tricks in the Mask RCNN system, like increasing RPN anchor numbers (from 12 to 15), training on anchors out of image boundary, enlarging the image (shorter side from 600 to 800 pixels), utilizing FPN features and aligned ROI pooling. These techniques & tricks should be orthogonal to our simple baseline.  Resources  Visual results on the first 5k images from COCO test set of our COCO 2016 challenge entry: OneDrive. Slides in ImageNet ILSVRC and COCO workshop 2016: OneDrive.   Disclaimer This is an official implementation for Fully Convolutional Instance-aware Semantic Segmentation (FCIS) based on MXNet. It is worth noticing that:  The original implementation is based on our internal Caffe version on Windows. There are slight differences in the final accuracy and running time due to the plenty details in platform switch. The code is tested on official MXNet@(commit 62ecb60) with the extra operators for FCIS. We trained our model based on the ImageNet pre-trained ResNet-v1-101 using a model converter. The converted model produces slightly lower accuracy (Top-1 Error on ImageNet val: 24.0% v.s. 23.6%). This repository used code from MXNet rcnn example and mx-rfcn.  License © Microsoft, 2017. Licensed under an MIT license. Citing FCIS If you find FCIS useful in your research, please consider citing: @inproceedings{li2016fully,   Author = {Yi Li, Haozhi Qi, Jifeng Dai, Xiangyang Ji and Yichen Wei}   Title = {Fully Convolutional Instance-aware Semantic Segmentation},   Conference = {CVPR},   year = {2017} }           Main Results     training data testing data mAP^r@0.5 mAP^r@0.7 time     FCIS, ResNet-v1-101 VOC 2012 train VOC 2012 val 66.0 51.9 0.23s        training data testing data mAP^r mAP^r@0.5 mAP^r@0.75 mAP^r@S mAP^r@M mAP^r@L     FCIS, ResNet-v1-101, OHEM coco trainval35k coco minival 29.2 50.8 29.7 7.9 31.4 51.1   FCIS, ResNet-v1-101, OHEM coco trainval35k coco test-dev 29.6 51.4 30.2 8.0 31.0 49.7    Running time is counted on a single Maxwell Titan X GPU (mini-batch size is 1 in inference). Requirements: Software   MXNet from the offical repository. We tested our code on MXNet@(commit 62ecb60). Due to the rapid development of MXNet, it is recommended to checkout this version if you encounter any issues. We may maintain this repository periodically if MXNet adds important feature in future release.   Python packages might missing: cython, opencv-python >= 3.2.0, easydict. If pip is set up on your system, those packages should be able to be fetched and installed by running pip install Cython pip install opencv-python==3.2.0.6 pip install easydict==1.6 pip install hickle             For Windows users, Visual Studio 2015 is needed to compile cython module.   Requirements: Hardware Any NVIDIA GPUs with at least 5GB memory should be OK Installation  Clone the FCIS repository, and we'll call the directory that you cloned FCIS as ${FCIS_ROOT}.  git clone https://github.com/msracver/FCIS.git             For Windows users, run cmd .\init.bat. For Linux user, run sh ./init.sh. The scripts will build cython module automatically and create some folders.   Install MXNet: Note: The MXNet's Custom Op cannot execute parallelly using multi-gpus after this PR. We strongly suggest the user rollback to version MXNet@(commit 998378a) for training (following Section 3.2 - 3.6). Quick start 3.1 Install MXNet and all dependencies by pip install -r requirements.txt           If there is no other error message, MXNet should be installed successfully. Build from source (alternative way) 3.2 Clone MXNet and checkout to MXNet@(commit 998378a) by git clone --recursive https://github.com/dmlc/mxnet.git git checkout 998378a git submodule init git submodule update           3.3 Copy channel operators in $(FCIS_ROOT)/fcis/operator_cxx to $(YOUR_MXNET_FOLDER)/src/operator/contrib by cp -r $(FCIS_ROOT)/fcis/operator_cxx/channel_operator* $(MXNET_ROOT)/src/operator/contrib/           3.4 Compile MXNet cd ${MXNET_ROOT} make -j $(nproc) USE_OPENCV=1 USE_BLAS=openblas USE_CUDA=1 USE_CUDA_PATH=/usr/local/cuda USE_CUDNN=1           3.5 Install the MXNet Python binding by Note: If you will actively switch between different versions of MXNet, please follow 3.5 instead of 3.4 cd python sudo python setup.py install           3.6 For advanced users, you may put your Python packge into ./external/mxnet/$(YOUR_MXNET_PACKAGE), and modify MXNET_VERSION in ./experiments/fcis/cfgs/*.yaml to $(YOUR_MXNET_PACKAGE). Thus you can switch among different versions of MXNet quickly.   Demo   To run the demo with our trained model (on COCO trainval35k), please download the model manually from OneDrive (Chinese users can also get it from BaiduYun with code tmd4), and put it under folder model/. Make sure it looks like this: ./model/fcis_coco-0000.params             Run python ./fcis/demo.py             Preparation for Training & Testing   Please download VOC 2012 dataset with additional annotations from SBD. Move inst, cls, img folders to VOCdevit and make sure it looks like this: Please use the train&val split in this repo, which follows the protocal of SDS. .data/VOCdevkit/VOCSDS/img/ .data/VOCdevkit/VOCSDS/inst/ .data/VOCdevkit/VOCSDS/cls/             Please download COCO dataset and annotations for the 5k image minival subset and val2014 minus minival (val35k). Make sure it looks like this: .data/coco/ .data/coco/annotations/instances_valminusminival2014.json .data/coco/annotations/instances_minival2014.json             Please download ImageNet-pretrained ResNet-v1-101 model manually from OneDrive, and put it under folder ./model. Make sure it looks like this: ./model/pretrained_model/resnet_v1_101-0000.params             Usage  All of our experiment settings (GPU #, dataset, etc.) are kept in yaml config files at folder ./experiments/fcis/cfgs. Two config files have been provided so far: FCIS@COCO with OHEM and FCIS@VOC without OHEM. We use 8 and 4 GPUs to train models on COCO and on VOC, respectively. To perform experiments, run the python scripts with the corresponding config file as input. For example, to train and test FCIS on COCO with ResNet-v1-101, use the following command python experiments/fcis/fcis_end2end_train_test.py --cfg experiments/fcis/cfgs/resnet_v1_101_coco_fcis_end2end_ohem.yaml           A cache folder would be created automatically to save the model and the log under output/fcis/coco/ or output/fcis/voc/. Please find more details in config files and in our code.  Misc. Code has been tested under:  Ubuntu 14.04 with a Maxwell Titan X GPU and Intel Xeon CPU E5-2620 v2 @ 2.10GHz Windows Server 2012 R2 with 8 K40 GPUs and Intel Xeon CPU E5-2650 v2 @ 2.60GHz Windows Server 2012 R2 with 4 Pascal Titan X GPUs and Intel Xeon CPU E5-2650 v4 @ 2.30GHz "
16,yassouali/pytorch-segmentation,943,https://github.com/yassouali/pytorch-segmentation,"Updated on Oct 13, 2021","Semantic Segmentation in PyTorch Requirements Main Features Models Datasets Losses Learning rate schedulers Data augmentation Training Inference Code structure Config file format Acknowledgement      README.md           Semantic Segmentation in PyTorch   Semantic Segmentation in PyTorch  Requirements Main Features  Models Datasets Losses Learning rate schedulers Data augmentation   Training Inference Code structure Config file format Acknowledgement    This repo contains a PyTorch an implementation of different semantic segmentation models for different datasets. Requirements PyTorch and Torchvision needs to be installed before running the scripts, together with PIL and opencv for data-preprocessing and tqdm for showing the training progress. PyTorch v1.1 is supported (using the new supported tensoboard); can work with ealier versions, but instead of using tensoboard, use tensoboardX. pip install -r requirements.txt          or for a local installation pip install --user -r requirements.txt          Main Features  A clear and easy to navigate structure, A json config file with a lot of possibilities for parameter tuning, Supports various models, losses, Lr schedulers, data augmentations and datasets,  So, what's available ? Models  (Deeplab V3+) Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation [Paper] (GCN) Large Kernel Matter, Improve Semantic Segmentation by Global Convolutional Network [Paper] (UperNet) Unified Perceptual Parsing for Scene Understanding [Paper] (DUC, HDC) Understanding Convolution for Semantic Segmentation [Paper] (PSPNet) Pyramid Scene Parsing Network [Paper] (ENet) A Deep Neural Network Architecture for Real-Time Semantic Segmentation [Paper] (U-Net) Convolutional Networks for Biomedical Image Segmentation (2015): [Paper] (SegNet) A Deep ConvolutionalEncoder-Decoder Architecture for ImageSegmentation (2016): [Paper] (FCN) Fully Convolutional Networks for Semantic Segmentation (2015): [Paper]  Datasets   Pascal VOC: For pascal voc, first download the original dataset, after extracting the files we'll end up with VOCtrainval_11-May-2012/VOCdevkit/VOC2012 containing, the image sets, the XML annotation for both object detection and segmentation, and JPEG images. The second step is to augment the dataset using the additionnal annotations provided by Semantic Contours from Inverse Detectors. First download the image sets (train_aug, trainval_aug, val_aug and test_aug) from this link: Aug ImageSets, and  add them the rest of the segmentation sets in /VOCtrainval_11-May-2012/VOCdevkit/VOC2012/ImageSets/Segmentation, and then download new annotations SegmentationClassAug and add them to the path VOCtrainval_11-May-2012/VOCdevkit/VOC2012, now we're set, for training use the path to VOCtrainval_11-May-2012   CityScapes: First download the images and the annotations (there is two types of annotations, Fine gtFine_trainvaltest.zip and Coarse gtCoarse.zip annotations, and the images leftImg8bit_trainvaltest.zip) from the official website cityscapes-dataset.com, extract all of them in the same folder, and use the location of this folder in config.json for training.   ADE20K: For ADE20K, simply download the images and their annotations for training and validation from sceneparsing.csail.mit.edu, and for the rest visit the website.   COCO Stuff: For COCO, there is two partitions, CocoStuff10k with only 10k that are used for training the evaluation, note that this dataset is outdated, can be used for small scale testing and training, and can be downloaded here. For the official dataset with all of the training 164k examples, it can be downloaded from the official website. Note that when using COCO dataset, 164k version is used per default, if 10k is prefered, this needs to be specified with an additionnal parameter partition = 'CocoStuff164k' in the config file with the corresponding path.   Losses In addition to the Cross-Entorpy loss, there is also  Dice-Loss, which measures of overlap between two samples and can be more reflective of the training objective (maximizing the mIoU), but is highly non-convexe and can be hard to optimize. CE Dice loss, the sum of the Dice loss and CE, CE gives smooth optimization while Dice loss is a good indicator of the quality of the segmentation results. Focal Loss, an alternative version of the CE, used to avoid class imbalance where the confident predictions are scaled down. Lovasz Softmax lends it self as a good alternative to the Dice loss, where we can directly optimization for the mean intersection-over-union based on the convex Lovász extension of submodular losses (for more details, check the paper: The Lovász-Softmax loss).  Learning rate schedulers  Poly learning rate, where the learning rate is scaled down linearly from the starting value down to zero during training. Considered as the go to scheduler for semantic segmentaion (see Figure below). One Cycle learning rate, for a learning rate LR, we start from LR / 10 up to LR for 30% of the training time, and we scale down to LR / 25 for remaining time, the scaling is done in a cos annealing fashion (see Figure bellow), the momentum is also modified but in the opposite manner starting from 0.95 down to 0.85 and up to 0.95, for more detail see the paper: Super-Convergence.   Data augmentation All of the data augmentations are implemented using OpenCV in \base\base_dataset.py, which are: rotation (between -10 and 10 degrees), random croping between 0.5 and 2 of the selected crop_size, random h-flip and blurring Training To train a model, first download the dataset to be used to train the model, then choose the desired architecture, add the correct path to the dataset and set the desired hyperparameters (the config file is detailed below), then simply run: python train.py --config config.json          The training will automatically be run on the GPUs (if more that one is detected and  multipple GPUs were selected in the config file, torch.nn.DataParalled is used for multi-gpu training), if not the CPU is used. The log files will be saved in saved\runs and the .pth chekpoints in saved\, to monitor the training using tensorboard, please run: tensorboard --logdir saved            Inference For inference, we need a PyTorch trained model, the images we'd like to segment and the config used in training (to load the correct model and other parameters), python inference.py --config config.json --model best_model.pth --images images_folder          The predictions will be saved as .png images using the default palette in the passed fodler name, if not, outputs\ is used, for Pacal VOC the default palette is:  Here are the parameters availble for inference: --output       The folder where the results will be saved (default: outputs). --extension    The extension of the images to segment (default: jpg). --images       Folder containing the images to segment. --model        Path to the trained model. --mode         Mode to be used, choose either `multiscale` or `sliding` for inference (multiscale is the default behaviour). --config       The config file used for training the model.           Trained Model:    Model Backbone PascalVoc val mIoU PascalVoc test mIoU Pretrained Model     PSPNet ResNet 50 82% 79% Dropbox    Code structure The code structure is based on pytorch-template pytorch-template/ │ ├── train.py - main script to start training ├── inference.py - inference using a trained model ├── trainer.py - the main trained ├── config.json - holds configuration for training │ ├── base/ - abstract base classes │   ├── base_data_loader.py │   ├── base_model.py │   ├── base_dataset.py - All the data augmentations are implemented here │   └── base_trainer.py │ ├── dataloader/ - loading the data for different segmentation datasets │ ├── models/ - contains semantic segmentation models │ ├── saved/ │   ├── runs/ - trained models are saved here │   └── log/ - default logdir for tensorboard and logging output │ └── utils/ - small utility functions     ├── losses.py - losses used in training the model     ├── metrics.py - evaluation metrics used     └── lr_scheduler - learning rate schedulers           Config file format Config files are in .json format: {   ""name"": ""PSPNet"",         // training session name   ""n_gpu"": 1,               // number of GPUs to use for training.   ""use_synch_bn"": true,     // Using Synchronized batchnorm (for multi-GPU usage)      ""arch"": {         ""type"": ""PSPNet"", // name of model architecture to train         ""args"": {             ""backbone"": ""resnet50"",     // encoder type type             ""freeze_bn"": false,         // When fine tuning the model this can be used             ""freeze_backbone"": false    // In this case only the decoder is trained         }     },      ""train_loader"": {         ""type"": ""VOC"",          // Selecting data loader         ""args"":{             ""data_dir"": ""data/"",  // dataset path             ""batch_size"": 32,     // batch size             ""augment"": true,      // Use data augmentation             ""crop_size"": 380,     // Size of the random crop after rescaling             ""shuffle"": true,             ""base_size"": 400,     // The image is resized to base_size, then randomly croped             ""scale"": true,        // Random rescaling between 0.5 and 2 before croping             ""flip"": true,         // Random H-FLip             ""rotate"": true,       // Random rotation between 10 and -10 degrees             ""blur"": true,         // Adding a slight amount of blut to the image             ""split"": ""train_aug"", // Split to use, depend of the dataset             ""num_workers"": 8         }     },      ""val_loader"": {     // Same for val, but no data augmentation, only a center crop         ""type"": ""VOC"",         ""args"":{             ""data_dir"": ""data/"",             ""batch_size"": 32,             ""crop_size"": 480,             ""val"": true,             ""split"": ""val"",             ""num_workers"": 4         }     },      ""optimizer"": {         ""type"": ""SGD"",         ""differential_lr"": true,      // Using lr/10 for the backbone, and lr for the rest         ""args"":{             ""lr"": 0.01,               // Learning rate             ""weight_decay"": 1e-4,     // Weight decay             ""momentum"": 0.9         }     },      ""loss"": ""CrossEntropyLoss2d"",     // Loss (see utils/losses.py)     ""ignore_index"": 255,              // Class to ignore (must be set to -1 for ADE20K) dataset     ""lr_scheduler"": {         ""type"": ""Poly"",               // Learning rate scheduler (Poly or OneCycle)         ""args"": {}     },      ""trainer"": {         ""epochs"": 80,                 // Number of training epochs         ""save_dir"": ""saved/"",         // Checkpoints are saved in save_dir/models/         ""save_period"": 10,            // Saving chechpoint each 10 epochs          ""monitor"": ""max Mean_IoU"",    // Mode and metric for model performance         ""early_stop"": 10,             // Number of epochs to wait before early stoping (0 to disable)          ""tensorboard"": true,        // Enable tensorboard visualization         ""log_dir"": ""saved/runs"",         ""log_per_iter"": 20,          ""val"": true,         ""val_per_epochs"": 5         // Run validation each 5 epochs     } }          Acknowledgement  PyTorch-Encoding Pytorch-Template Synchronized-BatchNorm-PyTorch "
17,akirasosa/mobile-semantic-segmentation,671,https://github.com/akirasosa/mobile-semantic-segmentation,"Updated on Sep 7, 2021","Real-Time Semantic Segmentation in Mobile device Example application Requirements About Model Steps to training Data Preparation Training Pretrained model Converting TBD      README.md           Real-Time Semantic Segmentation in Mobile device This project is an example project of semantic segmentation for mobile real-time app. The architecture is inspired by MobileNetV2 and U-Net. LFW, Labeled Faces in the Wild, is used as a Dataset. The goal of this project is to detect hair segments with reasonable accuracy and speed in mobile device. Currently, it achieves 0.89 IoU. About speed vs accuracy, more details are available at my post.  Example application  iOS Android (TODO)  Requirements  Python 3.8 pip install -r requirements.txt -f https://download.pytorch.org/whl/torch_stable.html CoreML for iOS app.  About Model At this time, there is only one model in this repository, MobileNetV2_unet. As a typical U-Net architecture, it has encoder and decoder parts, which consist of depthwise conv blocks proposed by MobileNets. Input image is encoded to 1/32 size, and then decoded to 1/2. Finally, it scores the results and make it to original size. Steps to training Data Preparation Data is available at LFW. To get mask images, refer issue #11 for more. After you got images and masks, put the images of faces and masks as shown below. data/   lfw/     raw/       images/         0001.jpg         0002.jpg       masks/         0001.ppm         0002.ppm           Training If you use 224 x 224 as input size, pre-trained weight of MobileNetV2 is available. It will be automatically downloaded when you train model with the following command. cd src python run_train.py params/002.yaml           Dice coefficient is used as a loss function. Pretrained model    Input size IoU Download     224 0.89 Google Drive    Converting As the purpose of this project is to make model run in mobile device, this repository contains some scripts to convert models for iOS and Android.  run_convert_coreml.py  It converts trained PyTorch model into CoreML model for iOS app.    TBD   Report speed vs accuracy in mobile device.  Convert pytorch to Android using TesorFlow Light "
18,luyanger1799/Amazing-Semantic-Segmentation,371,https://github.com/luyanger1799/Amazing-Semantic-Segmentation,"Updated on Oct 16, 2020","Amazing-Semantic-Segmentation Models Base Models Losses Optimizers Learning Rate Scheduler Dataset Setting Installation Usage Download Training Testing Predicting Evaluating PyPI Pre-trained Feedback      README.MD           Amazing-Semantic-Segmentation      Amazing Semantic Segmentation on Tensorflow && Keras (include FCN, UNet, SegNet, PSPNet, PAN, RefineNet, DeepLabV3, DeepLabV3+, DenseASPP, BiSegNet ...)   Models The project supports these semantic segmentation models as follows:   FCN-8s/16s/32s - Fully Convolutional Networks for Semantic Segmentation UNet - U-Net: Convolutional Networks for Biomedical Image Segmentation SegNet - SegNet:A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation Bayesian-SegNet - Bayesian SegNet: Model Uncertainty in Deep Convolutional Encoder-Decoder Architectures for Scene Understanding PSPNet - Pyramid Scene Parsing Network RefineNet - RefineNet: Multi-Path Refinement Networks for High-Resolution Semantic Segmentation PAN - Pyramid Attention Network for Semantic Segmentation DeepLabV3 - Rethinking Atrous Convolution for Semantic Image Segmentation DeepLabV3Plus - Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation DenseASPP - DenseASPP for Semantic Segmentation in Street Scenes BiSegNet - BiSeNet: Bilateral Segmentation Network for Real-time Semantic Segmentation    Base Models The project supports these backbone models as follows, and your can choose suitable base model according to your needs.   VGG16/19 - Very Deep Convolutional Networks for Large-Scale Image Recognition ResNet50/101/152 - Deep Residual Learning for Image Recognition DenseNet121/169/201/264 - Densely Connected Convolutional Networks MobileNetV1 - MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications MobileNetV2 - MobileNetV2: Inverted Residuals and Linear Bottlenecks Xception - Xception: Deep Learning with Depthwise Separable Convolutions Xception-DeepLab - Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation    Losses The project supports these loss functions:   Cross Entropy Focal Loss MIoU Loss Self Balanced Focal Loss original ...    Optimizers The project supports these optimizers:   SGD Adam Nadam AdamW NadamW SGDW    Learning Rate Scheduler The project supports these learning rate schedule strategies:   step decay poly decay cosine decay warm up   Dataset Setting The folds of your dataset must satisfy the following structures: |-- dataset |  |-- train |  |  |-- images |  |  |-- labels |  |-- valid |  |  |-- images |  |  |-- labels |  |-- test |  |  |-- images |  |  |-- labels |  |-- class_dict.csv |  |-- evaluated_classes             Installation  Numpy pip install numpy Pillow pip install pillow OpenCV pip install opencv-python Tensorflow pip install tensorflow-gpu  Note: The recommended version of tensorflow-gpu is 1.14 or 2.0. And if your tensorflow version is lower, you need to modify some API or upgrade your tensorflow.  Usage Download You can download the project through this command: git clone git@github.com:luyanger1799/Amazing-Semantic-Segmentation.git Training The project contains complete codes for training, testing and predicting. And you can perform a simple command as this to build a model on your dataset: python train.py --model FCN-8s --base_model ResNet50 --dataset ""dataset_path"" --num_classes ""num_classes""           The detailed command line parameters are as follows: usage: train.py [-h] --model MODEL [--base_model BASE_MODEL] --dataset DATASET                 [--loss {CE,Focal_Loss}] --num_classes NUM_CLASSES                 [--random_crop RANDOM_CROP] [--crop_height CROP_HEIGHT]                 [--crop_width CROP_WIDTH] [--batch_size BATCH_SIZE]                 [--valid_batch_size VALID_BATCH_SIZE]                 [--num_epochs NUM_EPOCHS] [--initial_epoch INITIAL_EPOCH]                 [--h_flip H_FLIP] [--v_flip V_FLIP]                 [--brightness BRIGHTNESS [BRIGHTNESS ...]]                 [--rotation ROTATION]                 [--zoom_range ZOOM_RANGE [ZOOM_RANGE ...]]                 [--channel_shift CHANNEL_SHIFT]                 [--data_aug_rate DATA_AUG_RATE]                 [--checkpoint_freq CHECKPOINT_FREQ]                 [--validation_freq VALIDATION_FREQ]                 [--num_valid_images NUM_VALID_IMAGES]                 [--data_shuffle DATA_SHUFFLE] [--random_seed RANDOM_SEED]                 [--weights WEIGHTS]            optional arguments:   -h, --help            show this help message and exit   --model MODEL         Choose the semantic segmentation methods.   --base_model BASE_MODEL                         Choose the backbone model.   --dataset DATASET     The path of the dataset.   --loss {CE,Focal_Loss}                         The loss function for traing.   --num_classes NUM_CLASSES                         The number of classes to be segmented.   --random_crop RANDOM_CROP                         Whether to randomly crop the image.   --crop_height CROP_HEIGHT                         The height to crop the image.   --crop_width CROP_WIDTH                         The width to crop the image.   --batch_size BATCH_SIZE                         The training batch size.   --valid_batch_size VALID_BATCH_SIZE                         The validation batch size.   --num_epochs NUM_EPOCHS                         The number of epochs to train for.   --initial_epoch INITIAL_EPOCH                         The initial epoch of training.   --h_flip H_FLIP       Whether to randomly flip the image horizontally.   --v_flip V_FLIP       Whether to randomly flip the image vertically.   --brightness BRIGHTNESS [BRIGHTNESS ...]                         Randomly change the brightness (list).   --rotation ROTATION   The angle to randomly rotate the image.   --zoom_range ZOOM_RANGE [ZOOM_RANGE ...]                         The times for zooming the image.   --channel_shift CHANNEL_SHIFT                         The channel shift range.   --data_aug_rate DATA_AUG_RATE                         The rate of data augmentation.   --checkpoint_freq CHECKPOINT_FREQ                         How often to save a checkpoint.   --validation_freq VALIDATION_FREQ                         How often to perform validation.   --num_valid_images NUM_VALID_IMAGES                         The number of images used for validation.   --data_shuffle DATA_SHUFFLE                         Whether to shuffle the data.   --random_seed RANDOM_SEED                         The random shuffle seed.   --weights WEIGHTS     The path of weights to be loaded.             If you only want to use the model in your own training code, you can do as this: from builders.model_builder import builder  model, base_model = builder(num_classes, input_size, model='SegNet', base_model=None)           Note: If you don't give the parameter ""base_model"", the default backbone will be used. Testing Similarly, you can evaluate the model on your own dataset: python test.py --model FCN-8s --base_model ResNet50 --dataset ""dataset_path"" --num_classes ""num_classes"" --weights ""weights_path""           Note: If the parameter ""weights"" is None, the weigths saved in default path will be loaded. Predicting You can get the prediction of a single RGB image as this: python predict.py --model FCN-8s --base_model ResNet50 --num_classes ""num_classes"" --weights ""weights_path"" --image_path ""image_path""           Evaluating If you already have the predictions of all test images or you don't want to evaluate all classes, you can do as this: python evaluate.py --dataset 'dataset_path' --predictions 'prediction_path'           Note: You must specify the class to be evaluated in dataset/evaluated_classes.txt.  PyPI Alternatively, you can install the project through PyPI. pip install semantic-segmentation           And you can use model_builders to build different models or directly call the class of semantic segmentation. from semantic_segmentation import model_builders net, base_net = model_builders(num_classes, input_size, model='SegNet', base_model=None)           or from semantic_segmentation import models net = models.FCN(num_classes, version='FCN-8s')(input_size=input_size)            Pre-trained Due to my limited computing resources, there is no pre-training model yet. And maybe it will be added in the future.  Feedback If you like this work, please give me a star! And if you find any errors or have any suggestions, please contact me. Email: luyanger1799@outlook.com "
19,lizhengwei1992/Semantic_Human_Matting,452,https://github.com/lizhengwei1992/Semantic_Human_Matting,"Updated on Apr 10, 2019","Semantic_Human_Matting update 2019/04/08 Requirements Usage Step 1: prepare dataset Step 2: build network Step 3: build loss Step 4: train Test      README.md           Semantic_Human_Matting The project is my reimplement of paper (Semantatic Human Matting) from Alibaba,  it proposes a new end-to-end scheme to predict human alpha from image. SHM is the first algorithm that learns to jointly fit both semantic information and high quality details with deep networks. One of the main contributions of the paper is that: A large scale high quality human matting dataset is created. It contains 35,513 unique human images with corresponding alpha mattes. But, the dataset is not avaiable. I collected 6k+ images as my dataset of the project. Worth noting that, the architecture of my network, which builded with mobilenet and shallow encoder-decoder net, is a light version compaired to original implement. update 2019/04/08   The company 爱分割 shared their dataset recently ! Requirements  python3.5 / 3.6 pytorch >= 0.4 opencv-python  Usage Directory structure of the project: Semantic_Human_Matting │   README.md │   train.py │   train.sh |   test_camera.py |   test_camera.sh └───model │   │   M_Net.py │   │   T_Net.py │   │   network.py └───data     │   dataset.py     │   gen_trimap.py     |   gen_trimap.sh     |   knn_matting.py     |   knn_matting.sh     └───image     └───mask     └───trimap     └───alpha           Step 1: prepare dataset ./data/train.txt contain image names according to 6k+ images(./data/image) and corresponding masks(./data/mask). Use ./data/gen_trimap.sh to get trimaps of the masks. Use ./data/knn_matting.sh to get alpha mattes(it will take long time...). Step 2: build network    Trimap generation: T-Net The T-Net plays the role of semantic segmentation. I use mobilenet_v2+unet as T-Net to predict trimap.   Matting network: M-Net The M-Net aims to capture detail information and generate alpha matte. I build M-Net same as the paper, but reduce channels of the original net.   Fusion Module Probabilistic estimation of alpha matte can be written as   Step 3: build loss The overall prediction loss for alpha_p at each pixel is The total loss is Read papers for more details, and my codes for two loss functions:     # -------------------------------------     # classification loss L_t     # ------------------------     criterion = nn.CrossEntropyLoss()     L_t = criterion(trimap_pre, trimap_gt[:,0,:,:].long())      # -------------------------------------     # prediction loss L_p     # ------------------------     eps = 1e-6     # l_alpha     L_alpha = torch.sqrt(torch.pow(alpha_pre - alpha_gt, 2.) + eps).mean()      # L_composition     fg = torch.cat((alpha_gt, alpha_gt, alpha_gt), 1) * img     fg_pre = torch.cat((alpha_pre, alpha_pre, alpha_pre), 1) * img     L_composition = torch.sqrt(torch.pow(fg - fg_pre, 2.) + eps).mean()     L_p = 0.5*L_alpha + 0.5*L_composition           Step 4: train Firstly, pre_train T-Net, use ./train.sh as : python3 train.py \ 	--dataDir='./data' \ 	--saveDir='./ckpt' \ 	--trainData='human_matting_data' \ 	--trainList='./data/train.txt' \ 	--load='human_matting' \ 	--nThreads=4 \ 	--patch_size=320 \ 	--train_batch=8 \ 	--lr=1e-3 \ 	--lrdecayType='keep' \ 	--nEpochs=1000 \ 	--save_epoch=1 \ 	--train_phase='pre_train_t_net'            Then, train end to end, use ./train.sh as: python3 train.py \ 	--dataDir='./data' \ 	--saveDir='./ckpt' \ 	--trainData='human_matting_data' \ 	--trainList='./data/train.txt' \ 	--load='human_matting' \ 	--nThreads=4 \ 	--patch_size=320 \ 	--train_batch=8 \ 	--lr=1e-4 \ 	--lrdecayType='keep' \ 	--nEpochs=2000 \ 	--save_epoch=1 \ 	--finetuning \ 	--train_phase='end_to_end'            Test run ./test_camera.sh "
20,fyu/dilation,749,https://github.com/fyu/dilation,"Updated on Mar 31, 2018","Multi-Scale Context Aggregation by Dilated Convolutions Introduction Citing License Installation Caffe Python Running Demo Training Implementation of Dilated Convolution      README.md           Multi-Scale Context Aggregation by Dilated Convolutions Introduction Properties of dilated convolution are discussed in our ICLR 2016 conference paper. This repository contains the network definitions and the trained models. You can use this code together with vanilla Caffe to segment images using the pre-trained models. If you want to train the models yourself, please check out the document for training. If you are looking for dilation models with state-of-the-art performance and Python implementation, please check out Dilated Residual Networks. Citing If you find the code or the models useful, please cite this paper: @inproceedings{YuKoltun2016, 	author    = {Fisher Yu and Vladlen Koltun}, 	title     = {Multi-Scale Context Aggregation by Dilated Convolutions}, 	booktitle = {ICLR}, 	year      = {2016}, }           License The code and models are released under the MIT License (refer to the LICENSE file for details). Installation Caffe Install Caffe and its Python interface. Make sure that the Caffe version is newer than commit 08c5df. Python The companion Python script is used to demonstrate the network definition and trained weights. The required Python packages are numba numpy opencv. Python release from Anaconda is recommended. In the case of using Anaconda conda install numba numpy opencv           Running Demo predict.py is the main script to test the pre-trained models on images. The basic usage is python predict.py <dataset name> <image path>           Given the dataset name, the script will find the pre-trained model and network definition. We currently support models trained from four datasets: pascal_voc, camvid, kitti, cityscapes. The steps of using the code is listed below:   Clone the code from Github git clone git@github.com:fyu/dilation.git cd dilation             Download pre-trained network sh pretrained/download_pascal_voc.sh             Run pascal voc model on GPU 0 python predict.py pascal_voc images/dog.jpg --gpu 0             Training You are more than welcome to train our model on a new dataset. To do that, please refer to the document for training. Implementation of Dilated Convolution Besides Caffe support, dilated convolution is also implemented in other deep learning packages. For example,  Torch: SpatialDilatedConvolution Lasagne: DilatedConv2DLayer "
21,nightrome/really-awesome-semantic-segmentation,395,https://github.com/nightrome/really-awesome-semantic-segmentation,"Updated on Mar 16, 2018","really-awesome-semantic-segmentation Dataset importance Details Survey papers Online demos      README.md     really-awesome-semantic-segmentation A list of all papers on Semantic Segmentation and the datasets they use. This site is maintained by Holger Caesar. From March 15, 2018, it will not be updated anymore. To complement or correct it, please contact me at holger-at-it-caesar.com or visit it-caesar.com. Also checkout really-awesome-gan and our COCO-Stuff dataset. Dataset importance  Details For details which paper uses which dataset, please open the Google Drive document. Survey papers  RTSeg: Real-time Semantic Segmentation Comparative Study Indoor Scene Understanding in 2.5/3D: A Survey A 2017 Guide to Semantic Segmentation with Deep Learning by Qure AI A Review on Deep Learning Techniques Applied to Semantic Segmentation Computer Vision for Autonomous Vehicles: Problems, Datasets and State-of-the-Art [Webpage]  Online demos  CRF as RNN SegNet "
22,TuSimple/TuSimple-DUC,577,https://github.com/TuSimple/TuSimple-DUC,"Updated on Oct 25, 2021","TuSimple-DUC Introduction Requirement Usage Citation Questions      README.md           TuSimple-DUC by Panqu Wang, Pengfei Chen, Ye Yuan, Ding Liu, Zehua Huang, Xiaodi Hou, and Garrison Cottrell. Introduction This repository is for Understanding Convolution for Semantic Segmentation (WACV 2018), which achieved state-of-the-art result on the CityScapes, PASCAL VOC 2012, and Kitti Road benchmark. Requirement We tested our code on: Ubuntu 16.04, Python 2.7 with MXNet (0.11.0), numpy(1.13.1), cv2(3.2.0), PIL(4.2.1), and cython(0.25.2) Usage   Clone the repository: git clone git@github.com:TuSimple/TuSimple-DUC.git python setup.py develop --user            Download the pretrained model from Google Drive.   Build MXNet (only tested on the TuSimple version): git clone --recursive git@github.com:TuSimple/mxnet.git vim make/config.mk (we should have USE_CUDA = 1, modify USE_CUDA_PATH, and have USE_CUDNN = 1 to enable GPU usage.) make -j cd python python setup.py develop --user          For more MXNet tutorials, please refer to the official documentation.   Training: cd train python train_model.py ../configs/train/train_cityscapes.cfg          The paths/dirs in the .cfg file need to be specified by the user.   Testing cd test python predict_full_image.py ../configs/test/test_full_image.cfg           The paths/dirs in the .cfg file need to be specified by the user.   Results: Modify the result_dir path in the config file to save the label map and visualizations. The expected scores are: (single scale testing denotes as 'ss' and multiple scale testing denotes as 'ms')  ResNet101-DUC-HDC on CityScapes testset (mIoU): 79.1(ss) / 80.1(ms) ResNet152-DUC on VOC2012 (mIoU): 83.1(ss)    Citation If you find the repository is useful for your research, please consider citing: @article{wang2017understanding,   title={Understanding convolution for semantic segmentation},   author={Wang, Panqu and Chen, Pengfei and Yuan, Ye and Liu, Ding and Huang, Zehua and Hou, Xiaodi and Cottrell, Garrison},   journal={arXiv preprint arXiv:1702.08502},   year={2017} }           Questions Please contact panqu.wang@tusimple.ai or pengfei.chen@tusimple.ai . "
23,BBuf/Keras-Semantic-Segmentation,270,https://github.com/BBuf/Keras-Semantic-Segmentation,"Updated on Sep 25, 2020","Keras-Sematic-Segmentation 配置 目录结构 已支持的分割模型  已支持的损失函数 已支持的评价指标 已支持训练可视化 训练 训练示例 测试 测试示例 数据增强 数据集 Model Zoo 模型部署 已支持OP 已支持网络 标准 个人制作2个类别小零件数据集分割结果 CamVid分割数据集分割结果 眼球血管分割数据集 个人制作2个类别小零件数据集分割可视化结果 CamVid数据集分割可视化结果 眼球病变数据集分割可视化结果 参考 微信公众号&交流群      README.md           Keras-Sematic-Segmentation  使用Keras实现深度学习中的一些语义分割模型。  配置  tensorflow 1.13.1+tensorboard keras 2.2.4 GTX 2080Ti x 2 Cuda 10.0 + Cudnn7 opencv-python labelme（标注数据需要用） PyCaffe（模型部署时用）  目录结构  data 存储输入图像和语义分割标签的文件夹  - data 	- dataset_name 		- train_image 		- train_label 		- test_image 		- test_label           Models 存储使用keras实现的一些经典分割模型 utils 存储工具代码，如数据预处理，自定义resize方式等 losses 常见的分割损失函数如Dice Loss，Tversky Loss等 metrics 常见的分割评价指标，比如dice分数，f1分数等 tools 模型转换工具，将输出的Keras模型转为caffe模型，再转到NCNN/TensorRT/OpenVINO等推理框架进行部署 data.py 加载1个batch的原始图片和分割标签图片 train.py 模型训练 test.py 模型测试 json_to_dataset.py 批量处理多张图片并一步建好所需目录及相关mask文件  已支持的分割模型    model_name Base Model Segmentation Model Params FLOPs Model Size Available     enet ENet Enet 371,558 759,829 1.4Mb True   fcn8 Vanilla CNN FCN8 3,609,196 7220708 29.0Mb True   unet Vanilla CNN UNet 7,167,618 14,344,197 57.6Mb True   attunet Vanilla CNN AttUNet 8,913,058 17,841,087 71.7Mb True   r2unet Vanilla CNN R2UNet 17,652,930 51,065,008 141.7Mb True   r2attunet Vanilla CNN R2AttUNet 16,958,530 46,532,640 136.2Mb True   unet++ Vanilla CNN NestedUNet 9,171,170 18,353,631 73.7Mb True   segnet Vanilla CNN SegNet 2,941,218 5,888,377 11.9Mb True   icnet Vanilla CNN ICNet 6,740,610 13,524,726 27.6Mb True   pspnet* Vanilla CNN PSPNet 964,226 8,894,120 3.9Mb True   mobilenet_unet MobileNet MobileNetUnet 407,778 825,856 1.9Mb True   mobilenet_fcn8 MobileNet MobileNetFCN8 3,432,764 6,880,358 14Mb False   seunet SENet SEUNet 1,964,530 3,932,843 8.2Mb True   scseunet SCSENet scSEUNet 1,959,266 3,923,359 8.1Mb True   vggunet VGGNet VGGUnet 25,884,170 51,789,952 103.8Mb True   unet_xception_resnetblock XceptionNet Unet_Xception_ResNetBlock 38,431,730 88,041,130 154.5Mb True   deeplab_v2 DeepLab DeepLabV2 37,799,752 75,574,697 151.3Mb True   hrnet HRNet HRNet 9524168 57,356,440 117.1Mb True    注：测试数据是基于输入图片大小为224x224的二分类模型。对于标*号的模型，图片大小为模型定义里支持的最小大小。  已支持的损失函数    Name (as argument) Type Available     ce Cross Entropy Yes   weighted_ce Weighted Categorical loss Yes   b_focal Binary Focal loss Yes   c_focal Categorical Focal loss Yes   dice Dice loss Yes   bce_dice BCE + Dice loss Yes   ce_dice CE + Dice loss Yes   g_dice Generalized Dice loss Yes   jaccard Jaccard loss Yes   bce_jaccard BCE + Jaccard loss Yes   ce_jaccard CE + Jaccard loss Yes   tversky Tversky loss Yes   f_tversky Focal Tversky loss Yes    注：weighted_ce 以及 c_focal 需要指定对应class的权重或者指定class数量。默认值为平分权重的二分类。 已支持的评价指标    Type Available     iou_score Yes   jaccard_score Yes   f1_score Yes   f2_score Yes   dice_score Yes    已支持训练可视化 为了更好的监督训练过程,我们已经提供了训练可视化,对损失函数,iou_score,dice_socoe,f1_score,f2_score等进行了可视化.在训练过程中会在模型保 存的文件夹下生成log文件夹,例如weights/unet/log,然后使用tensorboard --logdir=weights/unet/log,打开得到的网址即可获得可视化结果. 训练 使用下面的命令训练和保存模型，模型保存路径，训练超参数需要灵活设置。 export CUDA_VISIBLE_DEVICES=0,1 # 使用的GPU序号 python train.py ...          可用参数如下：  --exp_name 字符串，代表此次实验的名称，默认exp1。 --dataset_name 字符串，代表选择对应的数据集的名称，默认bbufdataset，支持camvid。 --loss 字符串，代表选择的损失函数的名称，默认ce，全部名称见支持的损失函数。 --n_classes 整型，代表分割图像中有几种类别的像素，默认为2。 --input_height整型，代表要分割的图像需要resize的长，默认为224。 --input_width 整型，代表要分割的图像需要resize的宽，默认为224。 --resize_op 整型，代表resize的方式，如果为1则为普通resize，如果为2，则为letterbox_resize，默认为1。 --validate布尔型，代表训练过程中是否需要验证集，默认为True，即使用验证集。 --epochs整型，代表要训练多少个epoch，默认为50。 --train_batch_size整型，代表训练时批量大小，默认为4。 --val_batch_size整型，代表训练时批量大小，默认为4。 --model_name  字符串类型，代表训练时使用哪个模型，支持enet,unet,segnet,fcn8等多种模型，默认为unet。 --train_save_path字符串类型，代表训练时保存模型的路径，默认为weights/unet，即会将模型保存在weights文件夹下，并且每个模型名字前缀以unet开头，后面接迭代次数和准确率构成完整的保存模型的路径。 --resume字符串类型，代表继续训练的时候加载的模型路径，默认值为``，即从头训练。 --optimizer_name字符串类型，代表训练模型时候的优化方法，支持sgd,adam,adadelta等多种优化方式，默认为adadelta。 --image_init字符串类型，代表输入图片初始化方式，支持sub_mean，sub_and_divide，divide，默认为divide。 --multi_gpus 布尔类型，代表使用是否多卡进行训练，默认为Fasle，如果为True，需将gpu_count参数设置为使用的显卡数量 --gpu_count 整型，当multi_gpus为True时代表使用的GPU数量。需要配合设置相应的环境变量CUDA_VISIBLE_DEVICES。  训练示例  训练本工程提供的二分类数据集：python train.py --dataset_name bbufdataset --model_name unet --input_height 224 --input_width 224 --image_init divide --n_classes 2 训练CamVid数据集：python train.py --dataset_name camvid --model_name unet --input_height 720 --input_width 960 --image_init sub_mean --n_classes 32 --train_batch_size 2 --val_batch_size 2  测试 使用下面的命令测试模型，加载模型的路径，图像输入分辨率等参数需要灵活设置。 python test.py ...          可用参数如下：  --test_images字符串类型，代表测试图所在的文件夹路径，默认为data/test/。 --output_path字符串类型，代表从测试图预测出的mask图输出路径，默认为data/output/。 --model_name 字符串类型，代表测试时使用哪个模型，支持enet,unet,segnet,fcn8等多种模型，默认为unet。 --weights_path字符串类型，代表预测时加载的模型权重，默认为weights/unet.18-0.856895.hdf5，即对应默认模型unet训练出来的模型权重路径。 --input_height整型，代表测试集输入到网络中需要被resize的长，默认为224。 --input_width整型，代表测试集输入到网络中需要被resize的宽，默认为224。 --resize_op 整型，代表resize的方式，如果为1则为普通resize，如果为2，则为letterbox_resize，默认为1。 --classes整型，代表图片中的像素类别数，默认为2。 --mIOU布尔型，代表是否启用评测mIOU，默认为False，一旦启用需要提供带有mask图的测试数据集。 --val_images字符串类型，代表启用mIOU后测试集原图的路径，默认为data/val_image/。 --val_annotations字符串类型，代表启用mIOU后测试集mask图的路径，默认为data/val_label/。 --image_init字符串类型，代表输入图片初始化方式，支持sub_mean，sub_and_divide，divide，默认为divide。  测试示例  测试二分类数据集：python test.py --model_name  unet --weights_path weights/unet.xx.hdf5 --classes 2 --image_init divide 测试CamVid数据集：python test.py --model_name unet --weights_path weights/unet.xx.hdf5 --classes 32 --image_init sub_mean --input_height 720 --input_width 960  数据增强 实现中... 数据集 数据集制作使用Labelme即可，然后将得到的json文件使用json_to_dataset.py转换为本工程要用的mask标签图，具体操作步骤为：  使用本工程中的json_to_dataset.py替换掉labelme/cli中的相应文件—json_to_dataset.py 。在cmd中输入python json_to_dateset.py  /path/你的json文件夹的路径。注意是把每张图的json文件都放在一个目录下，labelme标注出来的默认是一张图片一个文件夹。 运行后，在json文件夹中会出现mask_png、labelme_json文件夹，mask_png中存放的是所有8位掩码文件！也即是本工程中使用的标签图。 具体来说，我们的标签图就是分别指示每张图片上每一个位置的像素属于几，0是背景，然后你要的类别从1开始往后递增即可。  本工程训练和测试的一个2类的简单分割数据集，下载地址为：https://pan.baidu.com/s/1sVjBfmgALVK7uEjeWgIMug 本工程训练和测试的CamVid数据集，下载地址为：https://pan.baidu.com/s/1zequLd0aYXNseGoXn-tdog 本工程训练和测试的一个包的数据集，下载地址为：https://pan.baidu.com/s/1iau4E0tjm6179z_XTSqExw，提取码：sg22  Model Zoo 已经训练好的Keras模型放在这个工程下，模型按照名字进行对应： https://github.com/BBuf/Keras-Semantic-Segmentation-Model-Zoo 模型部署 首先将Keras模型转为Caffe模型，然后再转为NCNN/OpenVINO/TensorRT/M模型进行部署，已支持转换OP和网络如下。 已支持OP  InputLayer Conv2D/Convolution2D Conv2DTranspose DepthwiseConv2D SeparableConv2D BatchNormalization Dense ReLU ReLU6 LeakyReLU SoftMax SigMoid Cropping2D Concatenate Merge Add Flatten Reshape MaxPooling2D AveragePooling2D Dropout GlobalAveragePooling2D UpSampling2D ...  已支持网络  VGG16 SqueezeNet InceptionV3 InceptionV4 Xception V1 UNet ...  标准 个人制作2个类别小零件数据集分割结果    Resolution ResizeOp Epoch model_name Base Model Segmentation Model Acc iou_score dice_score f1_score f2_score     224x224 1 50 enet ENet Enet        224x224 1 50 fcn8 Vanilla CNN FCN8        224x224 1 50 unet Vanilla CNN UNet        224x224 1 50 attunet Vanilla CNN AttUNet        224x224 1 50 r2unet Vanilla CNN R2UNet        224x224 1 50 r2attunet Vanilla CNN R2AttUNet        224x224 1 50 unet++ Vanilla CNN NestedUNet        224x224 1 50 segnet Vanilla CNN SegNet        224x224 1 50 icnet Vanilla CNN ICNet        384x384 1 50 pspnet Vanilla CNN PSPNet        224x224 1 50 mobilenet_unet MobileNet MobileNetUnet        224x224 1 50 mobilenet_fcn8 MobileNet MobileNetFCN8        224x224 1 50 seunet SENet SEUNet        224x224 1 50 scseunet SCSENet scSEUNet        224x224 1 50 vggunet VGGNet VGGUnet        224x224 1 50 unet_xception_resnetblock XceptionNet Unet_Xception_ResNetBlock        320x320 1 50 deeplab_v2 DeepLab DeepLabV2        224x224 1 50 hrnet HRNet HRNet         CamVid分割数据集分割结果    Resolution ResizeOp Epoch model_name Base Model Segmentation Model Acc iou_score dice_score f1_score f2_score     960x704 1 50 enet ENet Enet        960x704 1 50 fcn8 Vanilla CNN FCN8        960x720 1 50 unet Vanilla CNN UNet 0.70 0.51 0.67 0.67     1 50 attunet Vanilla CNN AttUNet         1 50 r2unet Vanilla CNN R2UNet         1 50 r2attunet Vanilla CNN R2AttUNet         1 50 unet++ Vanilla CNN NestedUNet         1 50 segnet Vanilla CNN SegNet         1 50 icnet Vanilla CNN ICNet         1 50 pspnet Vanilla CNN PSPNet         1 50 mobilenet_unet MobileNet MobileNetUnet         1 50 mobilenet_fcn8 MobileNet MobileNetFCN8         1 50 seunet SENet SEUNet         1 50 scseunet SCSENet scSEUNet         1 50 vggunet VGGNet VGGUnet         1 50 unet_xception_resnetblock XceptionNet Unet_Xception_ResNetBlock         1 50 deeplab_v2 DeepLab DeepLabV2         1 50 hrnet HRNet HRNet         眼球血管分割数据集  全部基于UNet进行测试，这个数据集是为了测试工程中支持的各种损失函数的效果。以下指标在测试集上报告。     Resolution ResizeOp Epoch Loss Name Acc iou_score dice_score f1_score f2_score     224x224 1 50 ce 0.9238 0.7817 0.8770 0.8770 0.8770    1 50 weighted_ce 0.9194 0.8329 0.9088 0.9088 0.9088    1 50 b_focal 0.9106 0.6103 0.7579 0.7579 0.7579    1 50 c_focal 0.9194 0.8301 0.9071 0.9071 0.9071    1 50 dice 0.9198 0.7429 0.8523 0.8523 0.8523    1 50 bce_dice 0.9307 0.7628 0.8653 0.8653 0.8653    1 50 ce_dice         1 50 g_dice         1 50 jaccard         1 50 bce_jaccard         1 50 ce_jaccard         1 50 tversky         1 50 f_tversky         个人制作2个类别小零件数据集分割可视化结果    Input Image Output Segmentation Image          CamVid数据集分割可视化结果    Input Image Output Segmentation Image          眼球病变数据集分割可视化结果    Input Image Output Segmentation Image          参考  https://github.com/divamgupta/image-segmentation-keras https://github.com/uhfband/keras2caffe  微信公众号&交流群   QQ群号 1030186545 加群密码 qqq "
24,arahusky/Tensorflow-Segmentation,264,https://github.com/arahusky/Tensorflow-Segmentation,"Updated on Dec 19, 2017","Image segmentation Project overview Model architecture General overview Project Dataset Results: Requirements: References: Collaborators      README.md           Image segmentation This project implements neural network for semantic segmentation in Tensorflow . Project overview The main file of the project is convolutional_autoencoder.py, which contains code for dataset processing (class Dataset), model definition (class Model) and also code for training. To abstract layers in the model, we created layer.py class interface. This class has currently two implementations: conv2d.py and max_pool_2d.py. To infer on the trained model, have a look at infer.py file. Finally, there are several folders:  data* contain preprocessed dataset (Please note that current model implementation is supposed to work with at least 128x128 images.) imgaug contains code for data augmentation (https://github.com/aleju/imgaug) noteboks contains some interesting image segmentation ipython notebooks  Model architecture General overview There are many neural network architectures for semantic image segmentation (to have some basic overview, you can read project_summary.pdf), but most of them use convolutional encoder-decoder architecture.  Convolutional encoder-decoder architecture of popular SegNet model Encoder in these networks has in many cases structure similar to some image classification neural network (e.g. vgg-16). Layers in the decoder are then ussualy inverse to layers used in the encoder (e.g. for convolution that makes its input smaller, we use deconvolution; for max_pool we use some form of ""demax_pool""). Project Inspired by previous success of convolutional encoder-decoder architectures, we decided to implement it as well. In the encoder part, we use three similar ""modules"", each consisting of convolution layer with stride 2 followed by convolutution layer with stride 1 and no-overlapping max_pool with kernel 2. The decoder section then for each layer in the encoder contains its ""counter-part"" (network output dimension == input dimension):  for no-shrinking convolution layer use the same layer for shrinking convolution layer use transposed deconvolution with same arguments for max pool layer use nearest neighbour upsampling (tf.image.resize_nearest_neighbor)  We also found that adding skip-links from encoder to decoder makes the model perform better (~1%).  *Convolutional encoder-decoder architecture used in this project*  Dataset This project uses publicly available dataset of faces: http://vis-www.cs.umass.edu/lfw/part_labels. The repository contains three versions of this dataset differing in the image resolution (28x28, 128x128, 250x250). The original dataset consists of three target categories (face, hair, background). To make the segmentation easier, we decided to create two subsets of original targets: one containing merged hair and background classes(""targets_face_only"") and other containing merged hair and face classes(""targets""). Results: We experimented with several architectures (some of them are mentioned in project_summary.pdf). Even though original images are RGB, we decided to use them in grayscale. The best performance we managed to achieve on 128x128 images was 97.36% in means of per-pixel accuracy.  *Sample results of the best model segmentation. The first row contains input faces, second ground truth image segmentation, third model output and the fourth row shows thresholded model output*  Requirements:  Python 3.5 Tensorflow > 1.0 Opencv 3.x  References:   Tensorflow:  FCN implementation: https://github.com/MarvinTeichmann/tensorflow-fcn SegNet implementation: https://github.com/tkuanlun350/Tensorflow-SegNet Some interesting segmentation notebooks: https://github.com/warmspringwinds/tensorflow_notes    Papers:  Fully convolutional networks (Nov 14) : https://people.eecs.berkeley.edu/~jonlong/long_shelhamer_fcn.pdf SegNet: https://arxiv.org/pdf/1511.00561.pdf DeconvNet (May 15): https://arxiv.org/pdf/1505.04366v1.pdf DeepLab-LargeFOV (Dec 14): https://arxiv.org/pdf/1412.7062v4.pdf    Collaborators  Jakub Naplava Jan Kluj Ondrej Svec "
25,isl-org/Open3D-PointNet2-Semantic3D,396,https://github.com/isl-org/Open3D-PointNet2-Semantic3D,"Updated on Mar 23, 2021","Semantic3D semantic segmentation with Open3D and PointNet++ Intro Usage 1. Download 2. Convert txt to pcd file 3. Downsample 4. Compile TF Ops 5. Train 6. Predict 7. Interpolate 8. Submission Summary of directories      README.md           Semantic3D semantic segmentation with Open3D and PointNet++ Intro Demo project for Semantic3D (semantic-8) segmentation with Open3D and PointNet++. The purpose of this project is to showcase the usage of Open3D in deep learning pipelines and provide a clean baseline implementation for semantic segmentation on Semantic3D dataset.  Here's our entry on the semantic-8 test benchmark page. Open3D is an open-source library that supports rapid development of software that deals with 3D data. The Open3D frontend exposes a set of carefully selected data structures and algorithms in both C++ and Python. The backend is highly optimized and is set up for parallelization. We welcome contributions from the open-source community. In this project, Open3D was used for  Point cloud data loading, writing, and visualization. Open3D provides efficient implementations of various point cloud manipulation methods. Data pre-processing, in particular, voxel-based down-sampling. Point cloud interpolation, in particular, fast nearest neighbor search for label interpolation. And more.  This project is forked from Mathieu Orhan and Guillaume Dekeyser's repo, which, is forked from the original PointNet2. We thank the original authors for sharing their methods. Usage 1. Download Download the dataset Semantic3D and extract it by running the following commands: cd dataset/semantic_raw. bash download_semantic3d.sh. Open3D-PointNet2-Semantic3D/dataset/semantic_raw ├── bildstein_station1_xyz_intensity_rgb.labels ├── bildstein_station1_xyz_intensity_rgb.txt ├── bildstein_station3_xyz_intensity_rgb.labels ├── bildstein_station3_xyz_intensity_rgb.txt ├── ...          2. Convert txt to pcd file Run python preprocess.py          Open3D is able to read .pcd files much more efficiently. Open3D-PointNet2-Semantic3D/dataset/semantic_raw ├── bildstein_station1_xyz_intensity_rgb.labels ├── bildstein_station1_xyz_intensity_rgb.pcd (new) ├── bildstein_station1_xyz_intensity_rgb.txt ├── bildstein_station3_xyz_intensity_rgb.labels ├── bildstein_station3_xyz_intensity_rgb.pcd (new) ├── bildstein_station3_xyz_intensity_rgb.txt ├── ...          3. Downsample Run python downsample.py          The downsampled dataset will be written to dataset/semantic_downsampled. Points with label 0 (unlabled) are excluded during downsampling. Open3D-PointNet2-Semantic3D/dataset/semantic_downsampled ├── bildstein_station1_xyz_intensity_rgb.labels ├── bildstein_station1_xyz_intensity_rgb.pcd ├── bildstein_station3_xyz_intensity_rgb.labels ├── bildstein_station3_xyz_intensity_rgb.pcd ├── ...          4. Compile TF Ops We need to build TF kernels in tf_ops. First, activate the virtualenv and make sure TF can be found with current python. The following line shall run without error. python -c ""import tensorflow as tf""          Then build TF ops. You'll need CUDA and CMake 3.8+. cd tf_ops mkdir build cd build cmake .. make          After compilation the following .so files shall be in the build directory. Open3D-PointNet2-Semantic3D/tf_ops/build ├── libtf_grouping.so ├── libtf_interpolate.so ├── libtf_sampling.so ├── ...          Verify that that the TF kernels are working by running cd .. # Now we're at Open3D-PointNet2-Semantic3D/tf_ops python test_tf_ops.py          5. Train Run python train.py          By default, the training set will be used for training and the validation set will be used for validation. To train with both training and validation set, use the --train_set=train_full flag. Checkpoints will be output to log/semantic. 6. Predict Pick a checkpoint and run the predict.py script. The prediction dataset is configured by --set. Since PointNet2 only takes a few thousand points per forward pass, we need to sample from the prediction dataset multiple times to get a good coverage of the points. Each sample contains the few thousand points required by PointNet2. To specify the number of such samples per scene, use the --num_samples flag. python predict.py --ckpt log/semantic/best_model_epoch_040.ckpt \                   --set=validation \                   --num_samples=500          The prediction results will be written to result/sparse. Open3D-PointNet2-Semantic3D/result/sparse ├── sg27_station4_intensity_rgb.labels ├── sg27_station4_intensity_rgb.pcd ├── sg27_station5_intensity_rgb.labels ├── sg27_station5_intensity_rgb.pcd ├── ...           7. Interpolate The last step is to interpolate the sparse prediction to the full point cloud. We use Open3D's K-NN hybrid search with specified radius. python interpolate.py          The prediction results will be written to result/dense. Open3D-PointNet2-Semantic3D/result/dense ├── sg27_station4_intensity_rgb.labels ├── sg27_station5_intensity_rgb.labels ├── ...          8. Submission Finally, if you're submitting to Semantic3D benchmark, we've included a handy tools to rename the submission file names. python renamer.py          Summary of directories  dataset/semantic_raw: Raw Semantic3D data, .txt and .labels files. Also contains the .pcd file generated by preprocess.py. dataset/semantic_downsampled: Generated from downsample.py. Downsampled data, contains .pcd and .labels files. result/sparse: Generated from predict.py. Sparse predictions, contains .pcd and .labels files. result/dense: Dense predictions, contains .labels files. result/dense_label_colorized: Dense predictions with points colored by label type. "
26,MSiam/TFSegmentation,586,https://github.com/MSiam/TFSegmentation,"Updated on Sep 7, 2021","Real-time Semantic Segmentation Comparative Study Description Models Reported Results Test Set Validation Set Usage Run Examples to the running command in run.sh file: Main Dependencies All Dependencies Citation License Related Project      README.md           Real-time Semantic Segmentation Comparative Study The repository contains the official TensorFlow code used in our papers:  RTSEG: REAL-TIME SEMANTIC SEGMENTATION COMPARATIVE STUDY for comparing different realtime semantic segmentation architectures. SHUFFLESEG: REAL-TIME SEMANTIC SEGMENTATION NETWORK which introduces a new fast realtime semantic segmentation network based on the ShuffleNet unit.  Description Semantic segmentation benefits robotics related applications especially autonomous driving. Most of the research on semantic segmentation is only on increasing the accuracy of segmentation models with little attention to computationally efficient solutions. The few work conducted in this direction does not provide principled methods to evaluate the      different design choices for segmentation. In RTSeg, we address this gap by presenting a real-time semantic segmentation benchmarking framework with a decoupled design for feature extraction and decoding methods. The code and the experimental results are presented on the CityScapes dataset for urban scenes.    Models    Encoder Skip U-Net DilationV1 DilationV2     VGG-16 Yes Yes Yes No   ResNet-18 Yes Yes Yes No   MobileNet Yes Yes Yes Yes   ShuffleNet Yes Yes Yes Yes    NOTE: The rest of the pretrained weights for all the implemented models will be released soon. Stay in touch for the updates. Reported Results Test Set    Model GFLOPs Class IoU Class iIoU Category IoU Category iIoU     SegNet 286.03 56.1 34.2 79.8 66.4   ENet 3.83 58.3 24.4 80.4 64.0   DeepLab - 70.4 42.6 86.4 67.7   SkipNet-VGG16 - 65.3 41.7 85.7 70.1   ShuffleSeg 2.0 58.3 32.4 80.2 62.2   SkipNet-MobileNet 6.2 61.5 35.2 82.0 63.0    Validation Set    Encoder Decoder Coarse mIoU     MobileNet SkipNet No 61.3   ShuffleNet SkipNet No 55.5   ResNet-18 UNet No 57.9   MobileNet UNet No 61.0   ShuffleNet UNet No 57.0   MobileNet Dilation No 57.8   ShuffleNet Dilation No 53.9   MobileNet SkipNet Yes 62.4   ShuffleNet SkipNet Yes 59.3    ** GFLOPs is computed on image resolution 360x640. However, the mIOU(s) are computed on the official image resolution required by CityScapes evaluation script 1024x2048.** ** Regarding Inference time, issue is reported here. We were not able to outperform the reported inference time from ENet architecture it could be due to discrepencies in the optimization we perform. People are welcome to improve on the optimization method we're using. Usage  Download the weights, processed data, and trained meta graphs from here Extract pretrained_weights.zip Extract full_cityscapes_res.zip under data/ Extract unet_resnet18.zip under experiments/  Run The file named run.sh provide a good example for running different architectures. Have a look at this file. Examples to the running command in run.sh file: python3 main.py --load_config=[config_file_name].yaml [train/test] [Trainer Class Name] [Model Class Name]            Remove comment from run.sh for running fcn8s_mobilenet on the validation set of cityscapes to get its mIoU. Our framework evaluation will produce results lower than the cityscapes evaluation script by small difference, for the final evaluation we use the cityscapes evaluation script. UNet ResNet18 should have 56% on validation set, but with cityscapes script we got 57.9%. The results on the test set for SkipNet-MobileNet and SkipNet-ShuffleNet are publicly available on the Cityscapes Benchmark.  python3 main.py --load_config=unet_resnet18_test.yaml test Train LinkNET            To measure running time, run in inference mode.  python3 main.py --load_config=unet_resnet18_test.yaml inference Train LinkNET            To run on different dataset or model, take one of the configuration files such as: config/experiments_config/unet_resnet18_test.yaml and modify it or create another .yaml configuration file depending on your needs.  NOTE: The current code does not contain the optimized code for measuring inference time, the final code will be released soon. Main Dependencies Python 3 and above tensorflow 1.3.0/1.4.0 numpy 1.13.1 tqdm 4.15.0 matplotlib 2.0.2 pillow 4.2.1 PyYAML 3.12           All Dependencies pip install -r [requirements_gpu.txt] or [requirements.txt]           Citation If you find RTSeg useful in your research, please consider citing our work: @ARTICLE{2018arXiv180302758S,    author = {{Siam}, M. and {Gamal}, M. and {Abdel-Razek}, M. and {Yogamani}, S. and     {Jagersand}, M.},     title = ""{RTSeg: Real-time Semantic Segmentation Comparative Study}"",   journal = {ArXiv e-prints}, archivePrefix = ""arXiv"",    eprint = {1803.02758},  primaryClass = ""cs.CV"",  keywords = {Computer Science - Computer Vision and Pattern Recognition},      year = 2018,     month = mar,    adsurl = {http://adsabs.harvard.edu/abs/2018arXiv180302758S},   adsnote = {Provided by the SAO/NASA Astrophysics Data System} }           If you find ShuffleSeg useful in your research, please consider citing it as well: @ARTICLE{2018arXiv180303816G,    author = {{Gamal}, M. and {Siam}, M. and {Abdel-Razek}, M.},     title = ""{ShuffleSeg: Real-time Semantic Segmentation Network}"",   journal = {ArXiv e-prints}, archivePrefix = ""arXiv"",    eprint = {1803.03816},  primaryClass = ""cs.CV"",  keywords = {Computer Science - Computer Vision and Pattern Recognition},      year = 2018,     month = mar,    adsurl = {http://adsabs.harvard.edu/abs/2018arXiv180303816G},   adsnote = {Provided by the SAO/NASA Astrophysics Data System} }           License This project is licensed under the Apache License 2.0 - see the LICENSE file for details. Related Project Real-time Motion Segmentation using 2-stream shuffleseg Code "
27,milesial/Pytorch-UNet,4.8k,https://github.com/milesial/Pytorch-UNet,Updated 19 days ago,"U-Net: Semantic segmentation with PyTorch Quick start Without Docker With Docker Description Usage Docker Training Prediction Weights & Biases Pretrained model Data      README.md           U-Net: Semantic segmentation with PyTorch      Customized implementation of the U-Net in PyTorch for Kaggle's Carvana Image Masking Challenge from high definition images.  Quick start  Without Docker With Docker   Description Usage  Docker Training Prediction   Weights & Biases Pretrained model Data  Quick start Without Docker   Install CUDA   Install PyTorch   Install dependencies   pip install -r requirements.txt           Download the data and run training:  bash scripts/download_data.sh python train.py --amp          With Docker  Install Docker 19.03 or later:  curl https://get.docker.com | sh && sudo systemctl --now enable docker           Install the NVIDIA container toolkit:  distribution=$(. /etc/os-release;echo $ID$VERSION_ID) \    && curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add - \    && curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | sudo tee /etc/apt/sources.list.d/nvidia-docker.list sudo apt-get update sudo apt-get install -y nvidia-docker2 sudo systemctl restart docker           Download and run the image:  sudo docker run --rm --shm-size=8g --ulimit memlock=-1 --gpus all -it milesial/unet           Download the data and run training:  bash scripts/download_data.sh python train.py --amp          Description This model was trained from scratch with 5k images and scored a Dice coefficient of 0.988423 on over 100k test images. It can be easily used for multiclass segmentation, portrait segmentation, medical segmentation, ... Usage Note : Use Python 3.6 or newer Docker A docker image containing the code and the dependencies is available on DockerHub. You can download and jump in the container with (docker >=19.03): docker run -it --rm --shm-size=8g --ulimit memlock=-1 --gpus all milesial/unet          Training > python train.py -h usage: train.py [-h] [--epochs E] [--batch-size B] [--learning-rate LR]                 [--load LOAD] [--scale SCALE] [--validation VAL] [--amp]  Train the UNet on images and target masks  optional arguments:   -h, --help            show this help message and exit   --epochs E, -e E      Number of epochs   --batch-size B, -b B  Batch size   --learning-rate LR, -l LR                         Learning rate   --load LOAD, -f LOAD  Load model from a .pth file   --scale SCALE, -s SCALE                         Downscaling factor of the images   --validation VAL, -v VAL                         Percent of the data that is used as validation (0-100)   --amp                 Use mixed precision          By default, the scale is 0.5, so if you wish to obtain better results (but use more memory), set it to 1. Automatic mixed precision is also available with the --amp flag. Mixed precision allows the model to use less memory and to be faster on recent GPUs by using FP16 arithmetic. Enabling AMP is recommended. Prediction After training your model and saving it to MODEL.pth, you can easily test the output masks on your images via the CLI. To predict a single image and save it: python predict.py -i image.jpg -o output.jpg To predict a multiple images and show them without saving them: python predict.py -i image1.jpg image2.jpg --viz --no-save > python predict.py -h usage: predict.py [-h] [--model FILE] --input INPUT [INPUT ...]                   [--output INPUT [INPUT ...]] [--viz] [--no-save]                   [--mask-threshold MASK_THRESHOLD] [--scale SCALE]  Predict masks from input images  optional arguments:   -h, --help            show this help message and exit   --model FILE, -m FILE                         Specify the file in which the model is stored   --input INPUT [INPUT ...], -i INPUT [INPUT ...]                         Filenames of input images   --output INPUT [INPUT ...], -o INPUT [INPUT ...]                         Filenames of output images   --viz, -v             Visualize the images as they are processed   --no-save, -n         Do not save the output masks   --mask-threshold MASK_THRESHOLD, -t MASK_THRESHOLD                         Minimum probability value to consider a mask pixel white   --scale SCALE, -s SCALE                         Scale factor for the input images          You can specify which model file to use with --model MODEL.pth. Weights & Biases The training progress can be visualized in real-time using Weights & Biases.  Loss curves, validation curves, weights and gradient histograms, as well as predicted masks are logged to the platform. When launching a training, a link will be printed in the console. Click on it to go to your dashboard. If you have an existing W&B account, you can link it by setting the WANDB_API_KEY environment variable. Pretrained model A pretrained model is available for the Carvana dataset. It can also be loaded from torch.hub: net = torch.hub.load('milesial/Pytorch-UNet', 'unet_carvana', pretrained=True)          The training was done with a 50% scale and bilinear upsampling. Data The Carvana data is available on the Kaggle website. You can also download it using the helper script: bash scripts/download_data.sh           The input images and target masks should be in the data/imgs and data/masks folders respectively (note that the imgs and masks folder should not contain any sub-folder or any other files, due to the greedy data-loader). For Carvana, images are RGB and masks are black and white. You can use your own dataset as long as you make sure it is loaded properly in utils/data_loading.py.  Original paper by Olaf Ronneberger, Philipp Fischer, Thomas Brox: U-Net: Convolutional Networks for Biomedical Image Segmentation "
28,SimJeg/FC-DenseNet,472,https://github.com/SimJeg/FC-DenseNet,"Updated on Feb 16, 2018","Introduction Installation Data Run experiments Use a pretrained model About the ""m"" number in the paper      README.md           Introduction This repo contains the code to train and evaluate FC-DenseNets as described in The One Hundred Layers Tiramisu: Fully Convolutional DenseNets for Semantic Segmentation. We investigate the use of Densely Connected Convolutional Networks for semantic segmentation, and report state of the art results on datasets such as CamVid. Installation You need to install :  Theano. Preferably the last version Lasagne The dataset loader (Not yet available) (Recommended) The new Theano GPU backend. Compilation will be much faster.  Data The data loader is now available here : https://github.com/fvisin/dataset_loaders Thanks a lot to Francesco Visin, please cite if you use his data loader. Some adaptations may be do on the actual code, I hope to find some time to modify it !  The data-loader we used for the experiments will be released later. If you do want to train models now, you need to create a function load_data which returns 3 iterators (for training, validation and test). When applying next(), the iterator returns two values X, Y where X is the batch of input images (shape= (batch_size, 3, n_rows, n_cols), dtype=float32) and Y the batch of target segmentation maps (shape=(batch_size, n_rows, n_cols), dtype=int32) where each pixel in Y is an int indicating the class of the pixel. The iterator must also have the following methods (so they are not python iterators) : get_n_classes (returns the number of classes), get_n_samples (returns the number of examples in the set), get_n_batches (returns the number of batches necessary to see the entire set) and get_void_labels (returns a list containing the classes associated to void). It might be easier to change directly the files train.py and test.py. Run experiments The architecture of the model is defined in FC-DenseNet.py. To train a model, you need to prepare a configuration file (folder config) where all the parameters needed for creating and training your model are precised. DenseNets contain lot of connections making graph optimization difficult for Theano. We strongly recommend to use the flags described further. To train the FC-DenseNet103 model, use the command : THEANO_FLAGS='device=cuda,optimizer=fast_compile,optimizer_including=fusion' python train.py -c config/FC-DenseNet103.py -e experiment_name. All the logs of the experiments are stored in the folder experiment_name. On a Titan X 12GB, for the model FC-DenseNet103 (see folder config), compilation takes around 400 sec and 1 epoch 120 sec for training and 40 sec for validation. Use a pretrained model We publish the weights of our model FC-DenseNet103. Metrics claimed in the paper (jaccard and accuracy) can be verified running THEANO_FLAGS='device=cuda,optimizer=fast_compile,optimizer_including=fusion' python test.py About the ""m"" number in the paper There is a small error with the ""m"" number in the Table 2 of the paper (that you may understand when running the code!). All values from the bottleneck to the last block (880, 1072, 800 and 368) should be incremented by 16 (896, 1088, 816 and 384). Here how we compute this value representing the number of feature maps concatenated into the ""stack"" :  First convolution : m=48 In the downsampling part + bottleneck, m[B] = m[B-1] + n_layers[B] * growth_rate [linear growth]. First block : m = 48 + 4x16 = 112. Second block m = 112 + 5x16 = 192. Until the bottleneck : m = 656 + 15x16 = 896. In the upsampling part, m[B] is the sum of 3 terms : the m value corresponding to same resolution in the downsampling part (skip connection), the number of feature maps from the upsampled block (n_layers[B-1] * growth_rate) and the number of feature maps in the new block (n_layers[B] * growth_rate). First upsampling, m =  656 + 15x16 + 12x16 = 1088. Second upsampling, m = 464 + 12x16 + 10x16 = 816. Third upsampling, m = 304 + 10x16 + 7x16 = 576, Fourth upsampling, m = 192 + 7x16 + 5x16 = 384 and fifth upsampling, m = 112 + 5x16 + 4x16 = 256 "
29,nsavinov/semantic3dnet,177,https://github.com/nsavinov/semantic3dnet,"Updated on Aug 25, 2017","Point cloud semantic segmentation via Deep 3D Convolutional Neural Network How does it work Instructions for Linux (tested for Ubuntu 16.04.2 LTS): Parameters lib/point_cloud_util/data_loader_constants.h: src/point_cloud_constants.lua: Caveats      README.md           Point cloud semantic segmentation via Deep 3D Convolutional Neural Network This code implements a deep neural network for 3D point cloud semantic segmentation. It comes as a baseline for the benchmark http://www.semantic3d.net/ (reproduces DeepNet entry in reduced-8 track). It is written in C++/lua and is supposed to simplify starting to work with the benchmark. The code requires at least 8 Gb RAM and an Nvidia GPU (at least 6 Gb of memory, tested for Nvidia Titan X GPU). If you use this code or the benchmark in your research, please cite it as @article{hackel2017semantic3d, title={Semantic3D.net: A new Large-scale Point Cloud Classification Benchmark}, author={Hackel, Timo and Savinov, Nikolay and Ladicky, Lubor and Wegner, Jan and Schindler, Konrad and Pollefeys, Marc}, journal={arXiv preprint arXiv:1704.03847}, year={2017} }           How does it work  Each point in the point cloud is to be classified into one of the semantic classes like building/car/vegetation/etc. It is done by considering a range of neighbourhoods for the point, computing occupancy grids for them and applying 3D convolutional neural network on those grids. A more detailed description can be found at https://goo.gl/TUPqXo. Instructions for Linux (tested for Ubuntu 16.04.2 LTS):  install torch (tested for commit 5c1d3cfda8101123628a45e70435d545ae1bc771 from June 7, 2017), cuda (tested for v8.0) and cudnn (tested for v5.1). For the latter, follow the instructions https://github.com/soumith/cudnn.torch clone this repository run ""cd build; ./setup.sh"" to download data and transform it into necessary format. run ""./build_run.sh"" to prepare small train/validation sets to track optimization progress run ""cd ../src; ./launch_training.sh"". You can track the progress in nohup.out file. Wait until the train error becomes close to 0 and test error start to oscillate around some value. Then kill the process (took 304 epochs for the baseline). run ""./launch_prediction.sh"". You might want to change gpu indexes in this script depending on number of gpus you have available. You can also tweak number of openmp threads. Wait until it finishes (might take a day or so). after prediction finishes, run ""./prepare_to_submit.sh"" to put the submission into necessary format. submit data/benchmark/submit.zip to the server http://www.semantic3d.net/submit_public.php.  Parameters The changeable constants are in lib/point_cloud_util/data_loader_constants.h and in src/point_cloud_constants.lua Explanation of them: lib/point_cloud_util/data_loader_constants.h: const int kWindowSize = 16; // neighbourhood of the point is voxelized into kWindowSize ^ 3 voxels const int kBatchSize = 100; // since batch is constructed on cpp side, we need specify its size here const int kNumberOfClasses = 8; // integer labels 1, ..., 8 are considered const int kDefaultNumberOfScales = 5; // each sample in the batch is constructed as stacked multiples scales. in the deep net architecture fully connected layer outputs are concatenated const int kDefaultNumberOfRotations = 1; // optional implemenation of TI-pooling on top of multi-scale architecture. check the repository or read the paper for more details const float kSpatialResolution = 0.025; // voxel side in meters const int kBatchResamplingLimit = 100; // after this number of batches, the coordinate system is rotated by a random angle and the voxelized representations are recalculated from scratch (augmentation). src/point_cloud_constants.lua: opt.number_of_filters = 16 -- number of filters in the first convolutional layer, other layers size is also proportional to this constant. opt.kLargePrintingInterval = 100 -- how often to evaluate model and dump solution to disk opt.kWarmStart = false -- restart with the saved model opt.kModelDumpName = '../dump/model_dump' -- where the model is saved opt.kOptimStateDumpName = '../dump/optim_state_dump' -- where the optimization progress is saved opt.kStreamingPath = '../data/benchmark/sg28_station4_intensity_rgb_train.txt' -- from which file training data is sampled Caveats Data is randomly sampled from the training set, all the classes are made equally probable via this sampling. Thus it is required that the training file contains at least one sample of each class. "
30,nsavinov/semantic3dnet,177,https://github.com/nsavinov/semantic3dnet,"Updated on Aug 25, 2017","Point cloud semantic segmentation via Deep 3D Convolutional Neural Network How does it work Instructions for Linux (tested for Ubuntu 16.04.2 LTS): Parameters lib/point_cloud_util/data_loader_constants.h: src/point_cloud_constants.lua: Caveats      README.md           Point cloud semantic segmentation via Deep 3D Convolutional Neural Network This code implements a deep neural network for 3D point cloud semantic segmentation. It comes as a baseline for the benchmark http://www.semantic3d.net/ (reproduces DeepNet entry in reduced-8 track). It is written in C++/lua and is supposed to simplify starting to work with the benchmark. The code requires at least 8 Gb RAM and an Nvidia GPU (at least 6 Gb of memory, tested for Nvidia Titan X GPU). If you use this code or the benchmark in your research, please cite it as @article{hackel2017semantic3d, title={Semantic3D.net: A new Large-scale Point Cloud Classification Benchmark}, author={Hackel, Timo and Savinov, Nikolay and Ladicky, Lubor and Wegner, Jan and Schindler, Konrad and Pollefeys, Marc}, journal={arXiv preprint arXiv:1704.03847}, year={2017} }           How does it work  Each point in the point cloud is to be classified into one of the semantic classes like building/car/vegetation/etc. It is done by considering a range of neighbourhoods for the point, computing occupancy grids for them and applying 3D convolutional neural network on those grids. A more detailed description can be found at https://goo.gl/TUPqXo. Instructions for Linux (tested for Ubuntu 16.04.2 LTS):  install torch (tested for commit 5c1d3cfda8101123628a45e70435d545ae1bc771 from June 7, 2017), cuda (tested for v8.0) and cudnn (tested for v5.1). For the latter, follow the instructions https://github.com/soumith/cudnn.torch clone this repository run ""cd build; ./setup.sh"" to download data and transform it into necessary format. run ""./build_run.sh"" to prepare small train/validation sets to track optimization progress run ""cd ../src; ./launch_training.sh"". You can track the progress in nohup.out file. Wait until the train error becomes close to 0 and test error start to oscillate around some value. Then kill the process (took 304 epochs for the baseline). run ""./launch_prediction.sh"". You might want to change gpu indexes in this script depending on number of gpus you have available. You can also tweak number of openmp threads. Wait until it finishes (might take a day or so). after prediction finishes, run ""./prepare_to_submit.sh"" to put the submission into necessary format. submit data/benchmark/submit.zip to the server http://www.semantic3d.net/submit_public.php.  Parameters The changeable constants are in lib/point_cloud_util/data_loader_constants.h and in src/point_cloud_constants.lua Explanation of them: lib/point_cloud_util/data_loader_constants.h: const int kWindowSize = 16; // neighbourhood of the point is voxelized into kWindowSize ^ 3 voxels const int kBatchSize = 100; // since batch is constructed on cpp side, we need specify its size here const int kNumberOfClasses = 8; // integer labels 1, ..., 8 are considered const int kDefaultNumberOfScales = 5; // each sample in the batch is constructed as stacked multiples scales. in the deep net architecture fully connected layer outputs are concatenated const int kDefaultNumberOfRotations = 1; // optional implemenation of TI-pooling on top of multi-scale architecture. check the repository or read the paper for more details const float kSpatialResolution = 0.025; // voxel side in meters const int kBatchResamplingLimit = 100; // after this number of batches, the coordinate system is rotated by a random angle and the voxelized representations are recalculated from scratch (augmentation). src/point_cloud_constants.lua: opt.number_of_filters = 16 -- number of filters in the first convolutional layer, other layers size is also proportional to this constant. opt.kLargePrintingInterval = 100 -- how often to evaluate model and dump solution to disk opt.kWarmStart = false -- restart with the saved model opt.kModelDumpName = '../dump/model_dump' -- where the model is saved opt.kOptimStateDumpName = '../dump/optim_state_dump' -- where the optimization progress is saved opt.kStreamingPath = '../data/benchmark/sg28_station4_intensity_rgb_train.txt' -- from which file training data is sampled Caveats Data is randomly sampled from the training set, all the classes are made equally probable via this sampling. Thus it is required that the training file contains at least one sample of each class. "
31,HyeonwooNoh/DeconvNet,321,https://github.com/HyeonwooNoh/DeconvNet,"Updated on Jun 23, 2015","DeconvNet: Learning Deconvolution Network for Semantic Segmentation Introduction Citation Pre-trained Model Licence System Requirements Installing DeconvNet Training DeconvNet Inference EDeconvNet+CRF      README.md           DeconvNet: Learning Deconvolution Network for Semantic Segmentation Created by Hyeonwoo Noh, Seunghoon Hong and Bohyung Han at POSTECH Acknowledgements: Thanks to Yangqing Jia and the BVLC team for creating Caffe. Introduction DeconvNet is state-of-the-art semantic segmentation system that combines bottom-up region proposals with multi-layer decovolution network. Detailed description of the system will be provided by our technical report [arXiv tech report] http://arxiv.org/abs/1505.04366 Citation If you're using this code in a publication, please cite our papers. @article{noh2015learning,   title={Learning Deconvolution Network for Semantic Segmentation},   author={Noh, Hyeonwoo and Hong, Seunghoon and Han, Bohyung},   journal={arXiv preprint arXiv:1505.04366},   year={2015} }           Pre-trained Model If you need model definition and pre-trained model only, you can download them from following location: 0. caffe for DeconvNet: https://github.com/HyeonwooNoh/caffe 0. DeconvNet model definition: http://cvlab.postech.ac.kr/research/deconvnet/model/DeconvNet/DeconvNet_inference_deploy.prototxt 0. Pre-trained DeconvNet weight: http://cvlab.postech.ac.kr/research/deconvnet/model/DeconvNet/DeconvNet_trainval_inference.caffemodel Licence This software is being made available for research purpose only. Check LICENSE file for details. System Requirements This software is tested on Ubuntu 14.04 LTS (64bit). Prerequisites 0. MATLAB (tested with 2014b on 64-bit Linux) 0. prerequisites for caffe(http://caffe.berkeleyvision.org/installation.html#prequequisites) Installing DeconvNet By running ""setup.sh"" you can download all the necessary file for training and inference include: 0. caffe: you need modified version of caffe which support DeconvNet - https://github.com/HyeonwooNoh/caffe.git 0. data: data used for training stage 1 and 2 0. model: caffemodel of trained DeconvNet and other caffemodels required for training Training DeconvNet Training scripts are included in ./training/ directory To train DeconvNet you can simply run following scripts in order: 0. 001_start_train.sh : script for first stage training 0. 002_start_train.sh : script for second stage training 0. 003_start_make_bn_layer_testable : script converting trained DeconvNet with bn layer to inference mode Inference EDeconvNet+CRF Run run_demo.m to reproduce EDeconvNet+CRF results on VOC2012 test data. This script will generated EDeconvNet+CRF results through following steps: 0. run FCN-8s and cache the score [cache_FCN8s_results.m] 0. generate DeconvNet score and apply ensemble with FCN-8s score, post processing with densecrf [generate_EDeconvNet_CRF_results.m] EDeconvNet+CRF obtains 72.5 mean I/U on PASCAL VOC 2012 Test External dependencies [can be downloaded by running ""setup.sh"" script] 0. FCN-8s model and weight file [https://github.com/BVLC/caffe/wiki/Model-Zoo] 0. densecrf with matlab wrapper [https://github.com/johannesu/meanfield-matlab.git] 0. cached proposal bounding boxes extracted with edgebox object proposal [https://github.com/pdollar/edges] "
32,fuweifu-vtoo/Semantic-segmentation,172,https://github.com/fuweifu-vtoo/Semantic-segmentation,"Updated on Nov 18, 2021",Semantic-segmentation 这个repo适合新手入门pytorch和图像分割 实验结果 我的环境 如何运行 所有的相对路径均在代码中配置 包含文件 train_Seg.py train_Unet.py predict.py models/seg_net.py models/u_net.py utils/DataArgument.py 数据集 数据集下载 数据集处理 已处理好的数据集下载 联系我 代码运行过程中有任何问题，都可随时联系我（for free and feel free to contact me please），可以在issue中提问，我也会尽快回答。      README.md           Semantic-segmentation 这个repo适合新手入门pytorch和图像分割 一个采用Pytorch的语义分割项目 这个repo是在遥感图像语义分割项目时写的，但数据集不是我们遥感项目的数据集，而是网上download的一个遥感数据集。 实验结果    我的环境  windows10 Anaconda 3 pytorch 1.0 tensorflow tensorboard tensorboardX (用于可视化)  如何运行 所有的相对路径均在代码中配置  打开终端，输入  python train_Seg.py            调用Segnet 或者  python train_U.py            调用Unet 或者  python predict.py            进行推断inference（需要有已经训练好的模型才可以推断）  包含文件 train_Seg.py  调用Segnet进行训练网络 主函数  train_Unet.py  调用Unet进行训练网络 主函数  predict.py  对模型进行inference预测  models/seg_net.py  Segnet网络定义  models/u_net.py  Unet网络定义  utils/DataArgument.py  数据预处理文件，对数据切割，旋转加噪顺便做数据增强  数据集 数据集下载 https://www.cs.toronto.edu/~vmnih/data/ 数据集处理 进入utils文件夹，使用下面语句（相对路径要提前配置好） python DataArgument.py           DataArgument.py实现了对大图进行切割（成256 x 256），切割后旋转，加噪声等操作来生成训练数据。 已处理好的数据集下载 百度网盘 提取码：5b1v 下载后解压至 ./data/train/label  和 ./data/train/src 联系我 代码运行过程中有任何问题，都可随时联系我（for free and feel free to contact me please），可以在issue中提问，我也会尽快回答。 
33,wutianyiRosun/Segmentation.X,467,https://github.com/wutianyiRosun/Segmentation.X,"Updated on Jan 8, 2020","Segmentation Semantic Segmentation 2019 2018 2017 2016 2015 Before 2015 Repos Instance Segmentation Panoptic Segmentation Video Segmentation 2018 Saliency Detection RNN Graphical Models (CRF, MRF) Datasets: other papers Blog posts, other:      README.md           Segmentation   Semantic Segmentation  Instance Segmentation  Panoptic Segmentation  Video Segmentation  Saliency Detection  Semantic Segmentation 2019  CVPR 2019  Object-aware Aggregation with Bidirectional Temporal Graph for Video Captioning Pattern-Affinitive Propagation across Depth, Surface Normal and Semantic Segmentation Cross-Modal Relationship Inference for Grounding Referring Expressions Face Parsing with RoI Tanh-Warping Speech2Face: Learning the Face Behind a Voice Collaborative Global-Local Networks for Memory-Efficient Segmentation of Ultra-High Resolution Images  [code] Budget-aware Semi-Supervised Semantic and Instance Segmentation  [Workshop] DARNet: Deep Active Ray Network for Building Segmentation Zoom To Learn, Learn To Zoom A Simple Pooling-Based Design for Real-Time Salient Object Detection Pyramid Feature Attention Network for Saliency detection [code] Auto-DeepLab: Hierarchical Neural Architecture Search for Semantic Image Segmentation [code] Generating Classification Weights with GNN Denoising Autoencoders for Few-Shot Learning  [code] SCOPS: Self-Supervised Co-Part Segmentation Panoptic Feature Pyramid Networks Representation Similarity Analysis for Efficient Task taxonomy & Transfer Learning [code] Student Becoming the Master: Knowledge Amalgamation for Joint Scene Parsing, Depth Estimation, and More Data-Driven Neuron Allocation for Scale Aggregation Networks Bi-Directional Cascade Network for Perceptual Edge Detection [code] Scale-Adaptive Neural Dense Features: Learning via Hierarchical Context Aggregation DFANet: Deep Feature Aggregation for Real-Time Semantic Segmentation Devil is in the Edges: Learning Semantic Boundaries from Noisy Annotations Adaptive Weighting Multi-Field-of-View CNN for Semantic Segmentation in Pathology Pixel-Adaptive Convolutional Neural Networks A Relation-Augmented Fully Convolutional Network for Semantic Segmentation in Aerial Scenes Cross-Modal Self-Attention Network for Referring Image Segmentation [From NLP] Graphonomy: Universal Human Parsing via Graph Transfer Learning [code] Large-scale interactive object segmentation with human annotators In Defense of Pre-trained ImageNet Architectures for Real-time Semantic Segmentation of Road-driving Images A Cross-Season Correspondence Dataset for Robust Semantic Segmentation Decoders Matter for Semantic Segmentation: Data-Dependent Decoding Enables Flexible Feature Aggregation [code] Knowledge Adaptation for Efficient Semantic Segmentation Structured Knowledge Distillation for Semantic Segmentation FickleNet: Weakly and Semi-supervised Semantic Image Segmentation using Stochastic Inference Data augmentation using learned transforms for one-shot medical image segmentation Decoders Matter for Semantic Segmentation: Data-Dependent Decoding Enables Flexible Feature Aggregation Graph-Based Global Reasoning Networks [GCN] T-Net: Parametrizing Fully Convolutional Nets with a Single High-Order Tensor [Face Part Seg] Improving Semantic Segmentation via Video Propagation and Label Relaxation   other 19'conferences  CE-Net: Context Encoder Network for 2D Medical Image Segmentation [TMI] Tree-structured Kronecker Convolutional Network for Semantic Segmentation [ICME]   ICCV 2019  Incremental Class Discovery for Semantic Segmentation with RGBD Sensing Asymmetric Non-local Neural Networks for Semantic Segmentation Exploiting temporal consistency for real-time video depth estimation∗ Action recognition with spatial-temporal discriminative filter banks Temporal Knowledge Propagation for Image-to-Video Person Re-identification Semi-Supervised Video Salient Object Detection Using Pseudo-Labels LIP: Local Importance-based Pooling Frame-to-Frame Aggregation of Active Regions in Web Videos for Weakly Supervised Semantic Segmentation Metric Learning With HORDE: High-Order Regularizer for Deep Embeddings [code] Similarity-Preserving Knowledge Distillation Expectation-Maximization Attention Networks for Semantic Segmentation Orientation-aware Semantic Segmentation on Icosahedron Spheres Learning Aberrance Repressed Correlation Filters for Real-Time UAV Tracking Learning Lightweight Lane Detection CNNs by Self Attention Distillation [code]   arXiv  Consensus Feature Network for Scene Parsing Deep Co-Training for Semi-Supervised Image Segmentation Residual Pyramid Learning for Single-Shot Semantic Segmentation What Synthesis is Missing: Depth Adaptation Integrated with Weak Supervision for Indoor Scene Parsing Dynamic Deep Networks for Retinal Vessel Segmentation Efficient Smoothing of Dilated Convolutions for Image Segmentation Adaptive Masked Weight Imprinting for Few-Shot Segmentation An efficient solution for semantic segmentation: ShuffleNet V2 with atrous separable convolutions Lift-the-Flap: Context Reasoning Using Object-Centered Graphs Fast-SCNN: Fast Semantic Segmentation Network THE EFFECT OF SCENE CONTEXT ON WEAKLY SUPERVISED SEMANTIC SEGMENTATION MultiResUNet : Rethinking the U-Net Architecture for Multimodal Biomedical Image Segmentation On Boosting Semantic Street Scene Segmentation with Weak Supervision    2018   CVPR 2018  Compassionately Conservative Balanced Cuts for Image Segmentation icient interactive annotation of segmentation datasets with polygon rnn++ [code] Guided Proofreading of Automatic Segmentations for Connectomics DenseASPP for Semantic Segmentation in StreetScenes Context Contrasted Feature and Gated Multi-scale Aggregation for Scene Segmentation Recurrent Scene Parsing with Perspective Understanding in the Loop PAD-Net: Multi-Tasks Guided Prediction-and-Distillation Network for Simultaneous Depth Estimation and Scene Parsing Learning a Discriminative Feature Network for Semantic Segmentation Context Encoding for Semantic Segmentation Dynamic-structured Semantic Propagation Network In-Place Activated BatchNorm for Memory-Optimized Training of DNNs Error Correction for Dense Semantic Image Labeling Revisiting Dilated Convolution: A Simple Approach for Weakly- and Semi-Supervised Semantic Segmentation Dense Decoder Shortcut Connections for Single-Pass Semantic Segmentation On the Importance of Label Quality for Semantic Segmentation Referring Image Segmentation via Recurrent Refinement Networks [From NLP] [code] Learning Superpixels with Segmentation-Aware Affinity Loss [Superpixel seg] Weakly and Semi Supervised Human Body Part Parsing via Pose-Guided Knowledge Transfer [Human Part Seg]  [code] Multi-Evidence Filtering and Fusion for Multi-Label Classification, Object Detection and Semantic Segmentation Based on Weakly Supervised Learning [WSL] Learning Pixel-Level Semantic Affinity With Image-Level Supervision for Weakly Supervised Semantic Segmentation [WSL] Weakly-Supervised Semantic Segmentation Network With Deep Seeded Region Growing [WSL] Revisiting Dilated Convolution: A Simple Approach for Weakly- and Semi-Supervised Semantic Segmentation [WSL] Bootstrapping the Performance of Webly Supervised Semantic Segmentation [WSL] Normalized Cut Loss for Weakly-Supervised CNN Segmentation  [WSL] Weakly-Supervised Semantic Segmentation by Iteratively Mining Common Object Features [WSL] Weakly Supervised Instance Segmentation Using Class Peak Response [WSL]    ECCV 2018  Multi-Scale Context Intertwining for Semantic Segmentation Unified Perceptual Parsing for Scene Understanding ExFuse: Enhancing Feature Fusion for Semantic Segmentation BiSeNet: Bilateral Segmentation Network for Real-time Semantic Segmentation ESPNet: Efficient Spatial Pyramid of Dilated Convolutions for Semantic Segmentation PSANet: Point-wise Spatial Attention Network for Scene Parsing ICNet for Real-Time Semantic Segmentation on High-Resolution Images Adaptive Affinity Fields for Semantic Segmentation Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation    Other 18'conferences  RelationNet: Learning Deep-Aligned Representation for Semantic Image Segmentation [ICPR] High Resolution Feature Recovering for Accelerating Urban Scene Parsing [IJCAI] Mix-and-Match Tuning for Self-Supervised Semantic Segmentation [AAAI] Spatial As Deep: Spatial CNN for Traffic Scene Understanding Xingang [AAAI] A Probabilistic U-Net for Segmentation of Ambiguous Images [NIPS] DifNet: Semantic Segmentation by Diffusion Networks [NIPS] Beyond Grids: Learning Graph Representations for Visual Recognition [NIPS] [GCN] Symbolic Graph Reasoning Meets Convolutions [NIPS] [GCN] A^2-Nets: Double Attention Networks [NIPS] [GCN] Searching for Efficient Multi-Scale Architectures for Dense Image Prediction [NIPS] [NAS]    ArXiv  Improving Semantic Segmentation via Video Propagation and Label Relaxation Evaluating Bayesian Deep Learning Methods for Semantic Segmentation ShelfNet for Real-time Semantic Segmentation, Multi-path segmentation network CCNet: Criss-Cross Attention for Semantic Segmentation Dual Attention Network for Scene Segmentation Decoupled Spatial Neural Attention for Weakly Supervised Semantic Segmentation Locally Adaptive Learning Loss for Semantic Image Segmentation RTSEG: REAL-TIME SEMANTIC SEGMENTATION COMPARATIVE STUDY OCNet: Object Context Network for Scene Parsing CGNet: A Light-weight Context Guided Network for Semantic Segmentation    2017   CVPR 2017  Convolutional RandomWalk Networks for Semantic Image Segmentation Dilated Residual Networks Learning Adaptive Receptive Fields for Deep Image Parsing Network Loss Max-Pooling for Semantic Image Segmentation Semantic Segmentation via Structured Patch Prediction, Context CRF and Guidance CRF Pyramid Scene Parsing Network Full-Resolution Residual Networks for Semantic Segmentation in Street Scenes Refinenet: Multi-path refinement networks for high-resolution semantic segmentation Gated Feedback Refinement Network for Dense Image Labeling    ICCV 2017  Deep Dual Learning for Semantic Image Segmentation Semi Supervised Semantic Segmentation Using Generative Adversarial Network Scale-adaptive Convolutions for Scene Parsing Predicting Deeper into the Future of Semantic Segmentation Segmentation-Aware Convolutional Networks Using Local Attention Mask Dense and Low-Rank Gaussian CRFs Using Deep Embeddings Siddhartha FoveaNet: Perspective-aware Urban Scene Parsing    Other 17'conferences  Understanding Convolution for Semantic Segmentation[WACV] Learning Affinity via Spatial Propagation Networks[NIPS] Dual Path Networks[NIPS] Semantic Segmentation with Reverse Attention[BMVC] The One Hundred Layers Tiramisu: Fully Convolutional DenseNets for Semantic Segmentation[CVPRW] Fully Convolutional Networks for Semantic Segmentation [TPAMI]    ArXiv  Rethinking Atrous Convolution for Semantic Image Segmentation Pixel Deconvolutional Networks    2016   CVPR 2016  Semantic Image Segmentation with Task-Specific Edge Detection Using CNNs and a Discriminatively Trained Domain Transform    ECCV 2016  Semantic Object Parsing with Graph LSTM Attention to Scale: Scale-aware Semantic Image Segmentation Efficient Piecewise Training of Deep Structured Models for Semantic Segmentation    Other 16'conferences  Semantic Segmentation using Adversarial Networks [NIPSW] Speeding up Semantic Segmentation for Autonomous Driving [NIPSW] ReSeg: A Recurrent Neural Network-based Model for Semantic Segmentation [PyTorch] Multi-Scale Context Aggregation by Dilated Convolutions [ICLR] [PyTorch] Learning Dense Convolutional Embeddings for Semantic Segmentation[ICLR]    ArXiv  ENet: A Deep Neural Network Architecture for Real-Time Semantic Segmentation PixelNet: Towards a General Pixel-level Architecture MultiNet: Real-time Joint Semantic Reasoning for Autonomous Driving High-performance Semantic Segmentation Using Very Deep Fully Convolutional Networks    2015   CVPR 2015  Fully Convolutional Networks for Semantic Segmentation Hypercolumns for Object Segmentation and Fine-grained Localization Weakly supervised semantic segmentation for social images Scene Labeling with LSTM Recurrent Neural Networks Learning to Propose Objects  [PyTorch] [Project] Feedforward semantic segmentation with zoom-out features    ICCV 2015  Semantic Image Segmentation via Deep Parsing Network Learning deconvolution network for semantic segmentation    Other 15'conferences  U-Net: Convolutional Networks for Biomedical Image Segmentation[MICCAI] Decoupled Deep Neural Network for Semi-supervised Semantic Segmentation[NIPS]    ArXiv  SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation    Before 2015  Simultaneous Detection and Segmentation [ECCV2014] Nonparametric Scene Parsing via Label Transfer [TPAMI2011][Project] Dense Segmentation-aware Descriptors[CVPR2013] Semantic Segmentation with Second-Order Pooling [ECCV2012]  Repos  https://github.com/ZijunDeng/pytorch-semantic-segmentation [PyTorch] https://github.com/meetshah1995/pytorch-semseg [PyTorch]  Instance Segmentation  Learning to Segment Object Candidates Recurrent Instance Segmentation [ECCV2016] Instance-aware Semantic Segmentation via Multi-task Network Cascades Learning to Refine Object Segments Fully Convolutional Instance-aware Semantic Segmentation Mask R-CNN  Panoptic Segmentation  Panoptic Segmentation Single Network Panoptic Segmentation for Street Scene Understanding DeeperLab: Single-Shot Image Parser An End-to-End Network for Panoptic Segmentation  Video Segmentation 2018   CVPR 2018  Actor and Action Video Segmentation from a Sentence  [project] Dynamic Video Segmentation Network Semantic Video Segmentation by Gated Recurrent Flow Propagation Deep Spatio-Temporal Random Fields for Efficient Video Segmentation [code] Low-Latency Video Semantic Segmentation CNN in MRF: Video Object Segmentation via Inference in A CNN-Based Higher-Order Spatio-Temporal MRF Efficient Video Object Segmentation via Network Modulation [code] Instance Embedding Transfer to Unsupervised Video Object Segmentation Fast Video Object Segmentation by Reference-Guided Mask Propagation [code] Fast and Accurate Online Video Object Segmentation via Tracking Parts [code] Reinforcement Cutting-Agent Learning for Video Object Segmentation Blazingly Fast Video Object Segmentation With Pixel-Wise Metric Learning MoNet: Deep Motion Exploitation for Video Object Segmentation Motion-Guided Cascaded Refinement Network for Video Object Segmentation    others  Clockwork Convnets for Video Semantic Segmentation (ECCV2016) https://github.com/shelhamer/clockwork-fcn https://github.com/JingchunCheng/Seg-with-SPN http://segmentation.is.tue.mpg.de/    Saliency Detection  Contextual Encoder-Decoder Network for Visual Saliency Prediction Understanding and Visualizing Deep Visual Saliency Models (CVPR2019) SAC-Net: Spatial Attenuation Context for Salient Object Detectio  RNN  ReNet [https://arxiv.org/pdf/1505.00393.pdf]  Graphical Models (CRF, MRF)  https://github.com/cvlab-epfl/densecrf http://vladlen.info/publications/efficient-inference-in-fully-connected-crfs-with-gaussian-edge-potentials/ http://www.philkr.net/home/densecrf http://graphics.stanford.edu/projects/densecrf/ https://github.com/amiltonwong/segmentation/blob/master/segmentation.ipynb https://github.com/jliemansifry/super-simple-semantic-segmentation http://users.cecs.anu.edu.au/~jdomke/JGMT/ https://www.quora.com/How-can-one-train-and-test-conditional-random-field-CRF-in-Python-on-our-own-training-testing-dataset https://github.com/tpeng/python-crfsuite https://github.com/chokkan/crfsuite https://sites.google.com/site/zeppethefake/semantic-segmentation-crf-baseline https://github.com/lucasb-eyer/pydensecrf  Datasets:  Stanford Background Dataset Sift Flow Dataset Barcelona Dataset Microsoft COCO dataset MSRC Dataset LITS Liver Tumor Segmentation Dataset KITTI Pascal Context Data from Games dataset Human parsing dataset Mapillary Vistas Dataset Microsoft AirSim MIT Scene Parsing Benchmark COCO 2017 Stuff Segmentation Challenge ADE20K Dataset INRIA Annotations for Graz-02 Daimler dataset ISBI Challenge: Segmentation of neuronal structures in EM stacks INRIA Annotations for Graz-02 (IG02) Pratheepan Dataset Clothing Co-Parsing (CCP) Dataset Inria Aerial Image  other papers  Ranked List Loss for Deep Metric Learning [CVPR2019] Video Generation from Single Semantic Label Map [CVPR2019] Refine and Distill: Exploiting Cycle-Inconsistency and Knowledge Distillation for Unsupervised Monocular Depth Estimation [CVPR2019] Sliced Wasserstein Discrepancy for Unsupervised Domain Adaptation Group-wise Correlation Stereo Network [CVPR2019] Scene Memory Transformer for Embodied Agents in Long-Horizon Tasks [CVPR2019] Similarity Learning via Kernel Preserving Embedding [AAAI2019] Unsupervised Person Re-identification by Soft Multilabel Learning [CVPR2019] Learning Robust Representations by Projecting Superficial Statistics Out [ICLR2019] MFAS: Multimodal Fusion Architecture Search [CVPR2019] SimulCap : Single-View Human Performance Capture with Cloth Simulation [CVPR2019] Semantic Image Synthesis with Spatially-Adaptive Normalization [CVPR2019] Graph Convolutional Label Noise Cleaner: Train a Plug-and-play Action Classifier for Anomaly Detection [CVPR2019] QATM: Quality-Aware Template Matching For Deep Learning [CVPR2019] AdaGraph: Unifying Predictive and Continuous Domain Adaptation through Graphs [CVPR2019] Selective Kernel Networks [CVPR2019] Towards Robust Curve Text Detection with Conditional Spatial Expansion  [CVPR2019] Weakly-Supervised Discovery of Geometry-Aware Representation for 3D Human Pose Estimation  [CVPR2019] Dual Residual Networks Leveraging the Potential of Paired Operations for Image Restoration [CVPR2019] Networks for Joint Affine and Non-parametric Image Registration  [CVPR2019] OCGAN: One-class Novelty Detection Using GANs with Constrained Latent Representations  [CVPR2019] Semantic Alignment: Finding Semantically Consistent Ground-truth for Facial Landmark Detection  [CVPR2019] Scale-Adaptive Neural Dense Features: Learning via Hierarchical Context Aggregation  [CVPR2019] f-VAEGAN-D2: A Feature Generating Framework for Any-Shot Learning [CVPR2019] Residual Non-local Attention Networks for Image Restoration  [ICLR2019] Self-Supervised Learning via Conditional Motion Propagation [CVPR2019]  Blog posts, other:  https://handong1587.github.io/deep_learning/2015/10/09/segmentation.html http://www.andrewjanowczyk.com/efficient-pixel-wise-deep-learning-on-large-images/ https://devblogs.nvidia.com/parallelforall/image-segmentation-using-digits-5/ https://github.com/NVIDIA/DIGITS/tree/master/examples/binary-segmentation https://github.com/NVIDIA/DIGITS/tree/master/examples/semantic-segmentation "
34,sthalles/deeplab_v3,790,https://github.com/sthalles/deeplab_v3,"Updated on Nov 10, 2021","DeepLab_V3 Image Semantic Segmentation Network Dependencies Downloads Evaluation Retraining Training and Eval Retraining Datasets Serving Results      README.md            DeepLab_V3 Image Semantic Segmentation Network Implementation of the Semantic Segmentation DeepLab_V3 CNN as described at Rethinking Atrous Convolution for Semantic Image Segmentation. For a complete documentation of this implementation, check out the blog post. Dependencies  Python 3.x Numpy Tensorflow 1.10.1  Downloads Evaluation Pre-trained model.  checkpoints  Place the checkpoints folder inside ./tboard_logs. If the folder does not exist, create it. Retraining Original datasets used for training.  Dataset  Option 1 Option 2    Place the tfrecords files inside ./dataset/tfrecords. Create the folder if it does not exist. Training and Eval Once you have the training and validation TfRefords files, just run the command bellow. Before running Deeplab_v3, the code will look for the proper ResNets checkpoints inside ./resnet/checkpoints, if the folder does not exist, it will first be downloaded. python train.py --starting_learning_rate=0.00001 --batch_norm_decay=0.997 --crop_size=513 --gpu_id=0 --resnet_model=resnet_v2_50           Check out the train.py file for more input argument options. Each run produces a folder inside the tboard_logs directory (create it if not there). To evaluate the model, run the test.py file passing to it the model_id parameter (the name of the folder created inside tboard_logs during training). Note: Make sure the test.tfrecords is downloaded and placed inside ./dataset/tfrecords. python test.py --model_id=16645           Retraining To use a different dataset, you just need to modify the CreateTfRecord.ipynb notebook inside the dataset/ folder, to suit your needs. Also, be aware that originally Deeplab_v3 performs random crops of size 513x513 on the input images. This crop_size parameter can be configured by changing the crop_size hyper-parameter in train.py. Datasets To create the dataset, first make sure you have the Pascal VOC 2012 and/or the Semantic Boundaries Dataset and Benchmark datasets downloaded. Note: You do not need both datasets.  If you just want to test the code with one of the datasets (say the SBD), run the notebook normally, and it should work.  After, head to dataset/ and run the CreateTfRecord.ipynb notebook. The custom_train.txt file contains the name of the images selected for training. This file is designed to use the Pascal VOC 2012 set as a TESTING set. Therefore, it doesn't contain any images from the VOC 2012 val dataset. For more info, see the Training section of Deeplab Image Semantic Segmentation Network. Obs. You can skip that part and direct download the datasets used in this experiment - See the Downloads section Serving For full documentation on serving this Semantic Segmentation CNN, refer to How to deploy TensorFlow models to production using TF Serving. All the serving scripts are placed inside: ./serving/. To export the model and to perform client requests do the following:   Create a python3 virtual environment and install the dependencies from the serving_requirements.txt file;   Using the python3 env, run deeplab_saved_model.py. The exported model should reside into ./serving/model/;   Create a python2 virtual environment and install the dependencies from the client_requirements.txt file;   From the python2 env, run the deeplab_client.ipynb notebook;   Results  Pixel accuracy: ~91% Mean Accuracy: ~82% Mean Intersection over Union (mIoU): ~74% Frequency weighed Intersection over Union: ~86 "
35,iArunava/ENet-Real-Time-Semantic-Segmentation,251,https://github.com/iArunava/ENet-Real-Time-Semantic-Segmentation,"Updated on Apr 30, 2021","ENet - Real Time Semantic Segmentation How to use? Some results References Citations License      README.md           ENet - Real Time Semantic Segmentation A Neural Net Architecture for real time Semantic Segmentation. In this repository we have reproduced the ENet Paper - Which can be used on mobile devices for real time semantic segmentattion. The link to the paper can be found here: ENet How to use?  This repository comes in with a handy notebook which you can use with Colab. You can find a link to the notebook here: ENet - Real Time Semantic Segmentation Open it in colab: Open in Colab    Clone the repository and cd into it  git clone https://github.com/iArunava/ENet-Real-Time-Semantic-Segmentation.git cd ENet-Real-Time-Semantic-Segmentation/            Use this command to train the model  python3 init.py --mode train -iptr path/to/train/input/set/ -lptr /path/to/label/set/            Use this command to test the model  python3 init.py --mode test -m /path/to/the/pretrained/model.pth -i /path/to/image/to/infer.png            Use --help to get more commands  python3 init.py --help           Some results      References  A. Paszke, A. Chaurasia, S. Kim, and E. Culurciello. Enet: A deep neural network architecture for real-time semantic segmentation. arXiv preprint arXiv:1606.02147, 2016.  Citations @inproceedings{ BrostowSFC:ECCV08,   author    = {Gabriel J. Brostow and Jamie Shotton and Julien Fauqueur and Roberto Cipolla},   title     = {Segmentation and Recognition Using Structure from Motion Point Clouds},   booktitle = {ECCV (1)},   year      = {2008},   pages     = {44-57} }  @article{ BrostowFC:PRL2008,     author = ""Gabriel J. Brostow and Julien Fauqueur and Roberto Cipolla"",     title = ""Semantic Object Classes in Video: A High-Definition Ground Truth Database"",     journal = ""Pattern Recognition Letters"",     volume = ""xx"",     number = ""x"",     pages = ""xx-xx"",     year = ""2008"" }           License The code in this repository is distributed under the BSD v3 Licemse. Feel free to fork and enjoy :) "
36,xiaomengyc/Few-Shot-Semantic-Segmentation-Papers,404,https://github.com/xiaomengyc/Few-Shot-Semantic-Segmentation-Papers,Updated 9 days ago,"Few Shot Semantic Segmentation Papers 2021 2020 2019 2018 2017      README.md     Few Shot Semantic Segmentation Papers NOTE: If your paper is not in the list, plese feel free to raise an issue or drop me an e-mail. 2021    Title Venue PDF CODE     Rich Embedding Features for One-Shot Semantic Segmentation TNNLS PDF -   Simpler is Better: Few-shot Semantic Segmentation with Classifier Weight Transformer ICCV PDF CODE   Hypercorrelation Squeeze for Few-Shot Segmenation ICCV PDF CODE   Mining Latent Classes for Few-shot Segmentation ICCV PDF CODE   Few-Shot Semantic Segmentation with Cyclic Memory Network ICCV PDF -   Learning Meta-class Memory for Few-Shot Semantic Segmentation ICCV PDF CODE   Progressive One-Shot Human Parsing AAAI PDF CODE   Bidirectional RNN-based Few Shot Learning for 3D Medical Image Segmentation AAAI PDF CODE   Scale-Aware Graph Neural Network for Few-Shot Semantic Segmentation CVPR PDF -   Adaptive Prototype Learning and Allocation for Few-Shot Segmentation CVPR PDF CODE   Self-Guided and Cross-Guided Learning for Few-Shot Segmentation CVPR PDF CODE   Few-Shot Segmentation Without Meta-Learning: A Good Transductive Inference Is All You Need? CVPR PDF CODE   On the Texture Bias for Few-Shot CNN Segmentation WACV PDF CODE   A Location-Sensitive Local Prototype Network for Few-Shot Medical Image Segmentation ISBI PDF -   Dense Gaussian Processes for Few-Shot Segmentation arXiv PDF -   End-to-end One-shot Human Parsing arXiv PDF -   Few-Shot Segmentation with Global and Local Contrastive Learning arXiv PDF -   Few-shot Segmentation with Optimal Transport Matching and Message Flow arXiv PDF -   Uncertainty-Aware Semi-Supervised Few Shot Segmentation arXiv PDF -   Cost Aggregation Is All You Need for Few-Shot Segmentation arXiv PDF CODE    2020    Title Venue PDF CODE     Dynamic Extension Nets for Few-shot Semantic Segmentation MM PDF CODE   Semi-supervised few-shot learning for medical image segmentation arXiv PDF CODE   Few-Shot Segmentation Without Meta-Learning: A Good Transductive Inference Is All You Need? arXiv PDF CODE   Meta-Learning Initializations for Image Segmentation NeurIPS-W PDF CODE   BriNet: Towards Bridging the Intra-class and Inter-class Gaps in One-Shot Segmentation arXiv PDF CODE   Self-Supervision with Superpixels: Training Few-shot Medical Image Segmentation without Annotation ECCV PDF CODE   Generalized Few-Shot Semantic Segmentation arXiv PDF -   FSS-1000: A 1000-Class Dataset for Few-Shot Segmentation CVPR PDF CODE   Few-Shot Semantic Segmentation with Democratic Attention Networks ECCV PDF -   Prototype Mixture Models for Few-shot Semantic Segmentation ECCV PDF CODE   PFENet: Prior Guided Feature Enrichment Network for Few-shot Segmentation TPAMI PDF CODE   Part-aware Prototype Network for Few-shot Semantic Segmentation ECCV PDF CODE   SimPropNet: Improved Similarity Propagation for Few-shot Image Segmentation IJCAI PDF -   Objectness-Aware One-Shot Semantic Segmentation arXiv PDF -   Self-Supervised Tuning for Few-Shot Segmentation arXiv PDF -   SG-One: Similarity Guidance Network for One-Shot Semantic Segmentation TCYB PDF CODE   CAFENet: Class-Agnostic Few-Shot Edge Detection Network arXiv PDF -   FGN: Fully Guided Network for Few-Shot Instance Segmentation CVPR PDF -   CRNet: Cross-Reference Networks for Few-Shot Segmentation CVPR PDF -   Differentiable Meta-learning Model for Few-shot Semantic Segmentation AAAI PDF -   Prototype Refinement Network for Few-Shot Segmentation arXiv PDF -   Weakly Supervised Few-shot Object Segmentation using Co-Attention with Visual and Semantic Embeddings IJCAI PDF -    2019    Title Venue PDF CODE     A deep one-shot network for query-based logo retrieval PR PDF -   PANet: Few-Shot Image Semantic Segmentation with Prototype Alignment ICCV PDF CODE   Pyramid Graph Networks with Connection Attentions for Region-Based One-Shot Semantic Segmentation ICCV PDF -   AMP: Adaptive Masked Proxies for Few-Shot Segmentation ICCV PDF CODE   Feature Weighting and Boosting for Few-Shot Segmentation ICCV PDF -   CANet: Class-Agnostic Segmentation Networks With Iterative Refinement and Attentive Few-Shot Learning CVPR PDF CODE   Adaptive Masked Weight Imprinting for Few-Shot Segmentation ICLRW PDF -   A New Local Transformation Module for Few-Shot Segmentation MMMM PDF    A New Few-shot Segmentation Network Based on Class Representation arXiv PDF     2018    Title Venue PDF CODE     Conditional networks for few-shot semantic segmentation ICLRW PDF CODE   Few-Shot Semantic Segmentation with Prototype Learning BMVC PDF -    2017    Title Venue PDF CODE     One-Shot Learning for Semantic Segmentation BMVC PDF CODE "
37,shekkizh/FCN.tensorflow,1.2k,https://github.com/shekkizh/FCN.tensorflow,"Updated on Feb 29, 2020","FCN.tensorflow Prerequisites Results Observations Useful Links      README.md           FCN.tensorflow Tensorflow implementation of Fully Convolutional Networks for Semantic Segmentation (FCNs). The implementation is largely based on the reference code provided by the authors of the paper link. The model was applied on the Scene Parsing Challenge dataset provided by MIT http://sceneparsing.csail.mit.edu/.  Prerequisites Results Observations Useful links  Prerequisites  The results were obtained after training for ~6-7 hrs on a 12GB TitanX. The code was originally written and tested with tensorflow0.11 and python2.7. The tf.summary calls have been updated to work with tensorflow version 0.12. To work with older versions of tensorflow use branch tf.0.11_compatible. Some of the problems while working with tensorflow1.0 and in windows have been discussed in Issue #9. To train model simply execute python FCN.py To visualize results for a random batch of images use flag --mode=visualize debug flag can be set during training to add information regarding activations, gradients, variables etc. The IPython notebook in logs folder can be used to view results in color as below.  Results Results were obtained by training the model in batches of 2 with resized image of 256x256. Note that although the training is done at this image size - Nothing prevents the model from working on arbitrary sized images. No post processing was done on the predicted images. Training was done for 9 epochs - The shorter training time explains why certain concepts seem semantically understood by the model while others were not. Results below are from randomly chosen images from validation dataset. Pretty much used the same network design as in the reference model implementation of the paper in caffe. The weights for the new layers added were initialized with small values, and the learning was done using Adam Optimizer (Learning rate = 1e-4).      Observations  The small batch size was necessary to fit the training model in memory but explains the slow learning Concepts that had many examples seem to be correctly identified and segmented - in the example above you can see that cars, persons were identified better. I believe this can be solved by training for longer epochs. Also the resizing of images cause loss of information - you can notice this in the fact smaller objects are segmented with less accuracy.   Now for the gradients,  If you closely watch the gradients you will notice the inital training is almost entirely on the new layers added - it is only after these layers are reasonably trained do we see the VGG layers get some gradient flow. This is understandable as changes the new layers affect the loss objective much more in the beginning. The earlier layers of the netowrk are initialized with VGG weights and so conceptually would require less tuning unless the train data is extremely varied - which in this case is not. The first layer of convolutional model captures low level information and since this entrirely dataset dependent you notice the gradients adjusting the first layer weights to accustom the model to the dataset. The other conv layers from VGG have very small gradients flowing as the concepts captured here are good enough for our end objective - Segmentation. This is the core reason Transfer Learning works so well. Just thought of pointing this out while here.   Useful Links  Video of the presentaion given by the authors on the paper - link "
38,Eromera/erfnet_pytorch,353,https://github.com/Eromera/erfnet_pytorch,"Updated on Dec 5, 2019","ERFNet (PyTorch version) Publications Packages Requirements: License      README.md           ERFNet (PyTorch version) This code is a toolbox that uses PyTorch for training and evaluating the ERFNet architecture for semantic segmentation. For the Original Torch version please go HERE NOTE: This PyTorch version has a slightly better result than the ones in the Torch version (used in the paper): 72.1 IoU in Val set and 69.8 IoU in test set.  Publications If you use this software in your research, please cite our publications: ""Efficient ConvNet for Real-time Semantic Segmentation"", E. Romera, J. M. Alvarez, L. M. Bergasa and R. Arroyo, IEEE Intelligent Vehicles Symposium (IV), pp. 1789-1794, Redondo Beach (California, USA), June 2017. [Best Student Paper Award], [pdf] ""ERFNet: Efficient Residual Factorized ConvNet for Real-time Semantic Segmentation"", E. Romera, J. M. Alvarez, L. M. Bergasa and R. Arroyo, Transactions on Intelligent Transportation Systems (T-ITS), December 2017. [pdf] Packages For instructions please refer to the README on each folder:  train contains tools for training the network for semantic segmentation. eval contains tools for evaluating/visualizing the network's output. imagenet Contains script and model for pretraining ERFNet's encoder in Imagenet. trained_models Contains the trained models used in the papers. NOTE: the pytorch version is slightly different from the torch models.  Requirements:  The Cityscapes dataset: Download the ""leftImg8bit"" for the RGB images and the ""gtFine"" for the labels. Please note that for training you should use the ""_labelTrainIds"" and not the ""_labelIds"", you can download the cityscapes scripts and use the conversor to generate trainIds from labelIds Python 3.6: If you don't have Python3.6 in your system, I recommend installing it with Anaconda PyTorch: Make sure to install the Pytorch version for Python 3.6 with CUDA support (code only tested for CUDA 8.0). Additional Python packages: numpy, matplotlib, Pillow, torchvision and visdom (optional for --visualize flag)  In Anaconda you can install with: conda install numpy matplotlib torchvision Pillow conda install -c conda-forge visdom           If you use Pip (make sure to have it configured for Python3.6) you can install with: pip install numpy matplotlib torchvision Pillow visdom           License This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License, which allows for personal and research use only. For a commercial license please contact the authors. You can view a license summary here: http://creativecommons.org/licenses/by-nc/4.0/ "
39,shruti-jadon/Semantic-Segmentation-Loss-Functions,304,https://github.com/shruti-jadon/Semantic-Segmentation-Loss-Functions,"Updated on Jun 13, 2021","Semantic-Segmentation-Loss-Functions (SemSegLoss) Citation Summarized Loss functions and their use-cases      README.md           Semantic-Segmentation-Loss-Functions (SemSegLoss) This Repository is implementation of majority of Semantic Segmentation Loss Functions in Keras. Our paper is available open-source on following sites:  Survey Paper DOI: 10.1109/CIBCB48159.2020.9277638 Software Release DOI: https://doi.org/10.1016/j.simpa.2021.100078  In this paper we have summarized 15 such segmentation based loss functions that has been proven to provide state of results in different domain datasets. We are still in process of adding more loss functions, so far we this repo consists of:  Binary Cross Entropy Weighted Cross Entropy Balanced Cross Entropy Dice Loss Focal loss Tversky loss Focal Tversky loss log-cosh dice loss (ours)  This paper is extension of our work on traumatic brain lesion segmentation published at SPIE Medical Imaging'20. Github Code: https://github.com/shruti-jadon/Traumatic-Brain-Lesions-Segmentation Citation If you find our code useful, please consider citing our work using the bibtex: @inproceedings{jadon2020survey,   title={A survey of loss functions for semantic segmentation},   author={Jadon, Shruti},   booktitle={2020 IEEE Conference on Computational Intelligence in Bioinformatics and Computational Biology (CIBCB)},   pages={1--7},   year={2020},   organization={IEEE} } @article{JADON2021100078, title = {SemSegLoss: A python package of loss functions for semantic segmentation}, journal = {Software Impacts}, volume = {9}, pages = {100078}, year = {2021}, issn = {2665-9638}, doi = {https://doi.org/10.1016/j.simpa.2021.100078}, url = {https://www.sciencedirect.com/science/article/pii/S2665963821000269}, author = {Shruti Jadon}, keywords = {Deep Learning, Image segmentation, Medical imaging, Loss functions}, abstract = {Image Segmentation has been an active field of research as it has a wide range of applications, ranging from automated disease detection to self-driving cars. In recent years, various research papers proposed different loss functions used in case of biased data, sparse segmentation, and unbalanced dataset. In this paper, we introduce SemSegLoss, a python package consisting of some of the well-known loss functions widely used for image segmentation. It is developed with the intent to help researchers in the development of novel loss functions and perform an extensive set of experiments on model architectures for various applications. The ease-of-use and flexibility of the presented package have allowed reducing the development time and increased evaluation strategies of machine learning models for semantic segmentation. Furthermore, different applications that use image segmentation can use SemSegLoss because of the generality of its functions. This wide range of applications will lead to the development and growth of AI across all industries.} }           Summarized Loss functions and their use-cases "
40,MarvinTeichmann/KittiSeg,891,https://github.com/MarvinTeichmann/KittiSeg,"Updated on Mar 2, 2018","KittiSeg Requirements Setup To update an existing installation do: Tutorial Getting started Manage Data Storage RUNDIR and Experiment Organization Modifying Model & Train on your own data Utilize TensorVision backend Useful Flags & Variabels Questions? Citation      README.md           KittiSeg KittiSeg performs segmentation of roads by utilizing an FCN based model. The model achieved first place on the Kitti Road Detection Benchmark at submission time. Check out our paper for a detailed model description.   The model is designed to perform well on small datasets. The training is done using just 250 densely labelled images. Despite this a state-of-the art MaxF1 score of over 96% is achieved. The model is usable for real-time application. Inference can be performed at the impressive speed of 95ms per image. The repository contains code for training, evaluating and visualizing semantic segmentation in TensorFlow. It is build to be compatible with the TensorVision back end which allows to organize experiments in a very clean way. Also check out KittiBox a similar projects to perform state-of-the art detection. And finally the MultiNet repository contains code to jointly train segmentation, classification and detection. KittiSeg and KittiBox are utilized as submodules in MultiNet. Requirements The code requires Tensorflow 1.0, python 2.7 as well as the following python libraries:  matplotlib numpy Pillow scipy commentjson  Those modules can be installed using: pip install numpy scipy pillow matplotlib commentjson or pip install -r requirements.txt. Setup  Clone this repository: git clone https://github.com/MarvinTeichmann/KittiSeg.git Initialize all submodules: git submodule update --init --recursive [Optional] Download Kitti Road Data:  Retrieve kitti data url here: http://www.cvlibs.net/download.php?file=data_road.zip Call python download_data.py --kitti_url URL_YOU_RETRIEVED    Running the model using demo.py does not require you to download kitti data (step 3). Step 3 is only required if you want to train your own model using train.py or bench a model agains the official evaluation score evaluate.py. Also note, that I recommend using download_data.py instead of downloading the data yourself. The script will also extract and prepare the data. See Section Manage data storage if you like to control where the data is stored. To update an existing installation do:  Pull all patches: git pull Update all submodules: git submodule update --init --recursive  If you forget the second step you might end up with an inconstant repository state. You will already have the new code for KittiSeg but run it old submodule versions code. This can work, but I do not run any tests to verify this. Tutorial Getting started Run: python demo.py --input_image data/demo/demo.png to obtain a prediction using demo.png as input. Run: python evaluate.py to evaluate a trained model. Run: python train.py --hypes hypes/KittiSeg.json to train a model using Kitti Data. If you like to understand the code, I would recommend looking at demo.py first. I have documented each step as  	thoroughly as possible in this file. Manage Data Storage KittiSeg allows to separate data storage from code. This is very useful in many server environments. By default, the data is stored in the folder KittiSeg/DATA and the output of runs in KittiSeg/RUNS. This behaviour can be changed by setting the bash environment variables: $TV_DIR_DATA and $TV_DIR_RUNS. Include  export TV_DIR_DATA=""/MY/LARGE/HDD/DATA"" in your .profile and the all data will be downloaded to /MY/LARGE/HDD/DATA/data_road. Include export TV_DIR_RUNS=""/MY/LARGE/HDD/RUNS"" in your .profile and all runs will be saved to /MY/LARGE/HDD/RUNS/KittiSeg RUNDIR and Experiment Organization KittiSeg helps you to organize large number of experiments. To do so the output of each run is stored in its own rundir. Each rundir contains:  output.log a copy of the training output which was printed to your screen tensorflow events tensorboard can be run in rundir tensorflow checkpoints the trained model can be loaded from rundir [dir] images a folder containing example output images. image_iter controls how often the whole validation set is dumped [dir] model_files A copy of all source code need to build the model. This can be very useful of you have many versions of the model.  To keep track of all the experiments, you can give each rundir a unique name with the --name flag. The --project flag will store the run in a separate subfolder allowing to run different series of experiments. As an example, python train.py --project batch_size_bench --name size_5 will use the following dir as rundir:  $TV_DIR_RUNS/KittiSeg/batch_size_bench/size_5_KittiSeg_2017_02_08_13.12. The flag --nosave is very useful to not spam your rundir. Modifying Model & Train on your own data The model is controlled by the file hypes/KittiSeg.json. Modifying this file should be enough to train the model on your own data and adjust the architecture according to your needs. A description of the expected input format can be found here. For advanced modifications, the code is controlled by 5 different modules, which are specified in hypes/KittiSeg.json. ""model"": {    ""input_file"": ""../inputs/kitti_seg_input.py"",    ""architecture_file"" : ""../encoder/fcn8_vgg.py"",    ""objective_file"" : ""../decoder/kitti_multiloss.py"",    ""optimizer_file"" : ""../optimizer/generic_optimizer.py"",    ""evaluator_file"" : ""../evals/kitti_eval.py"" },           Those modules operate independently. This allows easy experiments with different datasets (input_file), encoder networks (architecture_file), etc. Also see TensorVision for a specification of each of those files. Utilize TensorVision backend KittiSeg is build on top of the TensorVision TensorVision backend. TensorVision modularizes computer vision training and helps organizing experiments. To utilize the entire TensorVision functionality install it using $ cd KittiSeg/submodules/TensorVision $ python setup.py install Now you can use the TensorVision command line tools, which includes: tv-train --hypes hypes/KittiSeg.json trains a json model. tv-continue --logdir PATH/TO/RUNDIR trains the model in RUNDIR, starting from the last saved checkpoint. Can be used for fine tuning by increasing max_steps in model_files/hypes.json . tv-analyze --logdir PATH/TO/RUNDIR evaluates the model in RUNDIR Useful Flags & Variabels Here are some Flags which will be useful when working with KittiSeg and TensorVision. All flags are available across all scripts. --hypes : specify which hype-file to use --logdir : specify which logdir to use --gpus : specify on which GPUs to run the code --name : assign a name to the run --project : assign a project to the run --nosave : debug run, logdir will be set to debug In addition the following TensorVision environment Variables will be useful: $TV_DIR_DATA: specify meta directory for data $TV_DIR_RUNS: specify meta directory for output $TV_USE_GPUS: specify default GPU behaviour. On a cluster it is useful to set $TV_USE_GPUS=force. This will make the flag --gpus mandatory and ensure, that run will be executed on the right GPU. Questions? Please have a look into the FAQ. Also feel free to open an issue to discuss any questions not covered so far. Citation If you benefit from this code, please cite our paper: @article{teichmann2016multinet,   title={MultiNet: Real-time Joint Semantic Reasoning for Autonomous Driving},   author={Teichmann, Marvin and Weber, Michael and Zoellner, Marius and Cipolla, Roberto and Urtasun, Raquel},   journal={arXiv preprint arXiv:1612.07695},   year={2016} } "
41,luwill/Semantic-Segmentation-Guide,155,https://github.com/luwill/Semantic-Segmentation-Guide,"Updated on Jul 9, 2020","深度学习语义分割理论与实战指南 A Theory and Practical Guide to Deep Learning Semantic Segmentation 引言 1. 语义分割概述 2. 关键技术组件 2.1 编码器与分类网络 2.2 解码器与上采样 双线性插值 转置卷积 反池化 2.3 Skip Connection 2.4 Dilate Conv与多尺度 2.5 后处理技术 2.6 深监督 2.7 通用技术 损失函数 精度描述 3. 数据Pipeline 3.1 Torch数据读取模板 3.2 transform与数据增强 4. 模型与算法 4.1 FCN 4.2 UNet 4.3 SegNet 4.4 Deeplab系列 4.5 PSPNet 4.6 UNet++ 5. 语义分割训练Tips 5.1 PyTorch代码搭建方式 5.2 可视化方法 Visdom TensorBoard 参考文献      README.md      深度学习语义分割理论与实战指南 A Theory and Practical Guide to Deep Learning Semantic Segmentation v1.0 louwill Machine Learning Lab   Fig0. Machine Learning Lab     引言 图像分类、目标检测和图像分割是基于深度学习的计算机视觉三大核心任务。三大任务之间明显存在着一种递进的层级关系，图像分类聚焦于整张图像，目标检测定位于图像具体区域，而图像分割则是细化到每一个像素。基于深度学习的图像分割具体包括语义分割、实例分割和全景分割。语义分割的目的是要给每个像素赋予一个语义标签。语义分割在自动驾驶、场景解析、卫星遥感图像和医学影像等领域都有着广泛的应用前景。本文作为基于PyTorch的语义分割技术手册，对语义分割的基本技术框架、主要网络模型和技术方法提供一个实战性指导和参考。 1. 语义分割概述 图像分割主要包括语义分割（Semantic Segmentation）和实例分割（Instance Segmentation）。那语义分割和实例分割具体都是什么含义？二者又有什么区别和联系？语义分割是对图像中的每个像素都划分出对应的类别，即实现像素级别的分类；而类的具体对象，即为实例，那么实例分割不但要进行像素级别的分类，还需在具体的类别基础上区别开不同的个体。例如，图像有多个人甲、乙、丙，那边他们的语义分割结果都是人，而实例分割结果却是不同的对象。另外，为了同时实现实例分割与不可数类别的语义分割，相关研究又提出了全景分割（Panoptic Segmentation）的概念。语义分割、实例分割和全景分割具体如图1（b）、（c）和（d）图所示。   Fig1. Image Segmentation  在开始图像分割的学习和尝试之前，我们必须明确语义分割的任务描述，即搞清楚语义分割的输入输出都是什么。输入是一张原始的RGB图像或者单通道图像，但是输出不再是简单的分类类别或者目标定位，而是带有各个像素类别标签的与输入同分辨率的分割图像。简单来说，我们的输入输出都是图像，而且是同样大小的图像。如图2所示。    Fig2. Pixel Representation  类似于处理分类标签数据，对预测分类目标采用像素上的one-hot编码，即为每个分类类别创建一个输出的通道。如图3所示。    Fig3. Pixel One-hot  图4是将分割图添加到原始图像上的叠加效果。这里需要明确一下mask的概念，在图像处理中我们将其译为掩码，如Mask R-CNN中的Mask。Mask可以理解为我们将预测结果叠加到单个通道时得到的该分类所在区域。    Fig4. Pixel labeling  所以，语义分割的任务就是输入图像经过深度学习算法处理得到带有语义标签的同样尺寸的输出图像。 2. 关键技术组件 在语义分割发展早期，为了能够让深度学习进行像素级的分类任务，在分类任务的基础上对CNN做了一些修改，将分类网络中浓缩语义表征的全连接层去掉，提出用全卷积网络（Fully Convolutional Networks）来处理语义分割问题。然后U-Net的提出，奠定了编解码结构的U形网络深度学习语义分割中的总统山地位。这里我们对语义分割的关键技术组件进行分开描述，编码器、解码器和Skip Connection属于分割网络的核心结构组件，空洞卷积（Dilate Conv）是独立于U形结构的第二大核心设计。条件随机场（CRF）和马尔科夫随机场（MRF）则是用于优化神经网络分割后的细节处理。深监督作为一种常用的结构设计Trick，在分割网络中也有广泛应用。除此之外，则是针对于语义分割的通用技术点。 2.1 编码器与分类网络 编码器对于分割网络来说就是进行特征提取和语义信息浓缩的过程，这对熟悉各种分类网络的我们来说并不陌生。编码器通过卷积和池化的组合不断对图像进行下采样，得到的特征图空间尺寸也会越来越小，但会更加具备语义分辨性。这也是大多数分类网络的通用模式，不断卷积池化使得特征图越来越小，然后配上几层全连接网络即可进行分类判别。常用的分类网络包括AlexNet、VGG、ResNet、Inception、DenseNet和MobileNet等等。  既然之前有那么多优秀的SOTA网络用来做特征提取，所以很多时候分割网络的编码器并不需要我们write from scratch，时刻要有迁移学习的敏感度，直接用现成分类网络的卷积层部分作为编码器进行特征提取和信息浓缩，往往要比从头开始训练一个编码器要快很多。  比如我们以VGG16作为SegNet编码器的预训练模型，以PyTorch为例，来看编码器的写法。 from torchvision import models  class SegNet(nn.Module):      def __init__(self, classes):         super().__init__()         vgg16 = models.vgg16(pretrained=True)         features = vgg16.features         self.enc1 = features[0: 4]         self.enc2 = features[5: 9]         self.enc3 = features[10: 16]         self.enc4 = features[17: 23]         self.enc5 = features[24: -1]          在上述代码中，可以看到我们将vgg16的31个层分作5个编码模块，每个编码模块的基本结构如下所示： (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): ReLU(inplace=True) (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (3): ReLU(inplace=True) (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)          2.2 解码器与上采样 编码器不断将输入不断进行下采样达到信息浓缩，而解码器则负责上采样来恢复输入尺寸。解码器中除了一些卷积方法用为辅助之外，最关键的还是一些上采样方法，主要包括双线性插值、转置卷积和反池化。 双线性插值 插值法（Interpolation）是一种经典的数值分析方法，一些经典插值大家或多或少都有听到过，比如线性插值、三次样条插值和拉格朗日插值法等。在说双线性插值前我们先来了解一下什么是线性插值（Linear interpolation）。线性插值法是指使用连接两个已知量的直线来确定在这两个已知量之间的一个未知量的值的方法。如下图所示：   Fig5. Linear Interpolation  已知直线上两点坐标分别为$(x_1,y_1)$和$(x_2,y_2)$，现在想要通过线性插值法来得到某一点$x$在直线上的值。基本就是一个初中数学题，这里就不做过多展开，点$x$在直线上的值$y$可以表示为： $y=\frac{x_2-x}{x_2-x_1}y_2+\frac{x-x_1}{x_2-x_1}y_1$ 再来看双线性插值。线性插值用到两个点来确定插值，双线性插值则需要四个点。在图像上采样中，双线性插值利用四个点的像素值来确定要插值的一个像素值，其本质上还是分别在$x$和$y$方向上分别进行两次线性插值。如下图所示，我们来看具体做法。   Fig6. Bilinear Interpolation  图中$Q_{11}-Q_{22}$四个黄色的点是已知数据点，红色点$P$是待插值点。假设$Q_{11}$为$(x_1,y_1)$，$Q_{12}$为$(x_1,y_2)$，$Q_{21}$为$(x_2,y_1)$，$Q_{22}$为$(x_2,y_2)$。我们先在$x$轴方向上进行线性插值，先求得$R_1$和$R_2$的插值。根据线性插值公式，有： $f(R_1)=\frac{x_2-x}{x_2-x_1}f(Q_{11})+\frac{x-x_1}{x_2-x_1}f(Q_{21})$ $f(R_2)=\frac{x_2-x}{x_2-x_1}f(Q_{12})+\frac{x-x_1}{x_2-x_1}f(Q_{22})$ 得到$R_1$和$R_2$点坐标之后，便可继续在$y$轴方向进行线性插值。可得目标点$P$的插值为： $f(P)=\frac{y_2-y}{y_2-y_1}f(R_1)+\frac{y-y_1}{y_2-y_1}f(R_2)$ 双线性插值在众多经典的语义分割网络中都有用到，比如说奠定语义分割编解码框架的FCN网络。假设将$3\times6$的图像通过双线性插值变为$6\times12$的图像，如下图所示。   Fig7. Bilinear Interpolation Example   双线性插值的优点是速度非常快，计算量小，但缺点就是效果不是特别理想。  转置卷积 转置卷积（Transposed Convolution）也叫解卷积（Deconvolution），有些人也将其称为反卷积，但这个叫法并不太准确。大家都知道，在常规卷积时，我们每次得到的卷积特征图尺寸是越来越小的。但在图像分割等领域，我们是需要逐步恢复输入时的尺寸的。如果把常规卷积时的特征图不断变小叫做下采样，那么通过转置卷积来恢复分辨率的操作可以称作上采样。 本质上来说，转置卷积跟常规卷积并无区别。不同之处在于先按照一定的比例进行padding来扩大输入尺寸，然后把常规卷积中的卷积核进行转置，再按常规卷积方法进行卷积就是转置卷积。假设输入图像矩阵为$X$，卷积核矩阵为$C$，常规卷积的输出为$Y$，则有： $Y=CX$ 两边同时乘以卷积核的转置$C^T$，这个公式便是转置卷积的输入输出计算。 $X=C^TY$ 假设输入大小为$4\times4$，滤波器大小为$3\times3$，常规卷积下输出为$2\times2$，为了演示转置卷积，我们将滤波器矩阵进行稀疏化处理为$4\times16$，将输入矩阵进行拉平为$16\times1$，相应输出结果也会拉平为$4\times1$，图示如下：   Fig8. Matrix of Convolution 然后按照转置卷积的做法我们把卷积核矩阵进行转置，按照$X=C^TY$进行验证：   Fig9. Matrix of Transpose Convolution 反池化 反池化（Unpooling）可以理解为池化的逆操作，相较于前两种上采样方法，反池化用的并不是特别多。其简要原理如下，在池化时记录下对应kernel中的坐标，在反池化时将一个元素根据kernel进行放大，根据之前的坐标将元素填写进去，其他位置补位为0即可。 2.3 Skip Connection 跳跃连接本身是在ResNet中率先提出，用于学习一个恒等式和残差结构，后面在DenseNet、FCN和U-Net等网络中广泛使用。最典型的就是U-Net的跳跃连接，在每个编码和解码层之间各添加一个跳跃连接，每一次下采样都会有一个跳跃连接与对应的上采样进行级联，这种不同尺度的特征融合对上采样恢复像素大有帮助。 2.4 Dilate Conv与多尺度 空洞卷积（Dilated/Atrous Convolution）也叫扩张卷积或者膨胀卷积，字面意思上来说就是在卷积核中插入空洞，起到扩大感受野的作用。空洞卷积的直接做法是在常规卷积核中填充0，用来扩大感受野，且进行计算时，空洞卷积中实际只有非零的元素起了作用。假设以一个变量a来衡量空洞卷积的扩张系数，则加入空洞之后的实际卷积核尺寸与原始卷积核尺寸之间的关系： $K=k+(k-1)(a-1)$ 其中$k$为原始卷积核大小，$a$为卷积扩张率（dilation rate），$K$为经过扩展后实际卷积核大小。除此之外，空洞卷积的卷积方式跟常规卷积一样。当$a=1$时，空洞卷积就退化为常规卷积。$a=1,2,4$时，空洞卷积示意图如下：   Fig10. Dialate Convolution  当$a=1$，原始卷积核size为$3\times3$，就是常规卷积。$a=2$时，加入空洞之后的卷积核$size=3+(3-1)\times(2-1)=5$，对应的感受野可计算为$2^{(a+2)}-1=7$。$a=3$时，卷积核size可以变化到$3+(3-1)(4-1)=9$，感受野则增长到$2^{(a+2)}-1=15$。对比不加空洞卷积的情况，在stride为1的情况下3层3x3卷积的叠加，第三层输出特征图对应的感受野也只有$1+(3-1)\times3=7$。所以，空洞卷积的一个重要作用就是增大感受野。 在语义分割的发展历程中，增大感受野是一个非常重要的设计。早期FCN提出以全卷积方式来处理像素级别的分割任务时，包括后来奠定语义分割baseline地位的U-Net，网络结构中存在大量的池化层来进行下采样，大量使用池化层的结果就是损失掉了一些信息，在解码上采样重建分辨率的时候肯定会有影响。特别是对于多目标、小物体的语义分割问题，以U-Net为代表的分割模型一直存在着精度瓶颈的问题。而基于增大感受野的动机背景下就提出了以空洞卷积为重大创新的deeplab系列分割网络，我们在深度学习语义分割模型中会对deeplab进行详述，这里不做过多展开。 对于语义分割而言，空洞卷积主要有三个作用：   第一是扩大感受野，具体前面已经说的比较多了，这里不做重复。但需要明确一点，池化也可以扩大感受野，但空间分辨率降低了，相比之下，空洞卷积可以在扩大感受野的同时不丢失分辨率，且保持像素的相对空间位置不变。简单而言就是空洞卷积可以同时控制感受野和分辨率。   第二就是获取多尺度上下文信息。当多个带有不同dilation rate的空洞卷积核叠加时，不同的感受野会带来多尺度信息，这对于分割任务是非常重要的。   第三就是可以降低计算量，不需要引入额外的参数，如上图空洞卷积示意图所示，实际卷积时只有带有红点的元素真正进行计算。    2.5 后处理技术  早期语义分割模型效果较为粗糙，在没有更好的特征提取模型的情况下，研究者们便在神经网络模型的粗糙结果进行后处理（Post-Processing），主要方法就是一些常用的概率图模型，比如说条件随机场（Conditional Random Field,CRF）和马尔可夫随机场（Markov Random Field,MRF）。 CRF是一种经典的概率图模型，简单而言就是给定一组输入序列的条件下，求另一组输出序列的条件概率分布模型，CRF在自然语言处理领域有着广泛应用。CRF在语义分割后处理中用法的基本思路如下：对于FCN或者其他分割网络的粗粒度分割结果而言，每个像素点$i$具有对应的类别标签$x_i$和观测值$y_i$，以每个像素为节点，以像素与像素之间的关系作为边即可构建一个CRF模型。在这个CRF模型中，我们通过观测变量$y_i$来预测像素$i$对应的标签值$x_i$。   Fig11. CRF 以上做法也叫DenseCRF，具体细节可参考论文： Efficient Inference in Fully Connected CRFs with Gaussian Edge Potentials，除此之外还有CRFasRNN，采用平均场近似的方式将CRF方法融入到神经网络过程中，本质上是对DenseCRF的一种优化。 另一种后处理概率图模型是MRF，MRF与CRF较为类似，只是对CRF的二元势函数做了调整，其优点在于可以使用平均场来构造CNN网络，并且推理过程可以一次性搞定。MRF在Deep Parsing Network(DPN)中有详细描述，相关细节可参考论文Semantic Image Segmentation via Deep Parsing Network。 语义分割发展前期，在分割网络模型的结果上加上CRF和MRF等后处理技术形成了早期的语义分割技术框架：   Fig12. Framework of Semantic Segmentation with CRF/MRF 但从Deeplab v3开始，主流的语义分割网络就不再热衷于后处理技术了。一个典型的观点认为神经网络分割效果不好才会用后处理技术，这说明在分割网络本身上还有很大的提升空间。一是CRF本身不太容易训练，二来语义分割任务的端到端趋势。后来语义分割领域的SOTA网络也确实证明了这一点。尽管如此，CRF等后处理技术作为语义分割发展历程上的一个重要方法，我们有必要在此进行说明。从另一方面看，深度学习和概率图的结合虽然并不是那么顺利，但相信未来依旧会大有前景。  2.6 深监督 所谓深监督（Deep Supervision），就是在深度神经网络的某些中间隐藏层加了一个辅助的分类器作为一种网络分支来对主干网络进行监督的技巧，用来解决深度神经网络训练梯度消失和收敛速度过慢等问题。 带有深监督的一个8层深度卷积网络结构如下图所示。   Fig13. Deep Supervision Example  可以看到，图中在第四个卷积块之后添加了一个监督分类器作为分支。Conv4输出的特征图除了随着主网络进入Conv5之外，也作为输入进入了分支分类器。如图所示，该分支分类器包括一个卷积块、两个带有Dropout和ReLu的全连接块和一个纯全连接块。带有深监督的卷积模块例子如下。 class C1DeepSup(nn.Module):     def __init__(self, num_class=150, fc_dim=2048, use_softmax=False):         super(C1DeepSup, self).__init__()         self.use_softmax = use_softmax         self.cbr = conv3x3_bn_relu(fc_dim, fc_dim // 4, 1)         self.cbr_deepsup = conv3x3_bn_relu(fc_dim // 2, fc_dim // 4, 1)         # 最后一层卷积         self.conv_last = nn.Conv2d(fc_dim // 4, num_class, 1, 1, 0)         self.conv_last_deepsup = nn.Conv2d(fc_dim // 4, num_class, 1, 1, 0)      # 前向计算流程     def forward(self, conv_out, segSize=None):         conv5 = conv_out[-1]         x = self.cbr(conv5)         x = self.conv_last(x)         # is True during inference         if self.use_softmax:             x = nn.functional.interpolate(                 x, size=segSize, mode='bilinear', align_corners=False)             x = nn.functional.softmax(x, dim=1)             return x         # 深监督模块         conv4 = conv_out[-2]         _ = self.cbr_deepsup(conv4)         _ = self.conv_last_deepsup(_)          # 主干卷积网络softmax输出         x = nn.functional.log_softmax(x, dim=1)         # 深监督分支网络softmax输出         _ = nn.functional.log_softmax(_, dim=1)         return (x, _)          2.7 通用技术 通用技术主要是指深度学习流程中会用到的基本模块，比如说损失函数的选取以及采用哪种精度衡量指标。其他的像优化器的选取，学习率的控制等，这里限于篇幅进行省略。 损失函数 常用的分类损失均可用作语义分割的损失函数。最常用的就是交叉熵损失函数，如果只是前景分割，则可以使用二分类的交叉熵损失（Binary CrossEntropy Loss, BCE loss），对于目标物体较小的情况我们可以使用Dice损失，对于目标物体类别不均衡的情况可以使用加权的交叉熵损失（Weighted CrossEntropy Loss, WCE Loss），另外也可以尝试多种损失函数的组合。 精度描述 语义分割作为经典的图像分割问题，其本质上还是一种图像像素分类。既然是分类，我们就可以使用常见的分类评价指标来评估模型好坏。语义分割常见的评价指标包括像素准确率（Pixel Accuracy）、平均像素准确率（Mean Pixel Accuracy）、平均交并比（Mean IoU）、频权交并比（FWIoU）和Dice系数（Dice Coeffcient）等。 像素准确率(PA)。 像素准确率跟分类中的准确率含义一样，即所有分类正确的像素数占全部像素的比例。PA的计算公式如下： $PA=\frac{\sum_{i=0}^{n}p_{ii}}{\sum_{i=0}^{n}\sum_{j=0}^{n}p_{ij}}$ 平均像素准确率(MPA)。 平均像素准确率其实更应该叫平均像素精确率，是指分别计算每个类别分类正确的像素数占所有预测为该类别像素数比例的平均值。所以，从定义上看，这是精确率(Precision)的定义，MPA的计算公式如下： $MPA=\frac{1}{n+1}\sum_{i=0}^{n}\frac{p_{ii}}{\sum_{j=0}^{n}p_{ij}}$ 平均交并比(MIoU)。 交并比（Intersection over Union）的定义很简单，将标签图像和预测图像看成是两个集合，计算两个集合的交集和并集的比值。而平均交并比则是将所有类的IoU取平均。 MIoU的计算公式如下： $MIoU=\frac{1}{n+1}\sum_{i=0}^{n}\frac{p_{ii}}{\sum_{j=0}^{n}p_{ij}+\sum_{j=0}^{n}p_{ji}-p_{ii}}$ 频权交并比(FWIoU)。 频权交并比顾名思义，就是以每一类别的频率为权重和其IoU加权计算出来的结果。FWIoU的设计思想很明确，语义分割很多时候会面临图像中各目标类别不平衡的情况，对各类别IoU直接求平均不是很合理，所以考虑各类别的权重就非常重要了。FWIoU的计算公式如下： $FWIoU=\frac{1}{\sum_{i=0}^{n}\sum_{j=0}^{n}p_{ij}}\sum_{i=0}^{n}\frac{\sum_{j=0}^{n}p_{ij}p_{ii}}{\sum_{j=0}^{n}p_{ij}+\sum_{j=0}^{n}p_{ji}-p_{ii}}$ Dice系数。 Dice系数是一种度量两个集合相似性的函数，是语义分割中最常用的评价指标之一。Dice系数定义为两倍的交集除以像素和，跟IoU有点类似，其计算公式如下： $dice=\frac{2|X\cap{Y}|}{|X|+|Y|}$ dice本质上跟分类指标中的F1-Score类似。作为最常用的分割指标之一，这里给出PyTorch的实现方式。 import torch  def dice_coef(pred, target):     """"""     Dice = (2*|X & Y|)/ (|X|+ |Y|)          =  2*sum(|A*B|)/(sum(A^2)+sum(B^2))     """"""     smooth = 1.     m1 = pred.view(-1).float()     m2 = target.view(-1).float()     intersection = (m1 * m2).sum().float()     dice = (2. * intersection + smooth) / (torch.sum(m1*m1) + torch.sum(m2*m2) + smooth)     return dice          3. 数据Pipeline 这里主要说一下PyTorch的自定义数据读取pipeline模板和相关trciks以及如何优化数据读取的pipeline等。我们从PyTorch的数据对象类Dataset开始。Dataset在PyTorch中的模块位于utils.data下。 from torch.utils.data import Dataset          3.1 Torch数据读取模板 PyTorch官方为我们提供了自定义数据读取的标准化代码代码模块，作为一个读取框架，我们这里称之为原始模板。其代码结构如下： from torch.utils.data import Dataset  class CustomDataset(Dataset):     def __init__(self, ...):         # stuff      def __getitem__(self, index):         # stuff         return (img, label)      def __len__(self):         # return examples size         return count          3.2 transform与数据增强 PyTorch数据增强功能可以放在transform模块下，添加transform后的数据读取结构如下所示： from torch.utils.data import Dataset from torchvision import transforms as T  class CustomDataset(Dataset):     def __init__(self, ...):         # stuff         # ...         # compose the transforms methods         self.transform = T.Compose([T.CenterCrop(100),                                 T.RandomResizedCrop(256),                                 T.RandomRotation(45),                                 T.ToTensor()])      def __getitem__(self, index):         # stuff         # ...         data = # Some data read from a file or image         labe = # Some data read from a file or image          # execute the transform         data = self.transform(data)         label = self.transform(label)         return (img, label)      def __len__(self):         # return examples size         return count  if __name__ == '__main__':     # Call the dataset     custom_dataset = CustomDataset(...)          需要说明的是，PyTorch transform模块所做的数据增强并不是我们所理解的广义上的数据增强。transform所做的增强，仅仅是在数据读取过程中随机地对某张图像做转化操作，实际数据量上并没有增多，可以将其视为是一种在线增强的策略。如果想要实现实际训练数据成倍数的增加，可以使用离线增强策略。 与图像分类仅需要对输入图像做增强不同的是，对于语义分割的数据增强而言，需要同时对输入图像和输入的mask同步进行数据增强工作。实际写代码时，要记得使用随机种子，在不失随机性的同时，保证输入图像和输出mask具备同样的转换。一个完整的语义分割在线数据增强代码实例如下： import os import random import torch from torch.utils.data import Dataset from PIL import Image  class SegmentationDataset(Dataset):     # read the input images     @staticmethod     def _load_input_image(path):         with open(path, 'rb') as f:             img = Image.open(f)             return img.convert('RGB')     # read the mask images     @staticmethod     def _load_target_image(path):         with open(path, 'rb') as f:             img = Image.open(f)             return img.convert('L')      def __init__(self, input_root, target_root, transform_input=None,                  transform_target=None, seed_fn=None):         self.input_root = input_root         self.target_root = target_root         self.transform_input = transform_input         self.transform_target = transform_target         self.seed_fn = seed_fn         # sort the ids         self.input_ids = sorted(img for img in os.listdir(self.input_root))         self.target_ids = sorted(img for img in os.listdir(self.target_root))         assert(len(self.input_ids) == len(self.target_ids))      # set random number seed     def _set_seed(self, seed):         random.seed(seed)         torch.manual_seed(seed)         if self.seed_fn:             self.seed_fn(seed)      def __getitem__(self, idx):         input_img = self._load_input_image(             os.path.join(self.input_root, self.input_ids[idx]))         target_img = self._load_target_image(             os.path.join(self.target_root, self.target_ids[idx]))          if self.transform_input:             # ensure that the input and output have the same randomness.             seed = random.randint(0, 2**32)             self._set_seed(seed)             input_img = self.transform_input(input_img)             self._set_seed(seed)             target_img = self.transform_target(target_img)         return input_img, target_img, self.input_ids[idx]      def __len__(self):         return len(self.input_ids)          其中transform_input和transform_target均可由transform模块下的函数封装而成。一个皮肤病灶分割的在线数据增强实例效果如下图所示。   Fig14. Example of online augmentation 4. 模型与算法 早期基于深度学习的图像分割以FCN为核心，旨在重点解决如何更好从卷积下采样中恢复丢掉的信息损失。后来逐渐形成了以U-Net为核心的这样一种编解码对称的U形结构。语义分割界迄今为止最重要的两个设计，一个是以U-Net为代表的U形结构，目前基于U-Net结构的创新就层出不穷，比如说应用于3D图像的V-Net，嵌套U-Net结构的U-Net++等。除此在外还有SegNet、RefineNet、HRNet和FastFCN。另一个则是以DeepLab系列为代表的Dilation设计，主要包括DeepLab系列和PSPNet。随着模型的Baseline效果不断提升，语义分割任务的主要矛盾也逐从downsample损失恢复像素逐渐演变为如何更有效地利用context上下文信息。 4.1 FCN FCN（Fully Convilutional Networks）是语义分割领域的开山之作。FCN的提出是在2016年，相较于此前提出的AlexNet和VGG等卷积全连接的网络结构，FCN提出用卷积层代替全连接层来处理语义分割问题，这也是FCN的由来，即全卷积网络。 FCN的关键点主要有三，一是全卷积进行特征提取和下采样，二是双线性插值进行上采样，三是跳跃连接进行特征融合。   Fig15. FCN 利用PyTorch实现一个FCN-8网络： import torch import torch.nn as nn import torch.nn.init as init import torch.nn.functional as F  from torch.utils import model_zoo from torchvision import models  class FCN8(nn.Module):      def __init__(self, num_classes):         super().__init__()          feats = list(models.vgg16(pretrained=True).features.children())          self.feats = nn.Sequential(*feats[0:9])         self.feat3 = nn.Sequential(*feats[10:16])         self.feat4 = nn.Sequential(*feats[17:23])         self.feat5 = nn.Sequential(*feats[24:30])          for m in self.modules():             if isinstance(m, nn.Conv2d):                 m.requires_grad = False          self.fconn = nn.Sequential(             nn.Conv2d(512, 4096, 7),             nn.ReLU(inplace=True),             nn.Dropout(),             nn.Conv2d(4096, 4096, 1),             nn.ReLU(inplace=True),             nn.Dropout(),         )         self.score_feat3 = nn.Conv2d(256, num_classes, 1)         self.score_feat4 = nn.Conv2d(512, num_classes, 1)         self.score_fconn = nn.Conv2d(4096, num_classes, 1)      def forward(self, x):         feats = self.feats(x)         feat3 = self.feat3(feats)         feat4 = self.feat4(feat3)         feat5 = self.feat5(feat4)         fconn = self.fconn(feat5)          score_feat3 = self.score_feat3(feat3)         score_feat4 = self.score_feat4(feat4)         score_fconn = self.score_fconn(fconn)          score = F.upsample_bilinear(score_fconn, score_feat4.size()[2:])         score += score_feat4         score = F.upsample_bilinear(score, score_feat3.size()[2:])         score += score_feat3          return F.upsample_bilinear(score, x.size()[2:])          从代码中可以看到，我们使用了vgg16作为FCN-8的编码部分，这使得FCN-8具备较强的特征提取能力。 4.2 UNet 早期基于深度学习的图像分割以FCN为核心，旨在重点解决如何更好从卷积下采样中恢复丢掉的信息损失。后来逐渐形成了以UNet为核心的这样一种编解码对称的U形结构。 UNet结构能够在分割界具有一统之势，最根本的还是其效果好，尤其是在医学图像领域。所以，做医学影像相关的深度学习应用时，一定都用过UNet，而且最原始的UNet一般都会有一个不错的baseline表现。2015年发表UNet的MICCAI，是目前医学图像分析领域最顶级的国际会议，UNet为什么在医学上效果这么好非常值得探讨一番。 U-Net结构如下图所示：   Fig16. UNet  乍一看很复杂，U形结构下貌似有很多细节问题。我们来把UNet简化一下，如下图所示：    Fig17. U-Net简化  从图中可以看到，简化之后的UNet的关键点只有三条线：  下采样编码 上采样解码 跳跃连接  下采样进行信息浓缩和上采样进行像素恢复，这是其他分割网络都会有的部分，UNet自然也不会跳出这个框架，可以看到，UNet进行了4次的最大池化下采样，每一次采样后都使用了卷积进行信息提取得到特征图，然后再经过4次上采样恢复输入像素尺寸。但UNet最关键的、也是最特色的部分在于图中红色虚线的Skip Connection。每一次下采样都会有一个跳跃连接与对应的上采样进行级联，这种不同尺度的特征融合对上采样恢复像素大有帮助，具体来说就是高层（浅层）下采样倍数小，特征图具备更加细致的图特征，底层（深层）下采样倍数大，信息经过大量浓缩，空间损失大，但有助于目标区域（分类）判断，当high level和low level的特征进行融合时，分割效果往往会非常好。从某种程度上讲，这种跳跃连接也可以视为一种Deep Supervision。 U-Net的简单实现如下： # 编码块 class UNetEnc(nn.Module):      def __init__(self, in_channels, out_channels, dropout=False):         super().__init__()          layers = [             nn.Conv2d(in_channels, out_channels, 3, dilation=2),             nn.ReLU(inplace=True),             nn.Conv2d(out_channels, out_channels, 3, dilation=2),             nn.ReLU(inplace=True),         ]         if dropout:             layers += [nn.Dropout(.5)]         layers += [nn.MaxPool2d(2, stride=2, ceil_mode=True)]          self.down = nn.Sequential(*layers)      def forward(self, x):         return self.down(x)  # 解码块 class UNetDec(nn.Module):      def __init__(self, in_channels, features, out_channels):         super().__init__()          self.up = nn.Sequential(             nn.Conv2d(in_channels, features, 3),             nn.ReLU(inplace=True),             nn.Conv2d(features, features, 3),             nn.ReLU(inplace=True),             nn.ConvTranspose2d(features, out_channels, 2, stride=2),             nn.ReLU(inplace=True),         )      def forward(self, x):         return self.up(x) # U-Net class UNet(nn.Module):      def __init__(self, num_classes):         super().__init__()          self.enc1 = UNetEnc(3, 64)         self.enc2 = UNetEnc(64, 128)         self.enc3 = UNetEnc(128, 256)         self.enc4 = UNetEnc(256, 512, dropout=True)         self.center = nn.Sequential(             nn.Conv2d(512, 1024, 3),             nn.ReLU(inplace=True),             nn.Conv2d(1024, 1024, 3),             nn.ReLU(inplace=True),             nn.Dropout(),             nn.ConvTranspose2d(1024, 512, 2, stride=2),             nn.ReLU(inplace=True),         )         self.dec4 = UNetDec(1024, 512, 256)         self.dec3 = UNetDec(512, 256, 128)         self.dec2 = UNetDec(256, 128, 64)         self.dec1 = nn.Sequential(             nn.Conv2d(128, 64, 3),             nn.ReLU(inplace=True),             nn.Conv2d(64, 64, 3),             nn.ReLU(inplace=True),         )         self.final = nn.Conv2d(64, num_classes, 1)      # 前向传播过程     def forward(self, x):         enc1 = self.enc1(x)         enc2 = self.enc2(enc1)         enc3 = self.enc3(enc2)         enc4 = self.enc4(enc3)         center = self.center(enc4)         # 包含了同层分辨率级联的解码块         dec4 = self.dec4(torch.cat([             center, F.upsample_bilinear(enc4, center.size()[2:])], 1))         dec3 = self.dec3(torch.cat([             dec4, F.upsample_bilinear(enc3, dec4.size()[2:])], 1))         dec2 = self.dec2(torch.cat([             dec3, F.upsample_bilinear(enc2, dec3.size()[2:])], 1))         dec1 = self.dec1(torch.cat([             dec2, F.upsample_bilinear(enc1, dec2.size()[2:])], 1))          return F.upsample_bilinear(self.final(dec1), x.size()[2:])          4.3 SegNet SegNet网络是典型的编码-解码结构。SegNet编码器网络由VGG16的前13个卷积层构成，所以通常是使用VGG16的预训练权重来进行初始化。每个编码器层都有一个对应的解码器层，因此解码器层也有13层。编码器最后的输出输入到softmax分类器中，输出每个像素的类别概率。SegNet如下图所示。   Fig18. SegNet结构 SegNet的一个简易参考实现如下： import torch import torch.nn as nn import torch.nn.init as init import torch.nn.functional as F from torchvision import models  # define Decoder class SegNetDec(nn.Module):      def __init__(self, in_channels, out_channels, num_layers):         super().__init__()         layers = [             nn.Conv2d(in_channels, in_channels // 2, 3, padding=1),             nn.BatchNorm2d(in_channels // 2),             nn.ReLU(inplace=True),         ]         layers += [             nn.Conv2d(in_channels // 2, in_channels // 2, 3, padding=1),             nn.BatchNorm2d(in_channels // 2),             nn.ReLU(inplace=True),         ] * num_layers         layers += [             nn.Conv2d(in_channels // 2, out_channels, 3, padding=1),             nn.BatchNorm2d(out_channels),             nn.ReLU(inplace=True),         ]         self.decode = nn.Sequential(*layers)      def forward(self, x):         return self.decode(x)  # SegNet class SegNet(nn.Module):      def __init__(self, classes):         super().__init__()         vgg16 = models.vgg16(pretrained=True)         features = vgg16.features         self.enc1 = features[0: 4]         self.enc2 = features[5: 9]         self.enc3 = features[10: 16]         self.enc4 = features[17: 23]         self.enc5 = features[24: -1]          for m in self.modules():             if isinstance(m, nn.Conv2d):                 m.requires_grad = False          self.dec5 = SegNetDec(512, 512, 1)         self.dec4 = SegNetDec(512, 256, 1)         self.dec3 = SegNetDec(256, 128, 1)         self.dec2 = SegNetDec(128, 64, 0)          self.final = nn.Sequential(*[             nn.Conv2d(64, classes, 3, padding=1),             nn.BatchNorm2d(classes),             nn.ReLU(inplace=True)         ])      def forward(self, x):         x1 = self.enc1(x)         e1, m1 = F.max_pool2d(x1, kernel_size=2, stride=2, return_indices=True)         x2 = self.enc2(e1)         e2, m2 = F.max_pool2d(x2, kernel_size=2, stride=2, return_indices=True)         x3 = self.enc3(e2)         e3, m3 = F.max_pool2d(x3, kernel_size=2, stride=2, return_indices=True)         x4 = self.enc4(e3)         e4, m4 = F.max_pool2d(x4, kernel_size=2, stride=2, return_indices=True)         x5 = self.enc5(e4)         e5, m5 = F.max_pool2d(x5, kernel_size=2, stride=2, return_indices=True)          def upsample(d):             d5 = self.dec5(F.max_unpool2d(d, m5, kernel_size=2, stride=2, output_size=x5.size()))             d4 = self.dec4(F.max_unpool2d(d5, m4, kernel_size=2, stride=2, output_size=x4.size()))             d3 = self.dec3(F.max_unpool2d(d4, m3, kernel_size=2, stride=2, output_size=x3.size()))             d2 = self.dec2(F.max_unpool2d(d3, m2, kernel_size=2, stride=2, output_size=x2.size()))             d1 = F.max_unpool2d(d2, m1, kernel_size=2, stride=2, output_size=x1.size())             return d1          d = upsample(e5)         return self.final(d)          4.4 Deeplab系列 Deeplab系列可以算是深度学习语义分割的另一个主要架构，其代表方法就是基于Dilation的多尺度设计。Deeplab系列主要包括：  Deeplab v1 Deeplab v2 Deeplab v3 Deeplab v3+  Deeplab v1主要是率先使用了空洞卷积，是Deeplab系列最原始的版本。Deeplab v2在Deeplab v1的基础上最大的改进在于提出了ASPP（Atrous Spatial Pyramid Pooling），即带有空洞卷积的金字塔池化，该设计的主要目的就是提取图像的多尺度特征。另外Deeplab v2也将Deeplab v1的Backone网络更换为ResNet。Deeplab v1和v2还有一个比较大的特点就是使用了CRF作为后处理技术。 这里重点说一下多尺度问题。多尺度问题就是当图像中的目标对象存在不同大小时，分割效果不佳的现象。比如同样的物体，在近处拍摄时物体显得大，远处拍摄时显得小。解决多尺度问题的目标就是不论目标对象是大还是小，网络都能将其分割地很好。Deeplab v2使用ASPP处理多尺度问题，ASPP设计结构如下图所示。   Fig19. ASPP 从Deeplab v3开始，Deeplab系列舍弃了CRF后处理模块，提出了更加通用的、适用任何网络的分割框架，对ResNet最后的Block做了复制和级联（Cascade），对ASPP模块做了升级，在其中添加了BN层。改进后的ASPP如下图所示。   Fig20. ASPP of Deeplab v3 Deeplab v3+在Deeplab v3的基础上做了扩展和改进，其主要改进就是在编解码结构上使用了ASPP。Deeplab v3+可以视作是融合了语义分割两大流派的一项工作，即编解码+ASPP结构。另外Deeplab v3+的Backbone换成了Xception，其深度可分离卷积的设计使得分割网络更加高效。Deeplab v3+结构如下图所示。   Fig21. ASPP 关于Deeplab系列各个版本的技术点构成总结如下表所示。Deeplab系列算法实现可参考GitHub上各版本，这里不再一一给出。   Fig22. Summary of Deeplab series. 4.5 PSPNet PSPNet是针对多尺度问题提出的另一种代表性分割网络。PSPNet认为此前的分割网络没有引入足够的上下文信息及不同感受野下的全局信息而存在分割出现错误的情况，因而引入Global-Scence-Level的信息解决该问题，其Backbone网络也是ResNet。简单来说，PSPNet就是将Deeplab的ASPP模块之前的特征图Pooling了四种尺度，然后将原始特征图和四种Pooling之后的特征图进行合并到一起，再经过一系列卷积之后进行预测的过程。PSPNet结构如下图所示。   Fig23. PSPNet 一个简易的带有深监督的PSPNet PPM模块写法如下： # Pyramid Pooling Module class PPMDeepsup(nn.Module):     def __init__(self, num_class=150, fc_dim=4096,                  use_softmax=False, pool_scales=(1, 2, 3, 6)):         super(PPMDeepsup, self).__init__()         self.use_softmax = use_softmax         # PPM         self.ppm = []         for scale in pool_scales:             self.ppm.append(nn.Sequential(                 nn.AdaptiveAvgPool2d(scale),                 nn.Conv2d(fc_dim, 512, kernel_size=1, bias=False),                 BatchNorm2d(512),                 nn.ReLU(inplace=True)             ))         self.ppm = nn.ModuleList(self.ppm)         # Deep Supervision         self.cbr_deepsup = conv3x3_bn_relu(fc_dim // 2, fc_dim // 4, 1)          self.conv_last = nn.Sequential(             nn.Conv2d(fc_dim+len(pool_scales)*512, 512,                       kernel_size=3, padding=1, bias=False),             BatchNorm2d(512),             nn.ReLU(inplace=True),             nn.Dropout2d(0.1),             nn.Conv2d(512, num_class, kernel_size=1)         )         self.conv_last_deepsup = nn.Conv2d(fc_dim // 4, num_class, 1, 1, 0)         self.dropout_deepsup = nn.Dropout2d(0.1)          4.6 UNet++ 自从2015年UNet网络提出后，这么多年大家没少在这个U形结构上折腾。大部分做语义分割的朋友都没少在UNet结构上做各种魔改，如果把UNet++算作是UNet的一种魔改的话，那它一定是最成功的魔改者。 UNet++是一种嵌套的U-Net结构，即内置了不同深度的UNet网络，并且利用了全尺度的跳跃连接（skip connection）和深度监督（deep supervisions）。另外UNet++还设计一种剪枝方案，加快了UNet++的推理速度。UNet++的结构示意图如下所示。   Fig24. UNet++ 单纯从结构设计的角度来看，UNet++效果好要归功于其嵌套结构和重新设计的跳跃连接，旨在解决UNet的两个关键挑战:1）优化整体结构的未知深度和2）跳跃连接的不必要的限制性设计。UNet++的一个简单的实现代码如下所示。 import torch from torch import nn  class NestedUNet(nn.Module):     def __init__(self, num_classes, input_channels=3, deep_supervision=False, **kwargs):         super().__init__()         nb_filter = [32, 64, 128, 256, 512]         self.deep_supervision = deep_supervision          self.pool = nn.MaxPool2d(2, 2)         self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)          self.conv0_0 = VGGBlock(input_channels, nb_filter[0], nb_filter[0])         self.conv1_0 = VGGBlock(nb_filter[0], nb_filter[1], nb_filter[1])         self.conv2_0 = VGGBlock(nb_filter[1], nb_filter[2], nb_filter[2])         self.conv3_0 = VGGBlock(nb_filter[2], nb_filter[3], nb_filter[3])         self.conv4_0 = VGGBlock(nb_filter[3], nb_filter[4], nb_filter[4])          self.conv0_1 = VGGBlock(nb_filter[0]+nb_filter[1], nb_filter[0], nb_filter[0])         self.conv1_1 = VGGBlock(nb_filter[1]+nb_filter[2], nb_filter[1], nb_filter[1])         self.conv2_1 = VGGBlock(nb_filter[2]+nb_filter[3], nb_filter[2], nb_filter[2])         self.conv3_1 = VGGBlock(nb_filter[3]+nb_filter[4], nb_filter[3], nb_filter[3])          self.conv0_2 = VGGBlock(nb_filter[0]*2+nb_filter[1], nb_filter[0], nb_filter[0])         self.conv1_2 = VGGBlock(nb_filter[1]*2+nb_filter[2], nb_filter[1], nb_filter[1])         self.conv2_2 = VGGBlock(nb_filter[2]*2+nb_filter[3], nb_filter[2], nb_filter[2])          self.conv0_3 = VGGBlock(nb_filter[0]*3+nb_filter[1], nb_filter[0], nb_filter[0])         self.conv1_3 = VGGBlock(nb_filter[1]*3+nb_filter[2], nb_filter[1], nb_filter[1])          self.conv0_4 = VGGBlock(nb_filter[0]*4+nb_filter[1], nb_filter[0], nb_filter[0])          if self.deep_supervision:             self.final1 = nn.Conv2d(nb_filter[0], num_classes, kernel_size=1)             self.final2 = nn.Conv2d(nb_filter[0], num_classes, kernel_size=1)             self.final3 = nn.Conv2d(nb_filter[0], num_classes, kernel_size=1)             self.final4 = nn.Conv2d(nb_filter[0], num_classes, kernel_size=1)         else:             self.final = nn.Conv2d(nb_filter[0], num_classes, kernel_size=1)       def forward(self, input):         x0_0 = self.conv0_0(input)         x1_0 = self.conv1_0(self.pool(x0_0))         x0_1 = self.conv0_1(torch.cat([x0_0, self.up(x1_0)], 1))          x2_0 = self.conv2_0(self.pool(x1_0))         x1_1 = self.conv1_1(torch.cat([x1_0, self.up(x2_0)], 1))         x0_2 = self.conv0_2(torch.cat([x0_0, x0_1, self.up(x1_1)], 1))          x3_0 = self.conv3_0(self.pool(x2_0))         x2_1 = self.conv2_1(torch.cat([x2_0, self.up(x3_0)], 1))         x1_2 = self.conv1_2(torch.cat([x1_0, x1_1, self.up(x2_1)], 1))         x0_3 = self.conv0_3(torch.cat([x0_0, x0_1, x0_2, self.up(x1_2)], 1))          x4_0 = self.conv4_0(self.pool(x3_0))         x3_1 = self.conv3_1(torch.cat([x3_0, self.up(x4_0)], 1))         x2_2 = self.conv2_2(torch.cat([x2_0, x2_1, self.up(x3_1)], 1))         x1_3 = self.conv1_3(torch.cat([x1_0, x1_1, x1_2, self.up(x2_2)], 1))         x0_4 = self.conv0_4(torch.cat([x0_0, x0_1, x0_2, x0_3, self.up(x1_3)], 1))          if self.deep_supervision:             output1 = self.final1(x0_1)             output2 = self.final2(x0_2)             output3 = self.final3(x0_3)             output4 = self.final4(x0_4)             return [output1, output2, output3, output4]         else:             output = self.final(x0_4)             return output          完整实现过程可参考GitHub开源代码。 以上仅对几个主要的语义分割网络模型进行介绍，从当年的FCN到如今的各种模型层出不穷，想要对所有的SOTA模型全部进行介绍已经不太可能。其他诸如ENet、DeconvNet、RefineNet、HRNet、PixelNet、BiSeNet、UpperNet等网络模型，均各有千秋。本小节旨在让大家熟悉语义分割的主要模型结构和设计。深度学习和计算机视觉发展日新月异，一个新的SOTA模型出来，肯定很快就会被更新的结构设计所代替，重点是我们要了解语义分割的发展脉络，对主流的前沿研究能够保持一定的关注。 5. 语义分割训练Tips PyTorch是一款极为便利的深度学习框架。在日常实验过程中，我们要多积累和总结，假以时日，人人都能总结出一套自己的高效模型搭建和训练套路。这一节我们给出一些惯用的PyTorch代码搭建方式，以及语义分割训练过程中的可视化方法，方便大家在训练过程中能够直观的看到训练效果。 5.1 PyTorch代码搭建方式 无论是分类、检测还是分割抑或是其他非视觉的深度学习任务，其代码套路相对来说较为固定，不会跳出基本的代码框架。一个深度学习的实现代码框架无非就是以下五个主要构成部分：  数据：Data 模型：Model 判断：Criterion 优化：Optimizer 日志：Logger  所以一个基本的顺序实现范式如下： # data dataset = VOC()||COCO()||ADE20K() data_loader = data.DataLoader(dataSet)  # model model = ... model_parallel = torch.nn.DataParallel(model)  # Criterion loss = criterion(...)  # Optimizer optimer = optim.SGD(...)  # Logger and Visulization visdom = ... tensorboard = ... textlog = ...  # Model Parameters data_size, batch_size, epoch_size, iterations = ..., ...          不论是哪种深度学习任务，一般都免不了以上五项基本模块。所以一个简单的、相对完整的PyTorch模型项目代码应该是如下结构的： |-- semantic segmentation example     |-- dataset.py     |-- models         |-- unet.py         |-- deeplabv3.py         |-- pspnet.py         |-- ...     |-- _config.yml     |-- main.py     |-- utils     |   |-- visual.py     |   |-- loss.py     |   |-- ...     |-- README.md     ...           上面的示例代码结构中，我们把训练和验证相关代码都放到main.py文件中，但在实际实验中，这块的灵活性极大。一般来说，模型训练策略有三种，一种是边训练边验证最后再测试、另一种则是在训练中验证，将验证过程糅合到训练过程中，还有一种最简单，就是训练完了再单独验证和测试。所以，我们这里也可以单独定义对应的函数，训练train()、验证val()以及测试test()除此之外，还有一些辅助功能需要设计，包括打印训练信息print()、绘制损失函数plot()、保存最优模型save()，调整训练参数update()。 所以训练代码控制流程可以归纳为TVT+PPSU的模式。 5.2 可视化方法 PyTorch原生的可视化支持模块是Visdom，当然鉴于TensorFlow的应用广泛性，PyTorch同时也支持TensorBoard的可视化方法。语义分割需要能够直观的看到训练效果，所以在训练过程中辅以一定的可视化方法是十分必要的。 Visdom visdom是一款用于创建、组织和共享实时大量训练数据可视化的灵活工具。深度学习模型训练通常放在远程的服务器上，服务器上训练的一个问题就在于不能方便地对训练进行可视化，相较于TensorFlow的可视化工具TensorBoard，visdom则是对应于PyTorch的可视化工具。直接通过pip install visdom即可完成安装，之后在终端输入如下命令即可启动visdom服务： python -m visdom.server           启动服务后输入本地或者远程地址，端口号8097，即可打开visdom主页。具体到深度学习训练时，我们可以在torch训练代码下插入visdom的可视化模块： if args.steps_plot > 0 and step % args.steps_plot == 0:     image = inputs[0].cpu().data     vis.image(image,f'input (epoch: {epoch}, step: {step})')     vis.image(outputs[0].cpu().max(0)[1].data, f'output (epoch: {epoch}, step: {step})')     vis.image(targets[0].cpu().data, f'target (epoch: {epoch}, step: {step})')     vis.image(loss, f'loss (epoch: {epoch}, step: {step})')          visdom效果展示如下：   Fig25. visdom example TensorBoard 很多TensorFlow用户更习惯于使用TensorBoard来进行训练的可视化展示。为了能让PyTorch用户也能用上TensorBoard，有开发者提供了PyTorch版本的TensorBoard，也就是tensorboardX。熟悉TensorBoard的用户可以无缝对接到tensorboardX，安装方式为： pip install tensorboardX           除了要安装PyTorch之外，还需要安装TensorFlow。跟TensorBoard一样，tensorboardX也支持scalar, image, figure, histogram, audio, text, graph, onnx_graph, embedding, pr_curve，video等不同类型对象的可视化展示方式。tensorboardX和TensorBoard的启动方式一样，直接在终端下运行： tensorboard --logdir runs           一个完整tensorboardX使用demo如下： import torch import torchvision.utils as vutils import numpy as np import torchvision.models as models from torchvision import datasets from tensorboardX import SummaryWriter  resnet18 = models.resnet18(False) writer = SummaryWriter() sample_rate = 44100 freqs = [262, 294, 330, 349, 392, 440, 440, 440, 440, 440, 440]  for n_iter in range(100):      dummy_s1 = torch.rand(1)     dummy_s2 = torch.rand(1)     # data grouping by `slash`     writer.add_scalar('data/scalar1', dummy_s1[0], n_iter)     writer.add_scalar('data/scalar2', dummy_s2[0], n_iter)      writer.add_scalars('data/scalar_group', {'xsinx': n_iter * np.sin(n_iter),                                              'xcosx': n_iter * np.cos(n_iter),                                              'arctanx': np.arctan(n_iter)}, n_iter)      dummy_img = torch.rand(32, 3, 64, 64)  # output from network     if n_iter % 10 == 0:         x = vutils.make_grid(dummy_img, normalize=True, scale_each=True)         writer.add_image('Image', x, n_iter)          dummy_audio = torch.zeros(sample_rate * 2)         for i in range(x.size(0)):             # amplitude of sound should in [-1, 1]             dummy_audio[i] = np.cos(freqs[n_iter // 10] * np.pi * float(i) / float(sample_rate))         writer.add_audio('myAudio', dummy_audio, n_iter, sample_rate=sample_rate)          writer.add_text('Text', 'text logged at step:' + str(n_iter), n_iter)          for name, param in resnet18.named_parameters():             writer.add_histogram(name, param.clone().cpu().data.numpy(), n_iter)          # needs tensorboard 0.4RC or later         writer.add_pr_curve('xoxo', np.random.randint(2, size=100), np.random.rand(100), n_iter)  dataset = datasets.MNIST('mnist', train=False, download=True) images = dataset.test_data[:100].float() label = dataset.test_labels[:100]  features = images.view(100, 784) writer.add_embedding(features, metadata=label, label_img=images.unsqueeze(1))  # export scalar data to JSON for external processing writer.export_scalars_to_json(""./all_scalars.json"") writer.close()          tensorboardX的展示界面如图所示。   Fig26. tensorboardX example  参考文献  awesome-semantic-segmentation Long J , Shelhamer E , Darrell T . Fully Convolutional Networks for Semantic Segmentation[J]. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2015, 39(4):640-651. Ronneberger O , Fischer P , Brox T . U-Net: Convolutional Networks for Biomedical Image Segmentation[J]. 2015. Badrinarayanan V , Kendall A , Cipolla R . SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation[J]. 2015. Zhao H , Shi J , Qi X , et al. Pyramid Scene Parsing Network[J]. 2016. Huang H , Lin L , Tong R , et al. UNet 3+: A Full-Scale Connected UNet for Medical Image Segmentation[J]. arXiv, 2020. UNet++: A Nested U-Net Architecture for Medical Image Segmentation https://github.com/4uiiurz1/pytorch-nested-unet Minaee S , Boykov Y , Porikli F , et al. Image Segmentation Using Deep Learning: A Survey[J]. 2020. Guo Y , Liu Y , Georgiou T , et al. A review of semantic segmentation using deep neural networks[J]. International Journal of Multimedia Information Retrieval, 2017. https://github.com/fabioperez/pytorch-examples/ "
42,Wizaron/instance-segmentation-pytorch,409,https://github.com/Wizaron/instance-segmentation-pytorch,"Updated on Mar 8, 2020","Semantic Instance Segmentation with a Discriminative Loss Function Modules Networks Installation Data CVPPP Code Structure Data Preparation CVPPP Visdom Server Training Evaluation Prediction Results CVPPP Scores on validation subset (28 images) Sample Predictions References      README.md           Semantic Instance Segmentation with a Discriminative Loss Function This repository implements Semantic Instance Segmentation with a Discriminative Loss Function with some enhancements.  Reference paper does not predict semantic segmentation mask, instead it uses ground-truth semantic segmentation mask. This code predicts semantic segmentation mask, similar to Towards End-to-End Lane Detection: an Instance Segmentation Approach. Reference paper predicts the number of instances implicity. It predicts embeddings for instances and predicts the number of instances as a result of clustering. Instead, this code predicts the number of instances as an output of network. Reference paper uses a segmentation network based on ResNet-38. Instead, this code uses either ReSeg with skip-connections based on first seven convolutional layers of VGG16 as segmentation network or an augmented version of Stacked Recurrent Hourglass. This code uses KMeans Clustering; however, reference paper uses ""a fast variant of the mean-shift algorithm"".   Modules  Convolutional GRU Coordinate Convolution  AddCoordinates CoordConv CoordConvTranspose CoordConvNet   Recurrent Hourglass ReNet VGG16  VGG16 SkipVGG16    Networks  ReSeg Stacked Recurrent Hourglass   In prediction phase, network inputs an image and outputs a semantic segmentation mask, the number of instances and embeddings for all pixels in the image. Then, foreground embeddings (which correspond to instances) are selected using semantic segmentation mask and foreground embeddings are clustered into ""the number of instances"" groups via clustering. Installation  Clone this repository : git clone --recursive https://github.com/Wizaron/instance-segmentation-pytorch.git Install ImageMagick : sudo apt install imagemagick Download and install Anaconda or Miniconda Create a conda environment : conda env create -f instance-segmentation-pytorch/code/conda_environment.yml  Data CVPPP  Download CVPPP dataset and extract downloaded zip file (CVPPP2017_LSC_training.zip) to instance-segmentation-pytorch/data/raw/CVPPP/ This work uses A1 subset of the dataset.  Code Structure  code: Codes for training and evaluation.  lib  lib/archs: Stores network architectures. lib/archs/modules: Stores basic modules for architectures. lib/model.py: Defines model (optimization, criterion, fit, predict, test, etc.). lib/dataset.py: Data loading, augmentation, minibatching procedures. lib/preprocess.py, lib/utils: Data augmentation methods. lib/prediction.py: Prediction module. lib/losses/dice.py: Dice loss for foreground semantic segmentation. lib/losses/discriminative.py: Discriminative loss for instance segmentation.   settings  settings/CVPPP/data_settings.py: Defines settings about data. settings/CVPPP/model_settings.py: Defines settings about model (hyper-parameters). settings/CVPPP/training_settings.py: Defines settings for training (optimization method, weight decay, augmentation, etc.).   train.py: Training script. pred.py: Prediction script for single image. pred_list.py: Prediction scripts for a list of images. evaluate.py: Evaluation script. Calculates SBD (symmetric best dice), |DiC| (absolute difference in count) and Foreground Dice (Dice score for semantic segmentation) as defined in the paper.   data:  Stores data and scripts to prepare dataset for training and evaluation.  metadata/CVPPP: Stores metadata; such as, training, validation and test splits, image shapes etc. processed/CVPPP: Stores processed form of the data. raw/CVPPP: Stores raw form of the data. scripts: Stores scripts to prepare dataset.  scripts/CVPPP: For CVPPP dataset.  scripts/CVPPP/1-create_annotations.py: Saves annotations as a numpy array to processed/CVPPP/semantic-annotations/ and processed/CVPPP/instance-annotations. scripts/CVPPP/1-remove_alpha.sh: Removes alpha channels from images. (In order to run this script, imagemagick should be installed.). scripts/CVPPP/2-get_image_means-stds.py: Calculates and prints channel-wise means and standard deviations from training subset. scripts/CVPPP/2-get_image_shapes.py:  Saves image shapes to metadata/CVPPP/image_shapes.txt. scripts/CVPPP/2-get_number_of_instances.py: Saves the number of instances in each image to metadata/CVPPP/number_of_instances.txt. scripts/CVPPP/2-get_image_paths.py: Saves image paths to metadata/CVPPP/training_image_paths.txt, metadata/CVPPP/validation_image_paths.txt scripts/CVPPP/3-create_dataset.py: Creates an lmdb dataset to processed/CVPPP/lmdb/. * scripts/CVPPP/prepare.sh: Runs the scripts above in a sequential manner.       models/CVPPP: Stores checkpoints of the trained models. outputs/CVPPP: Stores predictions of the trained models.  Data Preparation Data should be prepared prior to training and evaluation.  Activate previously created conda environment : source activate ins-seg-pytorch or conda activate ins-seg-pytorch  CVPPP  Place the extracted dataset to instance-segmentation-pytorch/data/raw/CVPPP/. Hence, raw dataset should be found at instance-segmentation-pytorch/data/raw/CVPPP/CVPPP2017_LSC_training/. In order to prepare the data go to instance-segmentation-pytorch/data/scripts/CVPPP/ and run sh prepare.sh.  Visdom Server Start a Visdom server in a screen or tmux.   Activate previously created conda environment : source activate ins-seg-pytorch or conda activate ins-seg-pytorch   Start visdom server : python -m visdom.server   We can access visdom server using http://localhost:8097   Training   Activate previously created conda environment : source activate ins-seg-pytorch or conda activate ins-seg-pytorch   Go to instance-segmentation-pytorch/code/ and run train.py.   usage: train.py [-h] [--model MODEL] [--usegpu] [--nepochs NEPOCHS]                 [--batchsize BATCHSIZE] [--debug] [--nworkers NWORKERS]                 --dataset DATASET  optional arguments:   -h, --help            show this help message and exit   --model MODEL         Filepath of trained model (to continue training)                         [Default: '']   --usegpu              Enables cuda to train on gpu [Default: False]   --nepochs NEPOCHS     Number of epochs to train for [Default: 600]   --batchsize BATCHSIZE                         Batch size [Default: 2]   --debug               Activates debug mode [Default: False]   --nworkers NWORKERS   Number of workers for data loading (0 to do it using                         main process) [Default : 2]   --dataset DATASET     Name of the dataset which is ""CVPPP""           Debug mode plots pixel embeddings to visdom, it reduces size of the embeddings to two-dimensions using TSNE. Hence, it slows training down. As training continues, models are saved to instance-segmentation-pytorch/models/CVPPP. Evaluation After training is completed, we can make predictions.   Activate previously created conda environment : source activate ins-seg-pytorch or conda activate ins-seg-pytorch   Go to instance-segmentation-pytorch/code/.   Run pred_list.py.   usage: pred_list.py [-h] --lst LST --model MODEL [--usegpu]                     [--n_workers N_WORKERS] --dataset DATASET  optional arguments:   -h, --help            show this help message and exit   --lst LST             Text file that contains image paths   --model MODEL         Path of the model   --usegpu              Enables cuda to predict on gpu   --dataset DATASET     Name of the dataset which is ""CVPPP""           For example: python pred_list.py --lst ../data/metadata/CVPPP/validation_image_paths.txt  --model ../models/CVPPP/2018-3-4_16-15_jcmaxwell_29-937494/model_155_0.123682662845.pth --usegpu --n_workers 4 --dataset CVPPP  Predictions are written to outputs directory. After prediction is completed we can run evaluate.py. It prints output metrics to the stdout.  usage: evaluate.py [-h] --pred_dir PRED_DIR --dataset DATASET  optional arguments:   -h, --help           show this help message and exit   --pred_dir PRED_DIR  Prediction directory   --dataset DATASET    Name of the dataset which is ""CVPPP""           For example: python evaluate.py --pred_dir ../outputs/CVPPP/2018-3-4_16-15_jcmaxwell_29-937494-model_155_0.123682662845/validation/ --dataset CVPPP Prediction After training is complete, we can make predictions. We can use pred.py to make predictions for a single image.   Activate previously created conda environment : source activate ins-seg-pytorch or conda activate ins-seg-pytorch   Go to instance-segmentation-pytorch/code/.   Run pred.py.   usage: pred.py [-h] --image IMAGE --model MODEL [--usegpu] --output OUTPUT                [--n_workers N_WORKERS] --dataset DATASET  optional arguments:   -h, --help            show this help message and exit   --image IMAGE         Path of the image   --model MODEL         Path of the model   --usegpu              Enables cuda to predict on gpu   --output OUTPUT       Path of the output directory   --dataset DATASET     Name of the dataset which is ""CVPPP""           Results CVPPP Scores on validation subset (28 images)    SBD |DiC| Foreground Dice     87.9 0.5 96.8    Sample Predictions    References  VERY DEEP CONVOLUTIONAL NETWORKS FOR LARGE-SCALE IMAGE RECOGNITION ReNet: A Recurrent Neural Network Based Alternative to Convolutional Networks DELVING DEEPER INTO CONVOLUTIONAL NETWORKS FOR LEARNING VIDEO REPRESENTATIONS ReSeg: A Recurrent Neural Network-based Model for Semantic Segmentation Semantic Instance Segmentation with a Discriminative Loss Function Instance Segmentation and Tracking with Cosine Embeddings and Recurrent Hourglass Networks An intriguing failing of convolutional neural networks and the CoordConv solution Leaf segmentation in plant phenotyping: A collation study "
43,charlesCXK/PyTorch_Semantic_Segmentation,175,https://github.com/charlesCXK/PyTorch_Semantic_Segmentation,"Updated on Dec 20, 2019","PyTorch_Semantic_Segmentation Introduction Has finished FCN8s RefineNet (CVPR 2017) PSPNet (CVPR 2017) PointNet (CVPR 2017) RDFNet (ICCV 2017) 3DGNN (ICCV 2017) DeepLab V3 DeepLab V3+ (ECCV 2018) DenseASPP (CVPR 2018) FastFCN (Arxiv 2019)      README.md           PyTorch_Semantic_Segmentation Implement some models of semantic segmentation in PyTorch, easy to run. Introduction  This repo includes some networks for Semantic Segmentation implemented in pytorch 1.0.0 and python3. See each directory for more information. I only provide architecture of network here. Dataset and train/test files aren't available here, for I think  it can be added according to individual needs. The code file containing the network structure can be run directly with the set simulation data. ResNet101 used in this repo is the one which PSPNet used. The difference between this resnet and the original resnet is that the first 7*7 conv layer in the old version is replaced by three small-kernel convs. Pretrained model can be downloaded from here.  Has finished FCN8s  RefineNet (CVPR 2017)  PSPNet (CVPR 2017)  PointNet (CVPR 2017)  RDFNet (ICCV 2017)  3DGNN (ICCV 2017)  DeepLab V3  DeepLab V3+ (ECCV 2018)  DenseASPP (CVPR 2018)  FastFCN (Arxiv 2019) "
44,donnyyou/torch-segmentation,168,https://github.com/donnyyou/torch-segmentation,"Updated on Apr 10, 2020","MT-Segmentation Implemented Papers QuickStart with TorchCV Performances with MT-Segmentation Semantic Segmentation Commands with MT-Segmentation      README.md           MT-Segmentation @misc{mt-segmentation,     author = {Ansheng You, Zhenhua Chai},     title = {MT-Segmentation},     howpublished = {\url{http://git.sankuai.com/users/youansheng/repos/mt-segmentation}},     year = {2020} }           This repository provides source code for most deep learning based cv problems. We'll do our best to keep this repository up-to-date.  If you do find a problem about this repository, please raise an issue or submit a pull request. Implemented Papers  Semantic Segmentation  DeepLabV3: Rethinking Atrous Convolution for Semantic Image Segmentation PSPNet: Pyramid Scene Parsing Network DenseASPP: DenseASPP for Semantic Segmentation in Street Scenes Asymmetric Non-local Neural Networks for Semantic Segmentation    QuickStart with TorchCV Now only support Python3.x, pytorch 1.3. pip3 install -r requirements.txt cd lib/exts sh make.sh          Performances with MT-Segmentation All the performances showed below fully reimplemented the papers' results. Semantic Segmentation  Cityscapes (Single Scale Whole Image Test): Base LR 0.01, Crop Size 769     Model Backbone Train Test mIOU BS Iters Scripts     PSPNet 3x3-Res101 train val 78.20 8 4W PSPNet   DeepLabV3 3x3-Res101 train val 79.13 8 4W DeepLabV3     ADE20K (Single Scale Whole Image Test): Base LR 0.02, Crop Size 520     Model Backbone Train Test mIOU PixelACC BS Iters Scripts     PSPNet 3x3-Res50 train val 41.52 80.09 16 15W PSPNet   DeepLabv3 3x3-Res50 train val 42.16 80.36 16 15W DeepLabV3   PSPNet 3x3-Res101 train val 43.60 81.30 16 15W PSPNet   DeepLabv3 3x3-Res101 train val 44.13 81.42 16 15W DeepLabV3    Commands with MT-Segmentation Take PSPNet as an example. (""tag"" could be any string, include an empty one.)  Training  cd scripts/cityscapes/ bash run_fs_pspnet_cityscapes_seg.sh train tag           Resume Training  cd scripts/cityscapes/ bash run_fs_pspnet_cityscapes_seg.sh train tag           Validate  cd scripts/cityscapes/ bash run_fs_pspnet_cityscapes_seg.sh val tag           Testing:  cd scripts/cityscapes/ bash run_fs_pspnet_cityscapes_seg.sh test tag "
45,Kaixhin/FCN-semantic-segmentation,181,https://github.com/Kaixhin/FCN-semantic-segmentation,"Updated on Sep 10, 2018","FCN-semantic-segmentation Requirements Instructions References      README.md           FCN-semantic-segmentation Simple end-to-end semantic segmentation using fully convolutional networks [1]. Takes a pretrained 34-layer ResNet [2], removes the fully connected layers, and adds transposed convolution layers with skip connections from lower layers. Initialises upsampling convolutions with bilinear interpolation filters and zeros the final (classification) layer. Uses an independent cross-entropy loss per class. Trained with SGD with momentum, plus weight decay only on convolutional weights. Calculates and plots class-wise and mean intersection-over-union. Checkpoints the network every epoch. Note: This code does not achieve great results (achieves ~40 IoU fairly quickly, but converges there). Contributions to fix this are welcome! The goal of this repo is to provide strong, simple and efficient baselines for semantic segmentation using the FCN method, so this shouldn't be restricted to using ResNet 34 etc. Requirements  CUDA PyTorch matplotlib Cityscapes Dataset  Instructions  Install all of the required software. To feasibly run the training, CUDA is needed. The crop size and batch size can be tailored to your GPU memory (the default crop and batch sizes use ~10GB of GPU RAM). Register on the Cityscapes website to access the dataset. Download and extract the training/validation RGB data (leftImg8bit_trainvaltest) and ground truth data (gtFine_trainvaltest). Run python main.py <options>.  First a Dataset object is set up, returning the RGB inputs, one-hot targets (for independent classification) and label targets. During training, the images are randomly cropped and horizontally flipped. Testing calculates IoU scores and produces a subset of coloured predictions that match the coloured ground truth. References [1] Fully convolutional networks for semantic segmentation [2] Deep Residual Learning for Image Recognition "
46,MendelXu/ANN,287,https://github.com/MendelXu/ANN,"Updated on Oct 29, 2019","ANN citation Table of contents Introduction Usage Results Acknowledgement      README.md           ANN This repository is for Asymmetric Non-local Neural Networks for Semantic Segmentation (to appear in ICCV 2019), by Zhen Zhu, Mengde Xu, Song Bai, Tengteng Huang and Xiang Bai. The source code is in preparing. We will release as soon as possible. citation If you find our paper useful in your research, please consider citing: @inproceedings{annn,   author    = {Zhen Zhu and                Mengde Xu and                Song Bai and                Tengteng Huang and                Xiang Bai},   title     = {Asymmetric Non-local Neural Networks for Semantic Segmentation},   booktitle={International Conference on Computer Vision},   year      = {2019},   url       = {http://arxiv.org/abs/1908.07678}, }           Table of contents  Introduction Usage Results Acknowledgement  Introduction  Fig.1 Model Architecture In this work, we present Asymmetric Non-local Neural Network to semantic segmentation for acquiring long range dependency efficiently and effectively. The whole network is shown in Fig. 1. It can fuse features between different level under a sufficient consideration of inter long range dependencies  with AFNB and refine features in the same level involving the inner long range  dependencies with APNB. Usage   Preparation  Prepare running env:  python==3.6.8 gcc==5.4.0 cuda==9.2            Install required package  pip install -r requirements.txt cd ext bash make.sh           Download dataset and pre-process it with scripts in datasets/seg/preprocess. Take Cityscapes for example.  Download the leftImg8bit.zip and gtFine.zip from cityscapes-dataset.com and extract those.  cd datasets/seg/preprocess/cityscapes            Modify the variable ORI_ROOT_DIR in cityscapes_seg_generator.sh to where you save the extracted dataset. run the preprocessing program.  bash cityscapes_seg_generator.sh            Add dataset path where you save the preprocessed dataset to data.conf or directly modify the data path in scripts. Download model pretrained on imagenet and save it in ./pretrained_models.    Train Take Cityscapes for example. For training only on train set: # you can change 'tag' to anything you want bash scripts/seg/cityscapes/run_fs_annn_cityscapes_seg.sh train tag           For training on both train set and validation set: bash scripts/seg/cityscapes/run_fs_annn_cityscapes_seg_test.sh train tag             Validation Download trained model to checkpoints/seg/cityscapes/fs_annn_cityscapes_segohem_latest.pth and test on validation set with single scale. bash scripts/seg/cityscapes/run_fs_annn_cityscapes_seg.sh val ohem           The final output should be:  classes          IoU      nIoU -------------------------------- road          : 0.984      nan sidewalk      : 0.863      nan building      : 0.930      nan wall          : 0.521      nan fence         : 0.629      nan pole          : 0.667      nan traffic light : 0.737      nan traffic sign  : 0.816      nan vegetation    : 0.927      nan terrain       : 0.648      nan sky           : 0.948      nan person        : 0.838    0.000 rider         : 0.663    0.000 car           : 0.958    0.000 truck         : 0.830    0.000 bus           : 0.894    0.000 train         : 0.858    0.000 motorcycle    : 0.669    0.000 bicycle       : 0.796    0.000 -------------------------------- Score Average : 0.799    0.000 --------------------------------   categories       IoU      nIoU -------------------------------- flat          : 0.987      nan construction  : 0.936      nan object        : 0.735      nan nature        : 0.929      nan sky           : 0.948      nan human         : 0.849    0.000 vehicle       : 0.945    0.000 -------------------------------- Score Average : 0.904    0.000 --------------------------------             Test on test set Download trained model to checkpoints/seg/cityscapes/fs_annn_cityscapes_segohem_latest.pth and test on test set with multiple scales. bash scripts/seg/cityscapes/run_fs_annn_cityscapes_seg_test.sh test ohem cd results/seg/cityscapes/fs_annn_cityscapes_segohem/test/label zip ../res.zip * # submit the result on cityscapes official web page.             Efficiency Statistics For details, please refer to ./efficiency_statics/readme.md.   Results Tab.1 Comparisons on the test set of Cityscapes with the state-of-the-art methods.(multi scale testing)    Method Backbone mIOU     DeepLab-V2 ResNet-101 70.4   RefineNet ResNet-101 73.6   GCN ResNet-101 76.9   DUC ResNet-101 77.6   SAC ResNet-101 78.1   ResNet-38 ResNet-101 78.4   PSPNet ResNet-101 78.4   BiSeNet ResNet-101 78.9   AAF ResNet-101 79.1   DFN ResNet-101 79.3   PSANet ResNet-101 79.3   DenseASPP DenseNet-101 80.1   Ours ResNet-101 81.3    Tab.2 Comparisons on the validation set of PASCAL Context with the state-of-the-art methods.(multi scale testing)    Method Backbone mIOU     FCN-8s - 37.8   Piecewise - 43.3   DeepLab-v2 ResNet-101 45.7   RefineNet ResNet-152 47.3   PSPNet Resnet-101 47.8   CCL ResNet-101 51.6   EncNet ResNet-101 51.7   Ours ResNet-101 52.8    Tab.3 Comparisons on the validation set of ADE20k with the state-of-the-art methods.(multi scale testing)    Method Backbone mIOU     RefineNet ResNet-152 40.70   UperNet ResNet-101 42.65   DSSPN ResNet-101 43.68   PSANet ResNet-101 43.77   SAC ResNet-101 44.30   EncNet ResNet-101 44.65   PSPNet ResNet-101 43.29   PSPNet ResNet-269 44.94   Ours ResNet-101 45.24    Acknowledgement We genuinely thank Ansheng You for his kind help and suggestions throughout our work. We also recommend others to implement computer vision algorithm with his framework torchcv. "
47,nyoki-mtl/pytorch-segmentation,340,https://github.com/nyoki-mtl/pytorch-segmentation,"Updated on Mar 6, 2020","PytorchSegmentation Pretrained model DeepLabV3+(Xception65+ASPP) MobilenetV2 How to train Dataset Directory tree Environments Reference Encoder Decoder SCSE IBN OC PSP ASPP      README.md           PytorchSegmentation This repository implements general network for semantic segmentation. You can train various networks like DeepLabV3+, PSPNet, UNet, etc., just by writing the config file.  Pretrained model You can run pretrained model converted from official tensorflow model. DeepLabV3+(Xception65+ASPP) $ cd tf_model $ wget http://download.tensorflow.org/models/deeplabv3_cityscapes_train_2018_02_06.tar.gz $ tar -xvf deeplabv3_cityscapes_train_2018_02_06.tar.gz $ cd ../src $ python -m converter.convert_xception65 ../tf_model/deeplabv3_cityscapes_train/model.ckpt 19 ../model/cityscapes_deeplab_v3_plus/model.pth           Then you can test the performance of trained network. $ python eval_cityscapes.py --tta           mIoU of cityscapes $ pip install cityscapesScripts $ export CITYSCAPES_RESULTS=../output/cityscapes_val/cityscapes_deeplab_v3_plus_tta $ export CITYSCAPES_DATASET=../data/cityscapes $ csEvalPixelLevelSemanticLabeling           classes          IoU      nIoU -------------------------------- road          : 0.984      nan sidewalk      : 0.866      nan building      : 0.931      nan wall          : 0.626      nan fence         : 0.635      nan pole          : 0.668      nan traffic light : 0.698      nan traffic sign  : 0.800      nan vegetation    : 0.929      nan terrain       : 0.651      nan sky           : 0.954      nan person        : 0.832    0.645 rider         : 0.644    0.452 car           : 0.956    0.887 truck         : 0.869    0.420 bus           : 0.906    0.657 train         : 0.834    0.555 motorcycle    : 0.674    0.404 bicycle       : 0.783    0.605 -------------------------------- Score Average : 0.802    0.578 --------------------------------   categories       IoU      nIoU -------------------------------- flat          : 0.988      nan construction  : 0.937      nan object        : 0.729      nan nature        : 0.931      nan sky           : 0.954      nan human         : 0.842    0.667 vehicle       : 0.944    0.859 -------------------------------- Score Average : 0.904    0.763 --------------------------------           MobilenetV2 $ cd tf_model $ wget http://download.tensorflow.org/models/deeplabv3_mnv2_cityscapes_train_2018_02_05.tar.gz $ tar -xvf deeplabv3_mnv2_cityscapes_train_2018_02_05.tar.gz $ cd ../src $ python -m converter.convert_mobilenetv2 ../tf_model/deeplabv3_mnv2_cityscapes_train/model.ckpt 19 ../model/cityscapes_mobilnetv2/model.pth           How to train In order to train model, you have only to setup config file. For example, write config file as below and save it as config/pascal_unet_res18_scse.yaml. Net:   enc_type: 'resnet18'   dec_type: 'unet_scse'   num_filters: 8   pretrained: True Data:   dataset: 'pascal'   target_size: (512, 512) Train:   max_epoch: 20   batch_size: 2   fp16: True   resume: False   pretrained_path: Loss:   loss_type: 'Lovasz'   ignore_index: 255 Optimizer:   mode: 'adam'   base_lr: 0.001   t_max: 10          Then you can train this model by: $ python train.py ../config/pascal_unet_res18_scse.yaml           Dataset  Cityscapes Pascal Voc  augmentation  http://home.bharathh.info/pubs/codes/SBD/download.html https://github.com/TheLegendAli/DeepLab-Context/issues/10      Directory tree . ├── config ├── data │   ├── cityscapes │   │   ├── gtFine │   │   └── leftImg8bit │   └── pascal_voc_2012 │        └── VOCdevkit │            └── VOC2012 │                ├── JPEGImages │                ├── SegmentationClass │                └── SegmentationClassAug ├── logs ├── model └── src     ├── dataset     ├── logger     ├── losses     │   ├── binary     │   └── multi     ├── models     └── utils           Environments  OS: Ubuntu18.04 python: 3.7.0 pytorch: 1.0.0 pretrainedmodels: 0.7.4 albumentations: 0.1.8  if you want to train models in fp16  NVIDIA/apex: 0.1  Reference Encoder  https://arxiv.org/abs/1505.04597 https://github.com/tugstugi/pytorch-saltnet  Decoder SCSE  https://arxiv.org/abs/1803.02579  IBN  https://arxiv.org/abs/1807.09441 https://github.com/XingangPan/IBN-Net https://github.com/SeuTao/Kaggle_TGS2018_4th_solution  OC  https://arxiv.org/abs/1809.00916 https://github.com/PkuRainBow/OCNet  PSP  https://arxiv.org/abs/1612.01105  ASPP  https://arxiv.org/abs/1802.02611 "
48,tucan9389/SemanticSegmentation-CoreML,210,https://github.com/tucan9389/SemanticSegmentation-CoreML,"Updated on Mar 27, 2021","SemanticSegmentation-CoreML How it works Requirements Models Download Matadata Inference Time − DeepLabV3 Inference Time − FaceParsing Labels − DeepLabV3 Labels − FaceParsing See also      README.md     SemanticSegmentation-CoreML    This project is Object Segmentation on iOS with Core ML.If you are interested in iOS + Machine Learning, visit here you can see various DEMOs.    DeepLabV3-DEMO1 FaceParsing-DEMO DeepLabV3-DEMO-2 DeepLabV3-DEMO-3            How it works  When use Metal   Requirements  Xcode 10.2+ iOS 12.0+ Swift 5  Models Download Download model from apple's model page. Matadata    Name Input Output Size iOS version+ Download     DeepLabV3 Image (Color 513 × 513) MultiArray (Int32 513 × 513) 8.6 MB iOS 12.0+ link   DeepLabV3FP16 Image (Color 513 × 513) MultiArray (Int32 513 × 513) 4.3 MB iOS 12.0+ link   DeepLabV3Int8LUT Image (Color 513 × 513) MultiArray (Int32 513 × 513) 2.3 MB iOS 12.0+ link   FaceParsing Image (Color 512 × 512) MultiArray (Int32) 512 × 512 52.7 MB iOS 14.0+ link    Inference Time − DeepLabV3    Device Inference Time Total Time (GPU) Total Time (CPU)     iPhone 12 Pro 29 ms 29 ms 240 ms   iPhone 12 Pro Max      iPhone 12 30 ms 31 ms 253 ms   iPhone 12 Mini 29 ms 30 ms 226 ms   iPhone 11 Pro 39 ms 40 ms 290 ms   iPhone 11 Pro Max 35 ms 36 ms 280 ms   iPhone 11      iPhone SE (2nd)      iPhone XS Max      iPhone XS 54 ms 55 ms 327 ms   iPhone XR 133 ms  402 ms   iPhone X 137 ms 143 ms 376 ms   iPhone 8+ 140 ms 146 ms 420 ms   iPhone 8 189 ms  529 ms   iPhone 7+ 240 ms  667 ms   iPhone 7 192 ms 208 ms 528 ms   iPhone 6S + 309 ms  1015 ms    : need to measure Inference Time − FaceParsing    Device Inference Time Total Time (GPU) Total Time (CPU)     iPhone 12 Pro      iPhone 11 Pro 37 ms 37 ms     Labels − DeepLabV3 # total 21 [""background"", ""aeroplane"", ""bicycle"", ""bird"", ""boat"", ""bottle"", ""bus"", ""car"", ""cat"", ""chair"", ""cow"", ""diningtable"", ""dog"", ""horse"", ""motorbike"", ""person"", ""pottedplant"", ""sheep"", ""sofa"", ""train"", ""tv""]           Labels − FaceParsing # total 19 [""background"", ""skin"", ""l_brow"", ""r_brow"", ""l_eye"", ""r_eye"", ""eye_g"", ""l_ear"", ""r_ear"", ""ear_r"", ""nose"", ""mouth"", ""u_lip"", ""l_lip"", ""neck"", ""neck_l"", ""cloth"", ""hair"", ""hat""]           See also  motlabs/iOS-Proejcts-with-ML-Models : The challenge using machine learning model created from tensorflow on iOS DeepLab on TensorFlow : The repository providing DeepLabV3 model FaceParsing: The repository providing the FaceParsing pytorch model "
49,amueller/segmentation,50,https://github.com/amueller/segmentation,"Updated on Nov 8, 2013",
50,kevinlee9/Semantic-Segmentation,163,https://github.com/kevinlee9/Semantic-Segmentation,"Updated on Aug 15, 2021","Weakly-Segmentation Top Work By Dataset PASCAL VOC2012 By Years ICCV2019 CVPR2019 Resources Tutorial Implementation Related Tasks Few-shot segmentation Weakly-supervised Instance Segmentation Weakly-supervised Panoptic Segmentation Reading List Under Review Published context graph bbox-level webly Saliency localization spp affinity region network regularizer evaluation measure architecture generative adversarial scene understanding other useful application Others priors diffusion analysis post processing common methods      README.md     Weakly-Segmentation List of useful codes and papers for weakly supervised Semantic/Instance/Panoptic/Few Shot Segmentation  Weakly-Segmentation  Top Work  By Dataset  PASCAL VOC2012   By Years  ICCV2019     Resources  Tutorial   Implementation   Related Tasks  Few-shot segmentation Weakly-supervised Instance Segmentation Weakly-supervised Panoptic Segmentation   Reading List  Under Review Published  context graph bbox-level webly Saliency localization spp affinity region network regularizer evaluation measure architecture generative adversarial scene understanding other useful application   Others  priors diffusion analysis post processing common methods      Top Work By Dataset PASCAL VOC2012    method val test notes     DSRGCVPR2018 61.4 63.2 deep seeded region growing, resnet-lfov|vgg-aspp   psaCVPR2018 61.7 63.7 pixel affinity network, resnet38   MDCCVPR2018 60.4 60.8 multi-dilated convolution, vgg-lfov   MCOFCVPR2018 60.3 61.2 iterative, RegionNet(sppx), resnet-lfov   GAINCVPR2018 55.3 56.8    DCSPBMVC2017 58.6 59.2 adversarial for saliency, and generate cues by cam+saliency(harmonic mean)   GuidedSegCVPR2017 55.7 56.7 saliency, TBD   BDSSWCVPR2018 63.0 63.9 webly, filter+enhance   WegSegarxiv 63.1 63.3 webly(pure), Noise filter module   SeeNetNIPS2018 63.1 62.8 based on DCSP   GraphECCV2018 63.6 64.5 graph partition   GraphECCV2018 64.5 65.6 use simple ImageNet dataset additionally   CIANCVPR2019 64.1 64.7 cross image affinity network   FickleNetCVPR2019 64.9 65.3 use dropout (a generalization of dilated convolution)    By Years ICCV2019 Joint Learning of Saliency Detection and Weakly Supervised Semantic Segmentation Self-Supervised Difference Detection for Weakly-Supervised Semantic Segmentation CVPR2019 FickleNet: Weakly and Semi-supervised Semantic Image Segmentation using Stochastic Inference Resources see this for more weakly lists and resources. see this for more semantic/instance/panoptic/video segmentation lists and resources. see this for more implementations a good architecture summary paper:Learning a Discriminative Feature Network for Semantic Segmentation Tutorial  Unsupervised Visual Learning Tutorial. CVPR 2018 [part 1] [part 2] Weakly Supervised Learning for Computer Vision. CVPR 2018 [web] [part 1] [part 2]  Implementation pytorch-segmentation-detection a library for dense inference and training of Convolutional Neural Networks, 68.0% rdn Dilated Residual Networks, 75.6%, may be the best available semantic segmentation in PyTorch? Detectron.pytorch A pytorch implementation of Detectron. Both training from scratch and inferring directly from pretrained Detectron weights are available. only for coco now AdvSemiSeg Adversarial Learning for Semi-supervised Semantic Segmentation.  heavily borrowed from a pytorch DeepLab implementation (Link) PyTorch-ENet PyTorch implementation of ENet tensorflow-deeplab-resnet Tensorflow implementation of deeplab-resnet(deeplabv2, resnet101-based): complete and detailed tensorflow-deeplab-lfov Tensorflow implementation of deeplab-LargeFOV(deeplabv2, vgg16-based): complete and detailed resnet38  Wider or Deeper: Revisiting the ResNet Model for Visual Recognition: implemented using MXNET pytorch_deeplab_large_fov: deeplab v1 pytorch-deeplab-resnetDeepLab resnet v2 model in pytorch DeepLab-ResNet-Pytorch Deeplab v3 model in pytorch, BDWSS Bootstrapping the Performance of Webly Supervised Semantic Segmentation psa Learning Pixel-level Semantic Affinity with Image-level Supervision for Weakly Supervised Semantic Segmentation DSRG: Caffe, CAM and DRFI provided SEC  original: Caffe BDSSW: MXNET SEC-tensorflow: tensorflow  Related Tasks Few-shot segmentation   One-shot learning for semantic segmentation, BMVC2017  Conditional networks for few-shot semantic segmentation, ICLR2018 Workshop  Few-Shot Segmentation Propagation with Guided Networks, preprint  Few-Shot Semantic Segmentation with Prototype Learning, BMVC2018  Attention-based Multi-Context Guiding for Few-Shot Semantic Segmentation, AAAI2019  CANet: Class-Agnostic Segmentation Networks with Iterative Refinement and Attentive Few-Shot Learning, CVPR2019  One-Shot Segmentation in Clutter, ICML 2018  Weakly-supervised Instance Segmentation   Weakly Supervised Instance Segmentation using Class Peak Response, CVPR2018  Weakly Supervised Learning of Instance Segmentation with Inter-pixel Relations, CVPR2019  Object Counting and Instance Segmentation with Image-level Supervision, CVPR2019  Cyclic Guidance for Weakly Supervised Joint Detection and Segmentation, CVPR2019  Where are the Masks: Instance Segmentation with Image-level Supervision, BMVC2019  Label-PEnet: Sequential Label Propagation and Enhancement Networks for Weakly Supervised Instance Segmentation, ICCV2019  Weakly-supervised Panoptic Segmentation   Weakly- and Semi-Supervised Panoptic Segmentation, ECCV2018  Reading List Under Review   Gated CRF Loss for Weakly Supervised Semantic Image Segmentation  Closed-Loop Adaptation for Weakly-Supervised Semantic Segmentation  Harvesting Information from Captions for Weakly Supervised Semantic Segmentation  Consistency regularization and CutMix for semi-supervised semantic segmentation  Zero-shot Semantic Segmentation  Self-supervised Scale Equivariant Network for Weakly Supervised Semantic Segmentation, propose an scale equivariant regularization.  Published context   Context Encoding for Semantic Segmentation: CVPR2018. use TEN  The Role of Context for Object Detection and Semantic Segmentation in the Wild: CVPR2014  Objects as Context for Detecting Their Semantic Parts: CVPR2018  Exploring context with deep structured models for semantic segmentation: TPAMI2017  dilated convolution  Deep TEN: Texture encoding network !!: CVPR2017. A global context vector, pooled from all spatial positions, can be concatenated to local features   Refinenet: Multi-path refinement networks for high-resolution semantic segmentation: CVPR2017. local features across different scales can be fused to encode global context  Non-local neural networks: CVPR2018. a densely connected graph with pairwise edges between all pixels  graph   Associating Inter-Image Salient Instances for Weakly Supervised Semantic Segmentation: ECCV2018  bbox-level Box-driven Class-wise Region Masking and Filling Rate Guided Loss for Weakly Supervised Semantic Segmentation, CVPR2019 webly   Weakly Supervised Semantic Segmentation Based on Web Image Cosegmentation: BMVC2017, training model using masks of web images which are generated by cosegmentation  Webly Supervised Semantic Segmentation: CVPR2017  Weakly Supervised Semantic Segmentation using Web-Crawled Videos: CVPR2017, learns a class-agnostic decoder(attention map -> binary mask), pseudo masks are generated from video frames by solving a graph-based optimization problem.  Bootstrapping the Performance of Webly Supervised Semantic Segmentation: target + web domain, target model filters web images, refine mask by combine target and web masks.  Learning from Weak and Noisy Labels for Semantic Segmentation: TPAMI2017  WebSeg: Learning Semantic Segmentation from Web Searches: arxiv, directly learning from keywork retrievaled web images. using saliency and region(MCG with edge)  STC: A Simple to Complex Framework for Weakly-supervised Semantic Segmentation: TPAMI 2017, Initial, Enhanced, Powerful three DCNN model. inital mask(generated by saliency and label using simple images) -> initial model -> enhanced mask(generated using simple images) -> Enhanced model -> powerful mask(generated using complex images) -> powerful model  saliency can not handle complex images, so BMVC2017 uses coseg instead    Saliency   Exploiting Saliency for Object Segmentation from Image Level Labels: CVPR2017  Discovering Class-Specific Pixels for Weakly-Supervised Semantic Segmentation: BMVC2017  combine saliency(off-shelf) and CAM to get cues, use harmonic mean function adapt CAM from head of Segmentation Network use erasing to get multiple objects' saliency    localization   Adversarial Complementary Learning for Weakly Supervised Object Localization, CVPR2018. two branchs, remove high activations from feature map. code  Tell me where to look: Guided Attention Inference Network, CVPR2018. origin image soft erasing(CAM after sigmoid as attention) -> end2end training, force erased images have zero activation  Self-Erasing Network for Integral Object Attention， NIPS2018: prohibit attentions from spreading to unexpected background regions.  cam -> tenary mask(attention, background, potential) self erasing only in attention + potential region(sign flip in background region instead of setting to 0 simply) self produced psedo label for background region(difference to SPG: 1.psedo label for background and attention 2.supervise low layer)    Self-produced Guidance for Weakly-supervised Object localization, ECCV2018:  self supervised use top down framework, for single label classification prob. add pixel-wise supervision when only have image level label B1, B2 sharing bottom guide top inversely(B1+B2 -> C)    spp   Superpixel convolutional networks using bilateral inceptions  Learning Superpixels with Segmentation-Aware Affinity Loss: good intro for superpixel algs.  affinity   Learning Pixel-level Semantic Affinity with Image-level Supervision for Weakly Supervised Semantic Segmentation: image-level, semantic affinity, learn a network to predict affinity  Adaptive Affinity Field for Semantic Segmentation: ECCV2018, semantic affinity. add a pairwise term in seg loss(similarity metric: KL divergence), use an adversarial method to determine optimal neighborhood size  region   Region-Based Convolutional Networks for Accurate Object Detection and Segmentation  Simultaneous Detection and Segmentation, 2014  Feedforward semantic segmentation with zoom-out features: 2015  network   Learned Shape-Tailored Descriptors for Segmentation  Normalized Cut Loss for Weakly-Supervised CNN Segmentation  Fully Convolutional Adaptation Networks for Semantic Segmentation  Learning to Adapt Structured Output Space for Semantic Segmentation  Semantic Segmentation with Reverse Attention: BMVC2017, equally responses of multi classes(confusion in boudary region). add reverse branch, predict the probability of pixel that doesn't belong to the corresponding class. and use attention to combine origin and reverse branch  Deep Clustering for Unsupervised Learning of Visual Features, ECCV2018. use assignments of knn as supervision to update weights of network  DEL: Deep Embedding Learning for Efficient Image Segmentation, IJCAI 2018. use spp embedding as init probs to do image segmentation  Learning a Discriminative Feature Network for Semantic Segmentation, CVPR2018, Smoother network: multi-scale+global context(FPN with channel atention), Broder Network: focal loss for boundary. code?  Convolutional Simplex Projection Network for Weakly Supervised Semantic Segmentation: BMVC 2018  Decoders Matter for Semantic Segmentation: Data-Dependent Decoding Enables Flexible Feature Aggregation: CVPR2019  regularizer   Normalized Cut Loss for Weakly-Supervised CNN Segmentation  Regularized Losses for Weakly-supervised CNN Segmentation  evaluation measure   Optimizing Intersection-Over-Union in Deep Neural Networks for Image Segmentation  The Lovasz-Softmax loss: A tractable surrogate for the optimization of the intersection-over-union measure in neural networks  What is a good evaluation measure for semantic segmentation?  architecture   The Devil is in the Decoders, BMVC2017  Dilated Residual Networks, CVPR2017. Dilated structure design for classification and localization.  Understanding Convolution for Semantic Segmentation, WACV2018. hybrid dilated convolution(2-2-2 -> 1-2-3)  Smoothed Dilated Convolutions for Improved Dense Prediction, KDD2018. separable and share conv(for smoothing) + dilated conv  Deeplab v1, v2, v3, v3+  Learning Fully Dense Neural Networks for Image Semantic Segmentation, AAAI2019  generative adversarial   Deep dual learning for semantic image segmentation:CVPR2017, image translation  Semantic Segmentation using Adversarial Networks, NIPS2016 workshop  add gan loss branch, Segnet as generator, D: GT mask or predicted mask    Adversarial Learning for Semi-Supervised Semantic Segmentation: BMVC2018  semi supervised: SegNet as G, FCN-type D(discriminate each location), use output of D as psedo label for unlabeled data    Semi and weakly Supervised Semantic Segmentation Using Generative Adversarial Network: ICCV2017, use SegNet as D, treat fake as new class  weakly, use conditionalGan, pixel-level, image-level, generated data are included in loss. performance boosts less when increasing fully data    generative adversarial learning towards Fast weakly supervised detection: CVPR2018  Adaptive Affinity Field for Semantic Segmentation: ECCV2018, semantic affinity. add a pairwise term in seg loss(similarity metric: KL divergence), use an adversarial method to determine optimal neighborhood size  scene understanding   ScanComplete: Large-Scale Scene Completion and Semantic Segmentation for 3D Scans  SeGAN: Segmenting and Generating the Invisible  other useful   Learning to Segment Every Thing: semi-supervised, weight transfer function (from bbox parameters to mask parameters)  Simple Does It: Weakly Supervised Instance and Semantic Segmentation: bbox-level, many methods, using graphcut, HED, MCG  Multi-Evidence Filtering and Fusion for Multi-Label Classification, Object Detection and Semantic Segmentation Based on Weakly Supervised Learning: tricky, curriculum learning: image level -> instance level -> pixel level  Combining Bottom-Up, Top-Down, and Smoothness Cues for Weakly Supervised Image Segmentation: CVPR2017  Improving Weakly-Supervised Object Localization By Micro-Annotation: BMVC2016, object classes always co-occur with same background elements(boat, train). propose a new annotation method. add human annotations to improve localization results of CAM, annotating based on clusters of dense features. each class uses a spectral clustering.(CAM has problem)  Co-attention CNNs for Unsupervised Object Co-segmentation: IJCAI 2018  Coarse-to-fine Image Co-segmentation with Intra and Inter Rank Constraints, IJCAI2018  Annotation-Free and One-Shot Learning for Instance Segmentation of Homogeneous Object Clusters, IJCAI2018  Image-level to Pixel-wise Labeling: From Theory to Practice: fully, analysis the effect of image labels on seg results. add a generator(recover original image). image label(binary, use a threshold small than 0.5, eg:0.25), IJCAI2018  application   SeGAN: Segmenting and Generating the Invisible: CVPR2018, generate occluded parts  Learning Hierarchical Semantic Image Manipulation through Structured Representations: NIPS2018, manipulate image on object-level by modify bbox  Others priors  Superpixels: An Evaluation of the State-of-the-Art link Learning Superpixels with Segmentation-Aware Affinity Losslink Superpixel based Continuous Conditional Random Field Neural Network for Semantic Segmentation link  diffusion Learning random-walk label propagation for weakly-supervised semantic segmentation: scribble Convolutional Random Walk Networks for Semantic Image Segmetation: fully, affinity branch(low level) Soft Proposal Networks for Weakly Supervised Object Localization: attention, semantic affinity Learning Pixel-level Semantic Affinity with Image-level Supervision for Weakly Supervised Semantic Segmentation: image-level, semantic affinity analysis image level to pixel wise labeling: from theory to practice: IJCAI 2018 analysis the effectiveness of class-level labels for segmentation(GT, predicted) Attention based Deep Multiple Instance Learning: ICML 2018. CAM from MIL perspective view post processing listed in : Co-attention CNNs for Unsupervised Object Co-segmentation  Otsu’s method GrabCut CRF  common methods  refine segmentation results using image-level labels multi-label classification branch(BDWSS) generative branch(to original image) crf "
51,nv-tlabs/GSCNN,788,https://github.com/nv-tlabs/GSCNN,"Updated on May 16, 2020","GSCNN Gated-SCNN: Gated Shape CNNs for Semantic Segmentation License Usage Clone this repo Python requirements Download pretrained models Download inferred images Evaluation (Cityscapes) Training      README.md           GSCNN This is the official code for: Gated-SCNN: Gated Shape CNNs for Semantic Segmentation Towaki Takikawa, David Acuna, Varun Jampani, Sanja Fidler ICCV 2019 [Paper]  [Project Page]  Based on based on https://github.com/NVIDIA/semantic-segmentation. License Copyright (C) 2019 NVIDIA Corporation. Towaki Takikawa, David Acuna, Varun Jampani, Sanja Fidler All rights reserved. Licensed under the CC BY-NC-SA 4.0 license (https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode).  Permission to use, copy, modify, and distribute this software and its documentation for any non-commercial purpose is hereby granted without fee, provided that the above copyright notice appear in all copies and that both that copyright notice and this permission notice appear in supporting documentation, and that the name of the author not be used in advertising or publicity pertaining to distribution of the software without specific, written prior permission.  THE AUTHOR DISCLAIMS ALL WARRANTIES WITH REGARD TO THIS SOFTWARE, INCLUDING ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR ANY PARTICULAR PURPOSE. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY SPECIAL, INDIRECT OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE. ~           Usage Clone this repo git clone https://github.com/nv-tlabs/GSCNN cd GSCNN          Python requirements Currently, the code supports Python 3  numpy PyTorch (>=1.1.0) torchvision scipy scikit-image tensorboardX tqdm torch-encoding opencv PyYAML  Download pretrained models Download the pretrained model from the Google Drive Folder, and save it in 'checkpoints/' Download inferred images Download (if needed) the inferred images from the Google Drive Folder Evaluation (Cityscapes) python train.py --evaluate --snapshot checkpoints/best_cityscapes_checkpoint.pth          Training A note on training- we train on 8 NVIDIA GPUs, and as such, training will be an issue with WiderResNet38 if you try to train on a single GPU. If you use this code, please cite: @article{takikawa2019gated,   title={Gated-SCNN: Gated Shape CNNs for Semantic Segmentation},   author={Takikawa, Towaki and Acuna, David and Jampani, Varun and Fidler, Sanja},   journal={ICCV},   year={2019} } "
52,oandrienko/fast-semantic-segmentation,207,https://github.com/oandrienko/fast-semantic-segmentation,"Updated on Nov 5, 2019","Real-Time Semantic Segmentation in TensorFlow Release information October 14, 2018 September 22, 2018 August 12, 2018 Documentation Model Depot Inference Tutorials Overview Training ICNet from Classification Weights ICNet Network Compression PSPNet Baseline Implementation Maintainers Related Work Thanks      README.md           Real-Time Semantic Segmentation in TensorFlow Perform pixel-wise semantic segmentation on high-resolution images in real-time with Image Cascade Network (ICNet), the highly optimized version of the state-of-the-art Pyramid Scene Parsing Network (PSPNet). This project implements ICNet and PSPNet50 in Tensorflow with training support for Cityscapes.  Download pre-trained ICNet and PSPNet50 models here     Deploy ICNet and preform inference at over 30fps on NVIDIA Titan Xp.  This implementation is based off of the original ICNet paper proposed by Hengshuang Zhao titled ICNet for Real-Time Semantic Segmentation on High-Resolution Images. Some ideas were also taken from their previous PSPNet paper, Pyramid Scene Parsing Network. The network compression implemented is based on the paper Pruning Filters for Efficient ConvNets. Release information October 14, 2018 An ICNet model trained in August, 2018 has been released as a pre-trained model in the Model Zoo. All the models were trained without coarse labels and are evaluated on the validation set. September 22, 2018 The baseline PSPNet50 pre-trained model files have been released publically in the Model Zoo. The accuracy of the model surpases that referenced in the ICNet paper. August 12, 2018 Initial release. Project includes scripts for training ICNet, evaluating ICNet and compressing ICNet from ResNet50 weights. Also includes scripts for training PSPNet and evaluating PSPNet as a baseline. Documentation  Installation: Setting up the project Dataset Format: Creating TFRecord files for training and evaluation Configs and Finetune Training: Creating your own configuration files for training and evaluation PSPNet50: Walkthrough for Training PSPNet50 baseline ICNet: Walkthrough for Training ICNet with compression Model Zoo: Links to pre-trained checkpoints  Model Depot Inference Tutorials  PSPNet50 Inference Notebook ICNet Inference Notebook  Overview     ICNet model in Tensorboard.  Training ICNet from Classification Weights This project has implemented the ICNet training process, allowing you to train your own model directly from ResNet50 weights as is done in the original work. Other available implementations simply convert the Caffe model to Tensorflow, only allowing for fine-tuning from weights trained on Cityscapes. By training ICNet on weights initialized from ImageNet, you have more flexibility in the transfer learning process. Read more about setting up this process can be found here. For training ICNet, follow the guide here. ICNet Network Compression In order to achieve real-time speeds, ICNet uses a form of network compression called filter pruning. This drastically reduces the complexity of the model by removing filters from convolutional layers in the network. This project has also implemented this ICNet compression process directly in Tensorflow. The compression is working, however which ""compression scheme"" to use is still somewhat ambiguous when reading the original ICNet paper. This is still a work in progress. PSPNet Baseline Implementation In order to also reproduce the baselines used in the original ICNet paper, you will also find implementations and pre-trained models for PSPNet50. Since ICNet can be thought of as a modified PSPNet, it can be useful for comparison purposes. Informtion on training or using the baseline PSPNet50 model can be found here. Maintainers  Oles Andrienko, github: oandrienko  If you found the project, documentation and the provided pretrained models useful in your work, consider citing it with @misc{fastsemseg2018,   author={Andrienko, Oles},   title={Fast Semantic Segmentation},   howpublished={\url{https://github.com/oandrienko/fast-semantic-segmentation}},   year={2018} }           Related Work This project and some of the documentation was based on the Tensorflow Object Detection API. It was the initial inspiration for this project. The third_party directory of this project contains files from OpenAI's Gradient Checkpointing project by Tim Salimans and Yaroslav Bulatov. The helper modules found in third_party/model_deploy.py are from the Tensorflow Slim project. Finally, another open source ICNet implementation which converts the original Caffe network weights to Tensorflow was used as a reference. Find all these projects below:  Tensorflow Object Detection API Saving memory using gradient-checkpointing Tensorflow Slim ICNet converted from Caffe using Caffe-Tensorflow  Thanks  This project could not have happened without the advice (and GPU access) given by Professor Steven Waslander and Ali Harakeh from the Waterloo Autonomous Vehicles Lab (now the Toronto Robotics and Artificial Intelligence Lab). "
53,fabianbormann/Tensorflow-DeconvNet-Segmentation,224,https://github.com/fabianbormann/Tensorflow-DeconvNet-Segmentation,"Updated on Oct 10, 2018","README.md           Tensorflow implementation of Learning Deconvolution Network for Semantic Segmentation. Install Instructions   Works with tensorflow 1.11.0 and uses the Keras API so use pip to install tensorflow-gpu in the latest version   Run the following commands in your terminal   git clone https://github.com/fabianbormann/Tensorflow-DeconvNet-Segmentation.git cd Tensorflow-DeconvNet-Segmentation sudo pip3 install -r requirements.txt  python3          Python 3.5.2+ (default, Sep 22 2016, 12:18:14) [GCC 6.2.0 20160927] on linux Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> from DeconvNet import DeconvNet >>> deconvNet = DeconvNet() # will start collecting the VOC2012 data >>> deconvNet.train(epochs=20, steps_per_epoch=500, batch_size=64) >>> deconvNet.save() >>> prediction = deconvNet.predict(any_image)          Contributions welcome! "
54,mohit-sh/Adversarial-Semisupervised-Semantic-Segmentation,150,https://github.com/mohit-sh/Adversarial-Semisupervised-Semantic-Segmentation,"Updated on Aug 9, 2018","Adversarial Learning For Semi-Supervised Semantic Segmentation Introduction Scope Results Reproduced Updates Journey Baseline Model Adversarial Models      README.md           Adversarial Learning For Semi-Supervised Semantic Segmentation Introduction This is a submission for ICLR 2018 Reproducibility Challenge. The central theme of the work by the authors is to incorporate adversarial training for semantic-segmentation task which enables the segmentation-network to learn in a semi-supervised fashion on top of the traditional supervised learning. The authors claim significant improvement in the performance (measured in terms of mean IoU) of segmentation network after the supervised-training is extended with adversarial and semi-supervised training. Scope My plan is to reproduce the improvement in the performance of the segmentation network (Resnet-101) by including adversarial and semi-supervised learning scheme over the baseline supervised training and document my experience along the way. The authors have used two datasets, PASCAL VOC 12 (extended version) and Cityscapes, to demonstrate  the benefits of their proposed training scheme. I will focus on PASCAL VOC 12 dataset for this work. Specifically, the target for this work is to reproduce the following table from the paper.    Method       Data Amount  1/2       full     Baseline (Resnet-101) 69.8      73.6   Baseline + Adversarial Training 72.6        74.9   Baseline + Adversarial Training +  Semi-supervised Learning 73.2        NA    Results Reproduced Following table summarizes the results I have been able to reproduce for the full dataset. For the full dataset, only the performance of the adversarial training on top of baseline can be evaluated.    Method (Full Dataset) Original Challenge     Baseline (Resnet-101) 73.6 69.98   Baseline + Adversarial Training 74.9 70.97   Baseline + Adversarial Training +  Semi-supervised Learning NA NA    Following table summarizes the results I was able to reproduce for the semi-supervised training where half of the training data is reserved for semi-supervised training with unlabeled data.    Method (1/2 Dataset) Original Challenge     Baseline (Resnet-101) 69.8 67.84   Baseline + Adversarial Training 72.6 68.89   Baseline + Adversarial Training +  Semi-supervised Learning 73.2 69.05    Updates   18th Dec 2017 Finished Refactoring of the code, re-ran the experiments and achieved some improvement on the previous results by training the network longer.   8th Dec 2017: Semi-supervised Learning with 1/2 of training data treated as unlabeled degrades the performance compare to baseline (68.05 mIoU) and baseline + adversarial training (70.31 mIoU). It might be related to one of the comments of the reviewer that initial predictions by the discriminator might be noisy which renders semi-supervised training unstable during early epochs. The authors have made a comment that semi-supervised training is only applied after 5k iterations. I'll include the results with this addition soon.   4th Dec 2017: Started working on Semi-supervised training.   2nd Dec 2017: Adversarial Training based on base105 improves mIoU from 68.86 to 69.93.   30th Nov 2017: Managed to improve adversarial training performance. For base105, mIoU was improved from 68.86 to 69.33.   28th Nov 2017: Started experiments with Imagenet-pretrained Resnet-101 segmentation network as the baseline. Best mIoU achieved is 65.97. So, moving forward to unsupervised training with the base104 (best baseline model) and base105 (baseline with best adversarial training results).   27th Nov 2017: Finally managed to stabilize the GAN training. I couldn't reproduce any significant improvement over the baseline Segmentation Network. In fact, the best performing segmentation network (base104 with mIoU 69.78) was worse off with the adversarial training (mIoU dropped to 68.07). I have documented the details of the experiments performed for adversarial training. As GAN training is considered to be very sensitive towards weight initialization, I feel this is the right time to incorporate ImageNet pretrained network in the training.   20th Nov 2017: Started working on adding adversarial learning for base-104 segmentation network.   17th Nov 2017: Baseline model (base-104) achieved  mean IoU of 69.78 on the full dataset. The model is still significantly away from the target mIoU of 73.6. Only significant component missing from the implementation is using Resnet-101 pre-trained on Imagenet (I am currently using MS-COCO pretrained Network as the baseline). Other minor additions (normalization of the input (included in base-105), number of iterations to wait before lr decay, etc) will also be included.   Journey Baseline Model    Name Details mIoU     base-101 - No Normalization   - No gradient for batch norm  - Drop last batch if not complete  - Volatile = false for eval  - Poly Decay every 10 iterations  - learnable upsampling with transposed convolution 35.91   base102 Exactly like base-101, except  - no polynomial decay  - fixed bilinear upsampling layers 68.84   base103 Exactly like base-102, except - with polynomial decay(every 10 iter)) 68.88   base104 Exactly like base-103, except  -with poly decay (every iter) 69.78   base105 base-104, except  - with normalization of input to 0 mean and unit variance 68.86   base110 - ImageNet pretrained  - Normalization  - poly decay(eveyr iter)  same lr for all layers 65.97   base111 - Imagenent pretrained  - Normalization  - poly decay (every iter)  - 10x lr for classification module 65.67    Adversarial Models    Name Details miou     adv101 - base105 as G  - Optim(D): SGD lr 0.0001, momentum=0.5,decay= 0.0001 68.96   adv102 - base105  - 0.25 label smoothing for real labels in D  - Optim(D) SGD lr 0.0001, momentum=0.5,decay= 0.0001 67.14   adv103 - base105  - 0.25 label smoothing for real labels in D  - Optim(D) ADAM 68.07   adv104 - base104  - 0.25 label smoothing for real labels in D  - Optim(D) SGD lr 0.0001, momentum=0.5,decay= 0.0001 63.37   adv105 base104 as G  - everything else like adv103 Very poor (didn't finish training)   adv105-cuda - base105  - 0.25 label smoothing for real labels in D  - Optim(D) SGD lr 0.0001, momentum=0.5,decay= 0.0001  - batch size 21 Very poor (didn't finish training)   adv106 - base104  - optim(D) ADAM  - batch_size = 21 61.50   adv201 - base 105  - label smoothing 0.25  - Adam 69.33   adv202 - base105  - label smoothing 0.1  - d_optim Adam 69.93   adv203 - base105  - label smoothing 0.1  - Adam d_lr = 0.0001 and g_lr =  0.00025 69.72   adv204 - base105  - label smoothing 0.1  - Adam d_lr = 0.00001, g_lr = 0.00025 69.28 "
55,loicland/superpoint_graph,593,https://github.com/loicland/superpoint_graph,Updated 12 days ago,"Large-scale Point Cloud Semantic Segmentation with Superpoint Graphs Code structure Disclaimer Requirements Troubleshooting Running the code Evaluation Other data sets Datasets without RGB Citation      README.md           Large-scale Point Cloud Semantic Segmentation with Superpoint Graphs This is the official PyTorch implementation of the papers: Large-scale Point Cloud Semantic Segmentation with Superpoint Graphs http://arxiv.org/abs/1711.09869 by Loic Landrieu and Martin Simonovski (CVPR2018), and Point Cloud Oversegmentation with Graph-Structured Deep Metric Learning https://arxiv.org/pdf/1904.02113. by Loic Landrieu and Mohamed Boussaha (CVPR2019),   Code structure  ./partition/* - Partition code (geometric partitioning and superpoint graph construction using handcrafted features) ./supervized_partition/* - Supervized partition code (partitioning with learned features) ./learning/* - Learning code (superpoint embedding and contextual segmentation).  To switch to the stable branch with only SPG, switch to release. Disclaimer Our partition method is inherently stochastic. Hence, even if we provide the trained weights, it is possible that the results that you obtain differ slightly from the ones presented in the paper. Requirements 0. Download current version of the repository. We recommend using the --recurse-submodules option to make sure the cut pursuit module used in /partition is downloaded in the process. Wether you did not used the following command, please, refer to point 4: git clone --recurse-submodules https://github.com/loicland/superpoint_graph           1. Install PyTorch and torchnet. pip install git+https://github.com/pytorch/tnt.git@master           2. Install additional Python packages: pip install future python-igraph tqdm transforms3d pynvrtc fastrlock cupy h5py sklearn plyfile scipy pandas           3. Install Boost (1.63.0 or newer) and Eigen3, in Conda: conda install -c anaconda boost; conda install -c omnia eigen3; conda install eigen; conda install -c r libiconv           4. Make sure that cut pursuit was downloaded. Otherwise, clone this repository or add it as a submodule in /partition: cd partition git submodule init git submodule update --remote cut-pursuit           5. Compile the libply_c and libcp libraries: CONDAENV=YOUR_CONDA_ENVIRONMENT_LOCATION cd partition/ply_c cmake . -DPYTHON_LIBRARY=$CONDAENV/lib/libpython3.6m.so -DPYTHON_INCLUDE_DIR=$CONDAENV/include/python3.6m -DBOOST_INCLUDEDIR=$CONDAENV/include -DEIGEN3_INCLUDE_DIR=$CONDAENV/include/eigen3 make cd .. cd cut-pursuit mkdir build cd build cmake .. -DPYTHON_LIBRARY=$CONDAENV/lib/libpython3.6m.so -DPYTHON_INCLUDE_DIR=$CONDAENV/include/python3.6m -DBOOST_INCLUDEDIR=$CONDAENV/include -DEIGEN3_INCLUDE_DIR=$CONDAENV/include/eigen3 make           6. (optional) Install Pytorch Geometric The code was tested on Ubuntu 14 and 16 with Python 3.5 to 3.8 and PyTorch 0.2 to 1.3. Troubleshooting Common sources of errors and how to fix them:  $CONDAENV is not well defined : define it or replace $CONDAENV by the absolute path of your conda environment (find it with locate anaconda) anaconda uses a different version of python than 3.6m : adapt it in the command. Find which version of python conda is using with locate anaconda3/lib/libpython you are using boost 1.62 or older: update it cut pursuit did not download: manually clone it in the partition folder or add it as a submodule as proposed in the requirements, point 4. error in make: 'numpy/ndarrayobject.h' file not found: set symbolic link to python site-package with sudo ln -s $CONDAENV/lib/python3.7/site-packages/numpy/core/include/numpy $CONDAENV/include/numpy  Running the code To run our code or retrain from scratch on different datasets, see the corresponding readme files. Currently supported dataset are as follow:    Dataset handcrafted partition learned partition     S3DIS yes yes   Semantic3D yes to come soon   vKITTI3D no yes   ScanNet to come soon to come soon    To use pytorch-geometric graph convolutions instead of our own, use the option --use_pyg 1 in ./learning/main.py. Their code is more stable and just as fast. Otherwise, use --use_pyg 0 Evaluation To evaluate quantitatively a trained model, use (for S3DIS and vKITTI3D only): python learning/evaluate.py --dataset s3dis --odir results/s3dis/best --cvfold 123456           To visualize the results and all intermediary steps, use the visualize function in partition (for S3DIS, vKITTI3D,a nd Semantic3D). For example: python partition/visualize.py --dataset s3dis --ROOT_PATH $S3DIR_DIR --res_file results/s3dis/pretrained/cv1/predictions_test --file_path Area_1/conferenceRoom_1 --output_type igfpres           output_type defined as such:  'i' = input rgb point cloud 'g' = ground truth (if available), with the predefined class to color mapping 'f' = geometric feature with color code: red = linearity, green = planarity, blue = verticality 'p' = partition, with a random color for each superpoint 'r' = result cloud, with the predefined class to color mapping 'e' = error cloud, with green/red hue for correct/faulty prediction 's' = superedge structure of the superpoint (toggle wireframe on meshlab to view it)  Add option --upsample 1 if you want the prediction file to be on the original, unpruned data (long). Other data sets You can apply SPG on your own data set with minimal changes:  adapt references to custom_dataset in /partition/partition.py you will need to create the function read_custom_format in /partition/provider.py which outputs xyz and rgb values, as well as semantic labels if available (already implemented for ply and las files) adapt the template function /learning/custom_dataset.py to your achitecture and design choices adapt references to custom_dataset in /learning/main.py add your data set colormap to get_color_from_label in /partition/provider.py adapt line 212 of learning/spg.py to reflect the missing or extra point features change --model_config to gru_10,f_K with K as the number of classes in your dataset, or gru_10_0,f_K to use matrix edge filters instead of vectors (only use matrices when your data set is quite large, and with many different point clouds, like S3DIS).  Datasets without RGB If your data does not have RGB values you can easily use SPG. You will need to follow the instructions in partition/partition.ply regarding the pruning. You will need to adapt the /learning/custom_dataset.py file so that it does not refer ro RGB values. You should absolutely not use a model pretrained on values with RGB. instead, retrain a model from scratch using the --pc_attribs xyzelpsv option to remove RGB from the shape embedding input. Citation If you use the semantic segmentation module (code in /learning), please cite: Large-scale Point Cloud Semantic Segmentation with Superpoint Graphs, Loic Landrieu and Martin Simonovski, CVPR, 2018. If you use the learned partition module (code in /supervized_partition), please cite: Point Cloud Oversegmentation with Graph-Structured Deep Metric Learning, Loic Landrieu and Mohamed Boussaha CVPR, 2019. To refer to the handcrafted partition (code in /partition) step specifically, refer to: Weakly Supervised Segmentation-Aided Classification of Urban Scenes from 3D LiDAR Point Clouds, Stéphane Guinard and Loic Landrieu. ISPRS Workshop, 2017. To refer to the L0-cut pursuit algorithm (code in github.com/loicland/cut-pursuit)  specifically, refer to: Cut Pursuit: Fast Algorithms to Learn Piecewise Constant Functions on General Weighted Graphs, Loic Landrieu and Guillaume Obozinski, SIAM Journal on Imaging Sciences, 2017 To refer to pytorch geometric implementation, see their bibtex in their repo. "
56,JunshengFu/semantic_segmentation,85,https://github.com/JunshengFu/semantic_segmentation,"Updated on Mar 12, 2021","Road Segmentation Objective Demo 1 Code & Files 1.1 My project includes the following files and folders 1.2 Dependencies & my environment 1.3 How to run the code 1.4. Release History 2 Network Architecture 2.1 Fully Convolutional Networks (FCN) in the Wild 2.2 Fully Convolutional Networks for Semantic Segmentation 2.3 Classification & Loss 3 Dataset 3.1 Training data examples from KITTI 3.2 Testing data 4 Experiments 5 Discussion 5.1 Good Performance 5.2 Limitations      README.md           Road Segmentation Objective In the case of the autonomous driving, given an front camera view, the car needs to know where is the road. In this project, we trained a neural network to label the pixels of a road in images, by using a method named Fully Convolutional Network (FCN). In this project, FCN-VGG16 is implemented and trained with KITTI dataset for road segmentation. Demo  (click to see the full video)  1 Code & Files 1.1 My project includes the following files and folders  main.py is the main code for demos project_tests.py includes the unittest helper.py includes some helper functions env-gpu-py35.yml is environmental file with GPU and Python3.5 data folder contains the KITTI road data, the VGG model and source images. model folder is used to save the trained model runs folder contains the segmentation examples of the testing data  1.2 Dependencies & my environment Miniconda is used for managing my dependencies.  Python3.5, tensorflow-gpu, CUDA8, Numpy, SciPy OS: Ubuntu 16.04 CPU: Intel® Core™ i7-6820HK CPU @ 2.70GHz × 8 GPU: GeForce GTX 980M (8G memory) Memory: 32G  1.3 How to run the code (1) Download KITTI data (training and testing) Download the Kitti Road dataset from here.  Extract the dataset in the data folder. This will create the folder data_road with all the training a test images. (2) Load pre-trained VGG Function maybe_download_pretrained_vgg() in helper.py will do it automatically for you. (3) Run the code: python main.py          (4) Use my trained model to predict new images You can download my trained model here and save it to the folder model. Also, you need to set the training flag to False in the main.py: training_flag = False           Then run the code by: python main.py          1.4. Release History   0.1.1  Updated documents Date 7 December 2017    0.1.0  The first proper release Date 6 December 2017     2 Network Architecture 2.1 Fully Convolutional Networks (FCN) in the Wild  FCNs can be described as the above example: a pre-trained model, follow by 1-by-1 convolutions, then followed by transposed convolutions. Also, we can describe it as encoder (a pre-trained model + 1-by-1 convolutions) and decoder (transposed convolutions). 2.2 Fully Convolutional Networks for Semantic Segmentation  The Semantic Segmentation network provided by this paper learns to combine coarse, high layer informaiton with fine, low layer information. The pooling and prediction layers are shown as grid that reveal relative spatial coarseness, while intermediate layers are shown as vertical lines   The encoder  VGG16 model pretrained on ImageNet for classification (see VGG16 architecutre below) is used in encoder. And the fully-connected layers are replaced by 1-by-1 convolutions.    The decoder  Transposed convolution is used to upsample the input to the original image size. Two skip connections are used in the model.    VGG-16 architecture  2.3 Classification & Loss we can approach training a FCN just like we would approach training a normal classification CNN. In the case of a FCN, the goal is to assign each pixel to the appropriate class, and cross entropy loss is used as the loss function. We can define the loss function in tensorflow as following commands. logits = tf.reshape(input, (-1, num_classes)) cross_entropy_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, labels))          Then, we have an end-to-end model for semantic segmentation. 3 Dataset 3.1 Training data examples from KITTI Origin Image  Mask image  In this project, 384 labeled images are used as training data. Download the Kitti Road dataset from here. 3.2 Testing data There are 4833 testing images are processed with the trained models. 4543 frames from are a video and other 290 images from random places in Karlsruhe. 4 Experiments Some key parameters in training stage, and the traning loss and training time for each epochs are shown in the following table. epochs = 37 batch_size = 8 learning_rate = 0.0001              epochs learning_rate exec_time (s) training_loss     1 0.0001 43.16 0.7978   2 0.0001 38.52 0.5058   3 0.0001 38.55 0.2141   4 0.0001 38.56 0.1696   5 0.0001 38.39 0.1339   6 0.0001 38.44 0.1215   7 0.0001 38.68 0.1089   8 0.0001 38.3 0.0926   9 0.0001 38.14 0.0913   10 0.0001 38.08 0.0837   11 0.0001 38.34 0.0703   12 0.0001 38.02 0.0663   13 0.0001 38.21 0.0585   14 0.0001 38.33 0.0549   15 0.0001 38.12 0.0525   16 0.0001 38.31 0.0483   17 0.0001 38.4 0.0465   18 0.0001 38.42 0.0454   19 0.0001 38.27 0.0421   20 0.0001 38.73 0.0404   21 0.0001 38.03 0.039   22 0.0001 38.22 0.0387   23 0.0001 37.95 0.0368   24 0.0001 38.22 0.0352   25 0.0001 38.91 0.0335   26 0.0001 38.67 0.0324   27 0.0001 38.21 0.0316   28 0.0001 38.2 0.0302   29 0.0001 38.13 0.0291   30 0.0001 38.19 0.0313   31 0.0001 38.15 0.0303   32 0.0001 38.16 0.0299   33 0.0001 38.11 0.0273   34 0.0001 38.21 0.0265   35 0.0001 38.16 0.0254   36 0.0001 38.62 0.0244   37 0.0001 37.99 0.0234     5 Discussion 5.1 Good Performance With only 384 labeled training images, the FCN-VGG16 performs well to find where is the road in the testing data, and the testing speed is about 6 fps in my laptop. The model performs very well on either highway or urban driving. Some testing examples are shown as follows:      5.2 Limitations Based on my test on 4833 testing images. There are two scenarios where th current model does NOT perform well: (1) turning spot, (2) over-exposed area. The bad performance at the turning spots might be caused by the fact of lacking training examples that from turning spots, because almost all the training images are taken when the car was driving straight or turning slightly. We might be able to improve the performance by adding more training data that are taken at the turning spots. As for the over-exposed area, it is more challenging.  One possible approach is to use white-balance techniques or image restoration methods to get the correct image. The other possible approach is to add more training data with over-exposed scenarios, and let the network to learn how to segment the road even under the over-expose scenarios. Turning spot  Over-exposed area "
57,daijifeng001/MNC,480,https://github.com/daijifeng001/MNC,"Updated on Jun 9, 2017","Instance-aware Semantic Segmentation via Multi-task Network Cascades Introduction Misc. Citing MNC Main Results Installation guide Demo Training Preparation: 1. End-to-end training of MNC for instance-aware semantic segmentation 2. Training of CFM for instance-aware semantic segmentation 2.1. Download pre-computed MCG proposals 2.2. Train the model 3. End-to-end training of Faster-RCNN for object detection      README.md           Instance-aware Semantic Segmentation via Multi-task Network Cascades By Jifeng Dai, Kaiming He, Jian Sun This python version is re-implemented by Haozhi Qi when he was an intern at Microsoft Research. Introduction MNC is an instance-aware semantic segmentation system based on deep convolutional networks, which won the first place in COCO segmentation challenge 2015, and test at a fraction of a second per image. We decompose the task of instance-aware semantic segmentation into related sub-tasks, which are solved by multi-task network cascades (MNC) with shared features. The entire MNC network is trained end-to-end with error gradients across cascaded stages.  MNC was initially described in a CVPR 2016 oral paper. This repository contains a python implementation of MNC, which is ~10% slower than the original matlab implementation. This repository includes a bilinear RoI warping layer, which enables gradient back-propagation with respect to RoI coordinates. Misc. This code has been tested on Linux (Ubuntu 14.04), using K40/Titan X GPUs. The code is built based on py-faster-rcnn. MNC is released under the MIT License (refer to the LICENSE file for details). Citing MNC If you find MNC useful in your research, please consider citing: @inproceedings{dai2016instance,     title={Instance-aware Semantic Segmentation via Multi-task Network Cascades},     author={Dai, Jifeng and He, Kaiming and Sun, Jian},     booktitle={CVPR},     year={2016} }           Main Results     training data test data mAP^r@0.5 mAP^r@0.7 time (K40) time (Titian X)     MNC, VGG-16 VOC 12 train VOC 12 val 65.0% 46.3% 0.42sec/img 0.33sec/img    Installation guide  Clone the MNC repository:  # Make sure to clone with --recursive git clone --recursive https://github.com/daijifeng001/MNC.git            Install Python packages: numpy, scipy, cython, python-opencv, easydict, yaml.   Build the Cython modules and the gpu_nms, gpu_mask_voting modules by:   cd $MNC_ROOT/lib make           Install Caffe and pycaffe dependencies (see: Caffe installation instructions for official installation guide)  Note: Caffe must be built with support for Python layers! # In your Makefile.config, make sure to have this line uncommented WITH_PYTHON_LAYER := 1 # CUDNN is recommended in building to reduce memory footprint USE_CUDNN := 1           Build Caffe and pycaffe: cd $MNC_ROOT/caffe-mnc # If you have all of the requirements installed # and your Makefile.config in place, then simply do: make -j8 && make pycaffe            Demo First, download the trained MNC model. ./data/scripts/fetch_mnc_model.sh          Run the demo: cd $MNC_ROOT ./tools/demo.py          Result demo images will be stored to data/demo/. The demo performs instance-aware semantic segmentation with a trained MNC model (using VGG-16 net). The model is pre-trained on ImageNet, and finetuned on VOC 2012 train set with additional annotations from SBD. The mAP^r of the model is 65.0% on VOC 2012 validation set. The test speed per image is ~0.33sec on Titian X and ~0.42sec on K40. Training This repository contains code to end-to-end train MNC for instance-aware semantic segmentation, where gradients across cascaded stages are counted in training. Preparation:  Run ./data/scripts/fetch_imagenet_models.sh to download the ImageNet pre-trained VGG-16 net. Download the VOC 2007 dataset to ./data/VOCdevkit2007 Run ./data/scripts/fetch_sbd_data.sh to download the VOC 2012 dataset together with the additional segmentation annotations in SBD to ./data/VOCdevkitSDS.  1. End-to-end training of MNC for instance-aware semantic segmentation To end-to-end train a 5-stage MNC model (on VOC 2012 train), use experiments/scripts/mnc_5stage.sh. Final mAP^r@0.5 should be ~65.0% (mAP^r@0.7 should be ~46.3%), on VOC 2012 validation. cd $MNC_ROOT ./experiments/scripts/mnc_5stage.sh [GPU_ID] VGG16 [--set ...] # GPU_ID is the GPU you want to train on # --set ... allows you to specify fast_rcnn.config options, e.g. #   --set EXP_DIR seed_rng 1701 RNG_SEED 1701          2. Training of CFM for instance-aware semantic segmentation The code also includes an entry to train a convolutional feature masking (CFM) model for instance aware semantic segmentation. @inproceedings{dai2015convolutional,     title={Convolutional Feature Masking for Joint Object and Stuff Segmentation},     author={Dai, Jifeng and He, Kaiming and Sun, Jian},     booktitle={CVPR},     year={2015} }           2.1. Download pre-computed MCG proposals Download and process the pre-computed MCG proposals. cd $MNC_ROOT ./data/scripts/fetch_mcg_data.sh python ./tools/prepare_mcg_maskdb.py --para_job 24 --db train --output data/cache/voc_2012_train_mcg_maskdb/ python ./tools/prepare_mcg_maskdb.py --para_job 24 --db val --output data/cache/voc_2012_val_mcg_maskdb/          Resulting proposals would be at folder data/MCG/. 2.2. Train the model Run experiments/scripts/cfm.sh to train on VOC 2012 train set. Final mAP^r@0.5 should be ~60.5% (mAP^r@0.7 should be ~42.6%), on VOC 2012 validation. cd $MNC_ROOT ./experiments/scripts/cfm.sh [GPU_ID] VGG16 [--set ...] # GPU_ID is the GPU you want to train on # --set ... allows you to specify fast_rcnn.config options, e.g. #   --set EXP_DIR seed_rng 1701 RNG_SEED 1701          3. End-to-end training of Faster-RCNN for object detection Faster-RCNN can be viewed as a 2-stage cascades composed of region proposal network (RPN) and object detection network. Run script experiments/scripts/faster_rcnn_end2end.sh to train a Faster-RCNN model on VOC 2007 trainval. Final mAP^b should be ~69.1% on VOC 2007 test. cd $MNC_ROOT ./experiments/scripts/faster_rcnn_end2end.sh [GPU_ID] VGG16 [--set ...] # GPU_ID is the GPU you want to train on # --set ... allows you to specify fast_rcnn.config options, e.g. #   --set EXP_DIR seed_rng1701 RNG_SEED 1701 "
58,wuhuikai/FastFCN,771,https://github.com/wuhuikai/FastFCN,"Updated on Nov 20, 2020","FastFCN: Rethinking Dilated Convolution in the Backbone for Semantic Segmentation Update Version Overview Framework Joint Pyramid Upsampling (JPU) Install Train and Test PContext ADE20K Training Set Training Set + Val Set Visual Results More Visual Results Acknowledgement      README.md           FastFCN: Rethinking Dilated Convolution in the Backbone for Semantic Segmentation [Project] [Paper] [arXiv] [Home]  Official implementation of FastFCN: Rethinking Dilated Convolution in the Backbone for Semantic Segmentation. A Faster, Stronger and Lighter framework for semantic segmentation, achieving the state-of-the-art performance and more than 3x acceleration. @inproceedings{wu2019fastfcn,   title     = {FastFCN: Rethinking Dilated Convolution in the Backbone for Semantic Segmentation},   author    = {Wu, Huikai and Zhang, Junge and Huang, Kaiqi and Liang, Kongming and Yu Yizhou},   booktitle = {arXiv preprint arXiv:1903.11816},   year = {2019} }           Contact: Hui-Kai Wu (huikaiwu@icloud.com) Update 2020-04-15: Now support inference on a single image !!! CUDA_VISIBLE_DEVICES=0,1,2,3 python -m experiments.segmentation.test_single_image --dataset [pcontext|ade20k] \     --model [encnet|deeplab|psp] --jpu [JPU|JPU_X] \     --backbone [resnet50|resnet101] [--ms] --resume {MODEL} --input-path {INPUT} --save-path {OUTPUT}          2020-04-15: New joint upsampling module is now available !!!  --jpu [JPU|JPU_X]: JPU is the original module in the arXiv paper; JPU_X is a pyramid version of JPU.  2020-02-20: FastFCN can now run on every OS with PyTorch>=1.1.0 and Python==3.*.*  Replace all C/C++ extensions with pure python extensions.  Version  Original code, producing the results reported in the arXiv paper. [branch:v1.0.0] Pure PyTorch code, with torch.nn.DistributedDataParallel and torch.nn.SyncBatchNorm. [branch:latest] Pure Python code. [branch:master]  Overview Framework  Joint Pyramid Upsampling (JPU)  Install  PyTorch >= 1.1.0 (Note: The code is test in the environment with python=3.6, cuda=9.0) Download FastFCN git clone https://github.com/wuhuikai/FastFCN.git cd FastFCN            Install Requirements nose tqdm scipy cython requests             Train and Test PContext python -m scripts.prepare_pcontext              Method Backbone mIoU FPS Model Scripts     EncNet ResNet-50 49.91 18.77     EncNet+JPU (ours) ResNet-50 51.05 37.56 GoogleDrive bash   PSP ResNet-50 50.58 18.08     PSP+JPU (ours) ResNet-50 50.89 28.48 GoogleDrive bash   DeepLabV3 ResNet-50 49.19 15.99     DeepLabV3+JPU (ours) ResNet-50 50.07 20.67 GoogleDrive bash   EncNet ResNet-101 52.60 (MS) 10.51     EncNet+JPU (ours) ResNet-101 54.03 (MS) 32.02 GoogleDrive bash    ADE20K python -m scripts.prepare_ade20k           Training Set    Method Backbone mIoU (MS) Model Scripts     EncNet ResNet-50 41.11     EncNet+JPU (ours) ResNet-50 42.75 GoogleDrive bash   EncNet ResNet-101 44.65     EncNet+JPU (ours) ResNet-101 44.34 GoogleDrive bash    Training Set + Val Set    Method Backbone FinalScore (MS) Model Scripts     EncNet+JPU (ours) ResNet-50  GoogleDrive bash   EncNet ResNet-101 55.67     EncNet+JPU (ours) ResNet-101 55.84 GoogleDrive bash    Note: EncNet (ResNet-101) is trained with crop_size=576, while EncNet+JPU (ResNet-101) is trained with crop_size=480 for fitting 4 images into a 12G GPU. Visual Results    Dataset Input GT EncNet Ours     PContext       ADE20K        More Visual Results Acknowledgement Code borrows heavily from PyTorch-Encoding. "
59,DrSleep/light-weight-refinenet,655,https://github.com/DrSleep/light-weight-refinenet,"Updated on Oct 12, 2021","Light-Weight RefineNet (in PyTorch) UPDATES 14, July, 2020: 5, June, 2020: a new version of the code has been pushed. It currently resides in src_v2/. The code now closely interacts with densetorch and supports transformations from albumentations, while also supporting torchvision datasets. Three training examples are provided in train/: Getting Started Dependencies Running examples Jupyter Notebooks [Local] Colab Notebooks [Web] Training scripts More to come More projects to check out License Acknowledgments      README.md           Light-Weight RefineNet (in PyTorch)      This repository provides official models from the paper Light-Weight RefineNet for Real-Time Semantic Segmentation, available here Light-Weight RefineNet for Real-Time Semantic Segmentation Vladimir Nekrasov, Chunhua Shen, Ian Reid In BMVC 2018           UPDATES 14, July, 2020:  New weights of Light-Weight RefineNet with the ResNet-50 backbone trained on COCO+BSD+VOC via the code in src_v2/ have been uploaded. The model shows 82.04% mean iou on the validation set in the single-scale regime, and 83.41% mean iou on the test set with multi-scale and horizontal flipping (per-class test results). New weights of Light-Weight RefineNet with the MobileNet-v2 backbone trained on COCO+BSD+VOC via the code in src_v2/ have been uploaded. The model shows 78.30% mean iou on the validation set in the single-scale regime, and 80.28% mean iou on the test set with multi-scale and horizontal flipping (per-class test results).  5, June, 2020: a new version of the code has been pushed. It currently resides in src_v2/. The code now closely interacts with densetorch and supports transformations from albumentations, while also supporting torchvision datasets. Three training examples are provided in train/:  train_v2_nyu.sh is analogous to nyu.sh, trains Light-Weight-RefineNet-50 on NYU, achieving ~42.4% mean IoU on the validation set (no TTA). train_v2_nyu_albumentations.sh uses transformations from the albumentations package, achieving ~42.5% mean IoU on the validation set (no TTA). train_v2_sbd_voc.sh trains Light-Weight-RefineNet-50 on SBD (5623 training images) and VOC (1464 training images) datasets from torchvision with transformations from the albumentations package; achieves ~76% mean IoU on the validation set with no TTA (1449 validation images).  If you want to train the network on your own dataset, specify the arguments (see the available options in src_v2/arguments.py) and provide implementation of your dataset in src_v2/data.py if it is not supported by either densetorch or torchvision. Getting Started For flawless reproduction of our results, the Ubuntu OS is recommended. The models have been tested using Python 2.7 and Python 3.6. Dependencies pip, pip3 torch>=0.4.0           To install required Python packages, please run pip install -r requirements.txt (Python2), or pip3 install -r requirements3.txt (Python3) - use the flag -u for local installation. The given examples can be run with, or without GPU. Running examples For the ease of reproduction, we have embedded all our examples inside Jupyter notebooks. One can either download them from this repository and proceed working with them on his/her local machine/server, or can resort to online version supported by the Google Colab service. Jupyter Notebooks [Local] If all the installation steps have been smoothly executed, you can proceed with running any of the notebooks provided in the examples/notebooks folder. To start the Jupyter Notebook server, on your local machine run jupyter notebook. This will open a web page inside your browser. If it did not open automatically, find the port number from the command's output and paste it into your browser manually. After that, navigate to the repository folder and choose any of the examples given. The number of FLOPs and runtime are measured on 625x468 inputs using a single GTX1080Ti, mean IoU is given on corresponding validation sets with a single scale input.    Models PASCAL VOC Person-Part PASCAL Context NYUv2, 40 Params, M FLOPs, B Runtime, ms     RF-LW-ResNet-50 78.5 64.9 - 41.7 27 33 19.56±0.29   RF-LW-ResNet-101 80.3 66.7 45.1 43.6 46 52 27.16±0.19   RF-LW-ResNet-152 82.1 67.6 45.8 44.4 62 71 35.82±0.23   RF-LW-MobileNet-v2 76.2 - - - 3.3 9.3 -    Inside the notebook, one can try out their own images, write loops to iterate over videos / whole datasets / streams (e.g., from webcam). Feel free to contribute your cool use cases of the notebooks! Colab Notebooks [Web] If you do not want to be involved in any hassle regarding the setup of the Jupyter Notebook server, you can proceed by using the same examples inside the Google colab environment - with free GPUs available!  PASCAL Context PASCAL Person-Part PASCAL VOC NYUv2-40  Training scripts We provide training scripts to get you started on the NYUv2-40 dataset. The methodology slightly differs from the one described in the paper and leads to better and more stable results (at least, on NYU). In particular, here we i) start with a lower learning rate (as we initialise weights using default PyTorch's intiialisation instead of normal(0.01)), ii) add more aggressive augmentation (random scale between 0.5 and 2.0), and iii) pad each image inside the batch to a fixed crop size (instead of resizing all of them). The training process is divided into 3 stages: after each the optimisers are re-created with the learning rates halved. All the training is done using a single GTX1080Ti GPU card. Additional experiments with this new methodology on the other datasets (and with the MobileNet-v2 backbone) are under way, and relevant scripts will be provided once available. Please also note that the training scripts were written in Python 3.6. To start training on NYU:  If not already done, download the dataset from here. Note that the white borders in all the images were already cropped. Build the helper code for calculating mean IoU written in Cython. For that, execute the following python src/setup.py build_ext --build-lib=./src/. Make sure to provide the correct paths to the dataset images either by modifying src/config.py or train/nyu.sh Run ./train/nyu.sh. On a single 1080Ti, the training takes around 3-6 hours (ResNet-50 - ResNet-152, correspondingly).  If you want to train the networks using your dataset, you would need to modify the following:  Add files with paths to your images and segmentation masks. The paths can either be relative or absolute - additional flags TRAIN_DIR and VAL_DIR in src/config.py can be used to prepend the relative paths. It is up to you to decide how to encode the segmentation masks - in the NYU example, the masks are encoded without a colourmap, i.e., with a single digit (label) per 2-D location; Make sure to adapt the implementation of the NYUDataset for your case in src/datasets.py: in particular, pay attention to how the images and masks are being read from the files; Modify src/config.py for your needs - do not forget about changing the number of classes (NUM_CLASSES); Finally, run your code - see train/nyu.sh for example.  More to come Once time permits, more things will be added to this repository:  NASNet-Mobile CityScapes' models Full training pipeline example Evaluation scripts (src/train.py provides the flag --evaluate)  More projects to check out  Our most recent work on real-time joint semantic segmentation and depth estimation is built on top of Light-Weight RefineNet with MobileNet-v2. Check out the paper here; the models are available here! RefineNet-101 trained on PASCAL VOC is available here  License For academic usage, this project is licensed under the 2-clause BSD License - see the LICENSE file for details. For commercial usage, please contact the authors. Acknowledgments  University of Adelaide and Australian Centre for Robotic Vision (ACRV) for making this project happen HPC Phoenix cluster at the University of Adelaide for making the training of the models possible PyTorch developers Google Colab Yerba mate tea "
60,wvangansbeke/Unsupervised-Semantic-Segmentation,227,https://github.com/wvangansbeke/Unsupervised-Semantic-Segmentation,"Updated on Oct 2, 2021","Unsupervised Semantic Segmentation by Contrasting Object Mask Proposals Contents Introduction Installation Training MaskContrast Setup Pre-train model Evaluation Linear Classifier (LC) Clustering (K-means) Semantic Segment Retrieval Model Zoo Citation License Acknoledgements      README.md           Unsupervised Semantic Segmentation by Contrasting Object Mask Proposals This repo contains the Pytorch implementation of our paper:  Unsupervised Semantic Segmentation by Contrasting Object Mask Proposals Wouter Van Gansbeke, Simon Vandenhende, Stamatios Georgoulis, and Luc Van Gool.   Accepted at ICCV 2021 (Slides).  SOTA for unsupervised semantic segmentation. Check out Papers With Code for the Unsupervised Semantic Segmentation benchmark and more details.     Contents  Introduction Installation Training MaskContrast  Setup Train   Evaluation  Linear Classifier Clustering Semantic Segment Retrieval   Model Zoo Citation  Introduction Being able to learn dense semantic representations of images without supervision is an important problem in computer vision. However, despite its significance, this problem remains rather unexplored, with a few exceptions that considered unsupervised semantic segmentation on small-scale datasets with a narrow visual domain. We make a first attempt to tackle the problem on datasets that have been traditionally utilized for the supervised case (e.g. PASCAL VOC). To achieve this, we introduce a novel two-step framework that adopts a predetermined prior in a contrastive optimization objective to learn pixel embeddings. Additionally, we argue about the importance of having a prior that contains information about objects, or their parts, and discuss several possibilities to obtain such a prior in an unsupervised manner. In particular, we adopt a mid-level visual prior to group pixels together and contrast the obtained object mask porposals. For this reason we name the method MaskContrast. Installation The Python code is compatible with Pytorch version 1.4 (version 1.5 should work as well). Assuming Anaconda, the most important packages can be installed as: conda install pytorch=1.4.0 torchvision=0.5.0 cudatoolkit=10.0 -c pytorch conda install -c conda-forge opencv           # For image transformations conda install matplotlib scipy scikit-learn   # For evaluation conda install pyyaml easydict                 # For using config files conda install termcolor                       # For colored print statements          We refer to the requirements.txt file for an overview of the packages in the environment we used to produce our results. The code was run on 2 Tesla V100 GPUs. Training MaskContrast Setup The PASCAL VOC dataset will be downloaded automatically when running the code for the first time. The dataset includes the precomputed supervised and unsupervised saliency masks, following the implementation from the paper. The following files (in the pretrain/ and segmentation/ directories) need to be adapted in order to run the code on your own machine:  Change the file path for the datasets in data/util/mypath.py. The PASCAL VOC dataset will be saved to this path. Specify the output directory in configs/env.yml. All results will be stored under this directory.  Pre-train model The training procedure consists of two steps. First, pixels are grouped together based upon a mid-level visual prior (saliency is used). Then, a pre-training strategy is proposed to contrast the pixel-embeddings of the obtained object masks. The code for the pre-training can be found in the pretrain/ directory and the configuration files are located in the pretrain/configs/ directory. You can choose to run the model with the masks from the supervised or unsupervised saliency model. For example, run the following command to perform the pre-training step on PASCAL VOC with the supervised saliency model: cd pretrain python main.py --config_env configs/env.yml --config_exp configs/VOCSegmentation_supervised_saliency_model.yml          Evaluation Linear Classifier (LC) We freeze the weights of the pre-trained model and train a 1 x 1 convolutional layer to predict the class assignments from the generated feature representations. Since the discriminative power of a linear classifier is low, the pixel embeddings need to be informative of the semantic class to solve the task in this way. To train the classifier run the following command: cd segmentation python linear_finetune.py --config_env configs/env.yml --config_exp configs/linear_finetune/linear_finetune_VOCSegmentation_supervised_saliency.yml          Note, make sure that the pretraining variable in linear_finetune_VOCSegmentation_supervised_saliency.yml points to the location of your pre-trained model. You should get the following results: mIoU is 63.95 IoU class background is 90.95 IoU class aeroplane is 83.78 IoU class bicycle is 30.66 IoU class bird is 78.79 IoU class boat is 64.57 IoU class bottle is 67.31 IoU class bus is 84.24 IoU class car is 76.77 IoU class cat is 79.10 IoU class chair is 21.24 IoU class cow is 66.45 IoU class diningtable is 46.63 IoU class dog is 73.25 IoU class horse is 62.61 IoU class motorbike is 69.66 IoU class person is 72.30 IoU class pottedplant is 40.15 IoU class sheep is 74.70 IoU class sofa is 30.43 IoU class train is 74.67 IoU class tvmonitor is 54.66           Unsurprisingly, the model has not learned a good representation for every class since some classes are hard to distinguish, e.g. chair or sofa. We visualize a few examples after CRF post-processing below.   Clustering (K-means) The feature representations are clustered with K-means. If the pixel embeddings are disentangled according to the defined class labels, we can match the predicted clusters with the ground-truth classes using the Hungarian matching algorithm. cd segmentation python kmeans.py --config_env configs/env.yml --config_exp configs/kmeans/kmeans_VOCSegmentation_supervised_saliency.yml          Remarks: Note that we perform the complete K-means fitting on the validation set to save memory and that the reported results were averaged over 5 different runs. You should get the following results (21 clusters): IoU class background is 88.17 IoU class aeroplane is 77.41 IoU class bicycle is 26.18 IoU class bird is 68.27 IoU class boat is 47.89 IoU class bottle is 56.99 IoU class bus is 80.63 IoU class car is 66.80 IoU class cat is 46.13 IoU class chair is 0.73 IoU class cow is 0.10 IoU class diningtable is 0.57 IoU class dog is 35.93 IoU class horse is 48.68 IoU class motorbike is 60.60 IoU class person is 32.24 IoU class pottedplant is 23.88 IoU class sheep is 36.76 IoU class sofa is 26.85 IoU class train is 69.90 IoU class tvmonitor is 27.56           Semantic Segment Retrieval We examine our representations on PASCAL through segment retrieval. First, we compute a feature vector for every object mask in the val set by averaging the pixel embeddings within the predicted mask. Next, we retrieve the nearest neighbors on the train_aug set for each object. cd segmentation python retrieval.py --config_env configs/env.yml --config_exp configs/retrieval/retrieval_VOCSegmentation_unsupervised_saliency.yml             Method MIoU (7 classes) MIoU (21 classes)     MoCo v2 48.0 39.0   MaskContrast* (unsup sal.) 53.4 43.3   MaskContrast* (sup sal.) 62.3 49.6    * Denotes MoCo init. Model Zoo Download the pretrained and linear finetuned models here.    Dataset Pixel Grouping Prior mIoU (LC) mIoU (K-means) Download link     PASCAL VOC Supervised Saliency - 44.2 Pretrained Model   PASCAL VOC Supervised Saliency 63.9 (65.5*) 44.2 Linear Finetuned   PASCAL VOC Unsupervised Saliency - 35.0 Pretrained Model   PASCAL VOC Unsupervised Saliency 58.4 (59.5*) 35.0 Linear Finetuned    * Denotes CRF post-processing. To evaluate and visualize the predictions of the finetuned model, run the following command: cd segmentation python eval.py --config_env configs/env.yml --config_exp configs/linear_finetune/linear_finetune_VOCSegmentation_supervised_saliency.yml --state-dict $PATH_TO_MODEL          You can optionally append the --crf-postprocess flag. Citation This code is based on the SCAN and MoCo repositories. If you find this repository useful for your research, please consider citing the following paper(s): @inproceedings{vangansbeke2020unsupervised,   title={Unsupervised Semantic Segmentation by Contrasting Object Mask Proposals},   author={Van Gansbeke, Wouter and Vandenhende, Simon and Georgoulis, Stamatios and Van Gool, Luc},   booktitle={International Conference on Computer Vision},   year={2021} } @inproceedings{vangansbeke2020scan,   title={Scan: Learning to classify images without labels},   author={Van Gansbeke, Wouter and Vandenhende, Simon and Georgoulis, Stamatios and Proesmans, Marc and Van Gool, Luc},   booktitle={European Conference on Computer Vision},   year={2020} } @inproceedings{he2019moco,   title={Momentum Contrast for Unsupervised Visual Representation Learning},   author={Kaiming He and Haoqi Fan and Yuxin Wu and Saining Xie and Ross Girshick},   booktitle = {Conference on Computer Vision and Pattern Recognition},   year={2019} }          For any enquiries, please contact the main authors. For an overview on self-supervised learning, have a look at the overview repository. License This software is released under a creative commons license which allows for personal and research use only. For a commercial license please contact the authors. You can view a license summary here. Acknoledgements This work was supported by Toyota, and was carried out at the TRACE Lab at KU Leuven (Toyota Research on Automated Cars in Europe - Leuven). "
61,martinkersner/train-CRF-RNN,196,https://github.com/martinkersner/train-CRF-RNN,"Updated on Oct 6, 2017","Train CRF-RNN for Semantic Image Segmentation Prerequisites Prepare dataset for training Download PASCAL VOC dataset Split classes Create LMDB database Training Visualization FAQ I don't want to train with 3 classes. What should I do?      README.md           Train CRF-RNN for Semantic Image Segmentation Martin Kersner, m.kersner@gmail.com This repository contains Python scripts necessary for training CRF-RNN for Semantic Image Segmentation with 3 classes. git clone --recursive https://github.com/martinkersner/train-CRF-RNN          Prerequisites In order to be able to train CRF-RNN you will need to install caffe from CRF-RNN. Prepare dataset for training First, you will need images with corresponding semantic labels. The easiest way is to employ PASCAL VOC dataset (!2GB) which provides those image/label pairs. Dataset consist of 21 different classes1, but in this example we will use only three of them in order to demonstrate training with different number classes than it was used in original CRF-RNN. Download PASCAL VOC dataset wget http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar tar -xvf VOCtrainval_11-May-2012.tar          After executing commands above you can find in VOCdevkit/VOC2012/SegmentationClass 2913 labels and in VOCdevkit/VOC2012/JPEGImages their corresponding original images2. In order to have a better access to those directories we will create symlinks to them. Therefore, from your cloned repository you should run following commands (replace $DATASETS with your actual path where you downloaded PASCAL VOC dataset). ln -s $DATASETS/VOCdevkit/VOC2012/SegmentationClass labels ln -s $DATASETS/VOCdevkit/VOC2012/JPEGImages images          Split classes In the next step we have to select only images that contain classes (in our case 3) for which we want to train our semantic segmentation algorithm. At first we create a list of all images that can be exploited for segmentation. find labels/ -printf '%f\n' | sed 's/\.png//'  | tail -n +2 > train.txt          Ground truth segmentations in PASCAL VOC 2012 dataset are defined as RGB images. However, if you decide to use different dataset or already preprocessed segmentations, you could be working with gray-level ones whose values exactly correspondent to label indexes in documentation. Because the workflow of creating dataset for training is separated to several parts, we access some images twice. In a case that we are working with unpreprocessed ground truth segmentations, we would have to perform conversion twice. Unfortunately, this conversion is rather time consuming (~2s), therefore we suggest to run following command first. It is not mandatory though. python convert_labels.py labels/ train.txt converted_labels/ # OPTIONAL          Then we decide which classes we are interested in and specify them in filter_images.py (on line 15 there is set bird, bottle and chair class). This script will create several text files (which list images containing our desired classes) named correspondingly to selected classes. Each file has the same structure as train.txt. In a case of experimenting with different classes it would be wise to generate those image list for all classes from dataset. You should be aware that if an image label is composed from more than one class in which we are interested in, that image will be always assigned to a class with lower id. This behavior could potentionally cause a problem if dataset consists of many images with the same label couples. However, this doesn't count for background class. python filter_images.py labels/ train.txt # in a case you DID NOT RUN convert_labels.py script #python filter_images.py converted_labels/ train.txt # you RUN convert_labels.py script          Create LMDB database Original CRF-RNN used for training images with size 500x500 px and we will do so as well. But if, for whatever reason, one would decide for different dimensions3 it can be changed on line 20 of data2lmdb.py. Currently, we expect that the larger side in no more than 500 px. Because images/labels don't always correspond to required dimensions, we padd them with zeros in order to obtain right image/label size. On line 21 we can set labels which we want to include into dataset. Within training we will regularly test our network's performance. Thus, besides the training data we will need a testing data. On line 22 we can set a ratio (currently 0.1 == 10 percent of data) which denotes how much percent of data from whole dataset will be included in the test data. Following command will create four directories with training/testing data for images/labels. python data2lmdb.py # in a case you DID NOT RUN convert_labels.py script #python data2lmdb.py converted_labels/ # you RUN convert_labels.py script          Training In order to be able to start a training we will need to download precomputed weights for CRF-RNN first. wget http://goo.gl/j7PrPZ -O TVG_CRFRNN_COCO_VOC.caffemodel python solve.py 2>&1 | tee train.log          Visualization During training we can visualize a loss using loss_from_log.py. Script accepts even more than one log file. That can be useful when we had to stop training and restarted it from the last state. Therefore, we end up with two or more log files. python loss_from_log.py train.log             FAQ I don't want to train with 3 classes. What should I do? You have to generate lists of images for more or less classes. This is described in a paragraph above called Split classes. Afterward, you will also have to change prototxt description of network TVG_CRFRNN_COCO_VOC_TRAIN_3_CLASSES.prototxt. Each line in this file which contains text CHANGED should be modified. At each of those lines is num_ouput: 4, denoting 3 classes and background. If you want to use for example 6 different classes, you should change parameter num_ouput at those lines to number 7.  (1) aeroplane, bicycle, bird, boat, bottle, bus, car , cat, chair, cow, diningtable, dog, horse, motorbike, person, potted plant, sheep, sofa, train, tv/monitor (2) Maybe one noticed that in JPEGImages directory there are more than 2913 images. This is because dataset is not used only for segmentation but also for detection. (3) The larger dimensions of input images are, the more memory for training is required. "
62,guosheng/refinenet,559,https://github.com/guosheng/refinenet,"Updated on May 31, 2019","Multipath RefineNet Pytorch implementation Update notes Results Trained models Network architecture and implementation Installation Testing 1. Multi-scale prediction and evaluation (new!) 2. Single scale prediction and evaluation 3. Evaluation and fusion on saved results (score map files and mask files) (new!) Training Citation License      README.md           Multipath RefineNet A MATLAB based framework for semantic image segmentation and general dense prediction tasks on images. This is the source code for the following paper and its extension:  RefineNet: Multi-Path Refinement Networks for High-Resolution Semantic Segmentation; CVPR 2017 https://arxiv.org/abs/1611.06612 RefineNet extension in TPAMI 2019: DOI Link  Pytorch implementation This codebase only provides MATLAB and MatConvNet based implementation. Vladimir Nekrasov kindly provides a Pytorch implementation and a light-weight version of RefineNet at: https://github.com/DrSleep/refinenet-pytorch Update notes  23 Dec 2016:  We did a major update of our code. (new!) 13 Feb 2018:  Multi-scale prediction and evaluation code are added. We add demo files for multi-scale prediction, fusion and evaluation. Please refer to the Testing section below for more details. New models available: trained models using improved residual pooling. Available for these datasets: NYUDv2, Person_Parts, PASCAL_Context, SUNRGBD, ADE20k. These models will give better performance than the reported results in our CVPR paper. New models available: trained models using ResNet-152 for all 7 datasets. Apart from ResNet-101 based models, our ResNet-152 based models of all 7 datasets are now available for download. Updated trained model for VOC2012: this updated model is slightly better than the previous one. We previously uploaded a wrong model. All models are now available in Google Drive and Baidu Pan. More details are provided on testing, training and implementation. Please refer to Important notes in each section below.    Results  Results on the CityScapes Dataset (single scale prediction using ResNet-101 based RefineNet)   Trained models  (new!) Trained models for the following datasets are available for download.   PASCAL VOC 2012 Cityscapes NYUDv2 Person_Parts PASCAL_Context SUNRGBD ADE20k   Downloads for the above datasets. Put the downloaded models in ./model_trained/  (new!) RefineNet models using ResNet-101: Google Drive or Baidu Pan (new!) RefineNet models using ResNet-152: Google Drive or Baidu Pan   Important notes:  For the test set performance of our method on the dataset PASCAl VOC and Cityscapes, kindly note that we do not use any images in the validation set for training. Our models are trained only using the training set images. The trained models of the the following datasets are using improved residual pooling: NYUDv2, Person_Parts, PASCAL_Context, SUNRGBD, ADE20k. These models will give better performance than the reported results in our CVPR paper. Please also refer to the Network architecture section below for more details about improved pooling. The model for VOC2012 is updated. We previously uploaded a wrong model.    Network architecture and implementation  You can find the network graphs that illustrate our architecture in the folder net_graphs. Please refer to our paper for more details. We include in this folder the details of improved residual pooling which improves the residual pooling block described in our CVPR paper. Important notes:  In our up-sampling and fusion layer, we simply use down-sampling for gradient back-propagation. Please refer to the implementation of our fusion layer for details: My_sum_layer.m. please refer to our training demo files for more details on implementation    Installation   Install MatConvNet and CuDNN. We have modified MatConvNet for our task. A modified copy of MatConvNet is provided in ./lib/. You need to compile the provided MatConvNet before running. Details of this modification and compiling can be found in main/my_matconvnet_resnet/README.md.   An example script for exporting lib paths is main/my_matlab.sh   Download the following ImageNet pre-trained models and place them in ./model_trained/:  imagenet-resnet-50-dag, imagenet-resnet-101-dag, imagenet-resnet-152-dag.  They can be downloaded from: MatConvNet, we also have a copy in Google Drive, Baidu Pan.   Testing 1. Multi-scale prediction and evaluation (new!)   First download the trained models and put them in ./model_trained/. Please refer to the above section Trained Models.   Then refer to the below example scripts for prediction on your images:  demo_predict_mscale_[dataset name].m e.g., demo_predict_mscale_voc.m, demo_predict_mscale_nyud, demo_predict_mscale_person_parts    You may need to carefully read through the comments in these demo scripts before using.   Important notes:  In the default setting, the example scripts will perform multi-scale prediction and fuse multi-scale results to generate final prediction. The generated masks and scores maps will be saved in your disk.  Note that the score maps are saved in the format of uint8 with values in [0 255]. You need to cast them into double and normalize into [0 1] if you want to use them. The above demo files are able to perform multi-scale prediction and evaluation (e.g., in terms of IoU scores) in a single run. However, in the default setting, the performance evaluation part is disabled. Please refer to the comments in the demo files to turn on the performance evaluation. Trained models using improved residual pooling will give better performance than the reported results in our CVPR paper. Please refer to the above section Trained models for more details. For the images from NYUDv2 dataset, you may need to remove the white borders of the images before applying our models. More details and crop tools can be found in the NYUDv2 dataset webpage.    2. Single scale prediction and evaluation   Single scale prediction and evaluation can be done by changing the scale setting in the multi-scale prediction demo files. Please refer the the above section for multi-scale prediction.   We also provide simplified demo files for prediction with much less configurations. They are only for single scale prediction. Examples can be found at: demo_test_simple_voc.m and demo_test_simple_city.m.   3. Evaluation and fusion on saved results (score map files and mask files) (new!)  We provide an example script to perform multi-scale fusion on a number of predictions (score maps) saved in your disk:  demo_fuse_saved_prediction_voc.m : fuse multiple cached predictions to generate the final prediction   We provide an example script to evaluate the prediction masks saved in your disk:  demo_evaluate_saved_prediction_voc.m : evaluate the segmentation performance, e.g., in terms of IoU scores.    Training  The following demo files are provided for training a RefineNet on your own dataset. Please carefully read through the comments in the demo files before using this training code.  demo_refinenet_train.m demo_refinenet_train_reduce_learning_rate.m   Important notes:  We use step-wise policy to reduce learning rate, and more importantly, you need to manually reduce the learning rate during the training stage. The setting of maximum training iteration just serves as a simple example and it should be adapted to your datasets. More details can be found in the comments of the training demo files. We use the improved version of chained pooling in this training code, which may achieve better result than using the above provided models.    Citation If you find the code useful, please cite our work as @inproceedings{Lin:2017:RefineNet,   title = {Refine{N}et: {M}ulti-Path Refinement Networks for High-Resolution Semantic Segmentation},   shorttitle = {RefineNet: Multi-Path Refinement Networks},   booktitle = {CVPR},   author = {Lin, G. and Milan, A. and Shen, C. and Reid, I.},   month = jul,   year = {2017} }           and @article{lin2019refinenet,   title={RefineNet: Multi-Path Refinement Networks for Dense Prediction},   author={Lin, Guosheng and Liu, Fayao and Milan, Anton and Shen, Chunhua and Reid, Ian},   journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},   year={2019},   publisher={IEEE},   doi={10.1109/TPAMI.2019.2893630}, }           License For academic usage, the code is released under the permissive BSD license. For any commercial purpose, please contact the authors. "
63,DeepMotionAIResearch/DenseASPP,317,https://github.com/DeepMotionAIResearch/DenseASPP,"Updated on Aug 11, 2021","DenseASPP for Semantic Segmentation in Street Scenes pdf Introduction Usage 1.  Clone the repository: 2. Download pretrained model: 3. Inference 4. Evaluation the results References      README.md           DenseASPP for Semantic Segmentation in Street Scenes pdf Introduction Semantic image segmentation is a basic street scene understanding task in autonomous driving, where each pixel in a high resolution image is categorized into a set of semantic labels. Unlike other scenarios, objects in autonomous driving scene exhibit very large scale changes, which poses great challenges for high-level feature representation in a sense that multi-scale information must be correctly encoded. To remedy this problem, atrous convolution[2, 3] was introduced to generate features with larger receptive fields without sacrificing spatial resolution. Built upon atrous convolution, Atrous Spatial Pyramid Pooling (ASPP)[3] was proposed to concatenate multiple atrous-convolved features using different dilation rates into a final feature representation. Although ASPP is able to generate multi-scale features, we argue the feature resolution in the scale-axis is not dense enough for the autonomous driving scenario. To this end, we propose Densely connected Atrous Spatial Pyramid Pooling (DenseASPP), which connects a set of atrous convolutional layers in a dense way, such that it generates multi-scale features that not only cover a larger scale range, but also cover that scale range densely, without significantly increasing the model size. We evaluate DenseASPP on the street scene benchmark Cityscapes[4] and achieve state-of-the-art performance. Usage 1.  Clone the repository: git clone https://github.com/DeepMotionAIResearch/DenseASPP.git           2. Download pretrained model: Put the model at the folder weights. We provide some checkpoints to run the code: DenseNet161 based model: GoogleDrive Mobilenet v2 based model: Coming soon. Performance of these checkpoints:    Checkpoint name Multi-scale inference Cityscapes mIOU (val) Cityscapes mIOU (test) File Size     DenseASPP161 False  True 79.9%   80.6 % -   79.5% 142.7 MB   MobileNetDenseASPP False  True 74.5%   75.0 % -   - 10.2 MB    Please note that the performance of these checkpoints can be further improved by fine-tuning. Besides, these models were trained with Pytorch 0.3.1 3. Inference First cd to your code root, then run:  python demo.py  --model_name DenseASPP161 --model_path <your checkpoint path> --img_dir <your img directory>           4. Evaluation the results Please cd to ./utils, then run:  python transfer.py           And eval the results with the official evaluation code of Cityscapes, which can be found at there References   DenseASPP for Semantic Segmentation in Street Scenes Maoke Yang, Kun Yu, Chi Zhang, Zhiwei Li, Kuiyuan Yang. link. In CVPR, 2018.   Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs Liang-Chieh Chen+, George Papandreou+, Iasonas Kokkinos, Kevin Murphy, Alan L. Yuille (+ equal contribution). link. In ICLR, 2015.   DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs Liang-Chieh Chen+, George Papandreou+, Iasonas Kokkinos, Kevin Murphy, and Alan L Yuille (+ equal contribution). link. TPAMI 2017.   The Cityscapes Dataset for Semantic Urban Scene Understanding Cordts, Marius, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, Bernt Schiele. link. In CVPR, 2016. "
64,JackieZhangdx/WeakSupervisedSegmentationList,566,https://github.com/JackieZhangdx/WeakSupervisedSegmentationList,"Updated on Apr 28, 2019","Weakly Supervised Semantic Segmentation list Typical weak supervised segmentation problems 1.Bounding box supervision 2.One-Shot segmentation supervision 3.Image/video label supervision 3.1 Deep activation 3.2 Weakly supervised Detection / Localization(TODO) 4.Other supervision Points Scribbles 5.Close Related or unpublished work      README.md     Weakly Supervised Semantic Segmentation list This repository contains lists of state-or-art weakly supervised semantic segmentation works. Papers and resources are listed below according to supervision types. There are some personal views and notes, just ignore if not interested. Last update 2019/4   Paper list   instance  box  one-shot  others    Resources  some unsupervised segment proposal methods and datasets here. CVPR 2018 Tutorial : WSL web&ppt, Part1 ,Part2 Typical weak supervised segmentation problems    No Supervision Difficulty Domain Core issues     1 Bounding box middle annotated classes transfer learning   2 One-shot segment middle similar objects one-shot learning   3 Image/video label hard annotated classes transfer learning   4 Others n/a n/a n/a    1.Bounding box supervision   Box-driven Class-wise Region Masking and Filling Rate Guided Loss for Weakly Supervised Semantic Segmentation, CVPR 2019   Learning to Segment Every Thing, CVPR 2018 :Learning weight transfer from well-annotated subset, transfer class-specific weights(output layers) from detection and classification branch, based on Mask-RCNN   Pseudo Mask Augmented Object Detection, CVPR 2018 :State-of-art weakly supervised instance segmentation with bounding box annotation. EM optimizes pseudo mask and segmentation parameter like Boxsup. Graphcut on superpixel is employed to refine pseudo mask.   Simple Does It: Weakly Supervised Instance and Semantic Segmentation, CVPR 2017 [web] [ref-code][supp] :Grabcut+(HED bounday) and MCG , train foreground segmentation network directly with generated mask semantic segmentaion, sensitive to env(quality) of training images.   Weakly- and Semi-Supervised Learning of a DCNN for Semantic Image Segmentation, ICCV 2015 :Based on CRF refine, EM seems not work   BoxSup: Exploiting Bounding Boxes to Supervise Convolutional Networks for Semantic Segmentation, ICCV 2015 :Iteratively update parameters and region proposal labels, proposals are selected by network output masks   Deepcut: Object segmentation from bounding box annotations using convolutional neural networks, TMI 2017   Adversarial Learning for Semi-Supervised Semantic Segmentation, BMVC 2018, [code]   2.One-Shot segmentation supervision DAVIS Challenge: http://davischallenge.org/ : Davis17/18(Semi-supervised Video segmentation task), Davis16 is video salient object segmentation without the first frame annotations.             Fast and Accurate Online Video Object Segmentation via Tracking Parts, CVPR 2018(Spotlight) [code] :state-of-art, 82.4%/1.8s 77.9%/0.6s   OSVOS: One-Shot Video Object Segmentation, CVPR 2017 [web][code] :milestone, fine-tuning parent network with the first frame mask, 79.8%/10s   3.Image/video label supervision   Self-produced Guidance for Weakly-supervised Object Localization, ECCV 2018   Convolutional Simplex Projection Network (CSPN) for Weakly Supervised Semantic Segmentation, BMVC 2018   Weakly Supervised Instance Segmentation using Class Peak Response, CVPR 2018(Spotlight) :state-of-art practice for instance seg with only class label.      Weakly-Supervised Semantic Segmentation by Iteratively Mining Common Object Features, CVPR 2018 :Superpixel-> RegionNet(RoI classfier)-> Saliency refine, iteratively update with PixelNet(FCN)   Revisiting Dilated Convolution: A Simple Approach for Weakly- and SemiSupervised Semantic Segmentation, CVPR 2018(Spotlight)   Weakly-Supervised Semantic Segmentation Network With Deep Seeded Region Growing, CVPR 2018 [web][code]   Adversarial Complementary Learning for Weakly Supervised Object Localization, CVPR 2018   Learning Pixel-level Semantic Affinity with Image-level Supervision for Weakly Supervised Semantic Segmentation, CVPR 2018   Multi-Evidence Filtering and Fusion for Multi-Label Classification, Object Detection and Semantic Segmentation Based on Weakly Supervised Learning, CVPR 2018   Weakly Supervised Semantic Segmentation using Web-Crawled Videos, CVPR 2017(Spotlight) [web]   Object Region Mining with Adversarial Erasing: A Simple Classification to Semantic Segmentation Approach, CVPR 2017   WILDCAT: Weakly Supervised Learning of Deep ConvNets for Image Classification, Pointwise Localization and Segmentation, CVPR 2017 [web][code]   Learning random-walk label propagation for weakly-supervised semantic segmentation, CVPR 2017(Oral)   Combining Bottom-Up, Top-Down, and Smoothness Cues for Weakly Supervised Image Segmentation, CVPR 2017   Weakly Supervised Semantic Segmentation Using Superpixel Pooling Network, AAAI 2017   Learning from Weak and Noisy Labels for Semantic Segmentation, PAMI 2017   Learning to Segment Human by Watching YouTube, PAMI 2017   Seed, Expand, Constrain: Three Principles for Weakly-Supervised Image Segmentation, ECCV 2016 [code]   Backtracking ScSPM Image Classifier for Weakly Supervised Top-down Saliency, CVPR 2016, TIP 2018 Version   Constrained Convolutional Neural Networks for Weakly Supervised Segmentation, ICCV 2015 [code]   From Image-level to Pixel-level Labeling with Convolutional Networks, CVPR 2015   Resource  Yunchao Wei talk in Chinese about WSL with image label  Arxiv paper   Learning to Exploit the Prior Network Knowledge for Weakly-Supervised Semantic Segmentation, Arxiv1804   Weakly Supervised Object Discovery by Generative Adversarial & Ranking Networks, Arxiv 1711   3.1 Deep activation    Propagate method Papers     Global Max Pooling(GMP) Is object localization for free? - Weakly-supervised learning with convolutional neural networks,CVPR 2015   Global Average Pooling(GAP) Learning Deep Features for Discriminative Localization CVPR 2016   Log-sum-exponential Pooling(LSE) ProNet: Learning to Propose Object-specific Boxes for Cascaded Neural Networks,CVPR 2016   Global Weighted Rank Pooling(GWRP) SEC ECCV 2016   Global rank Max-Min Pooling(GRP) WILDCAT, CVPR 2017    3.2 Weakly supervised Detection / Localization(TODO)   PCL: Proposal Cluster Learning for Weakly Supervised Object Detection, PAMI 2018 [code]   Weakly Supervised Region Proposal Network and Object Detection, ECCV 2018   TS2C: Tight Box Mining with Surrounding Segmentation Context for Weakly Supervised Object Detection, ECCV 2018   Zigzag Learning for Weakly Supervised Object Detection, CVPR 2018   W2F: A Weakly-Supervised to Fully-Supervised Framework for Object Detection, CVPR 2018   Generative Adversarial Learning Towards Fast Weakly Supervised Detection, CVPR 2018   Min-Entropy Latent Model for Weakly Supervised Object Detection, CVPR 2018 , PAMI19, [code]   Weakly Supervised Cascaded Convolutional Networks, CVPR 2017   Multiple Instance Detection Network with Online Instance Classifier Refinement, CVPR 2017 [code]   4.Other supervision Points  Deep Extreme Cut: From Extreme Points to Object Segmentation, CVPR 2018 [web][code] What's the Point: Semantic Segmentation with Point Supervision, ECCV 2016 [web][code]  Scribbles  Normalized Cut Loss for Weakly-supervised CNN Segmentation, CVPR 2018 ScribbleSup: Scribble-Supervised Convolutional Networks for Semantic Segmentation, CVPR 2016 Learning to segment under various forms of weak supervision, CVPR 2015  5.Close Related or unpublished work   Learning to Segment via Cut-and-Paste, Arxiv 1803   WebSeg: Learning Semantic Segmentation from Web Searches, Arxiv1803   On Regularized Losses for Weakly-supervised CNN Segmentation, Arxiv1803   Weakly-Supervised Action Segmentation with Iterative Soft Boundary Assignment, CVPR 2018   Cross-Domain Weakly-Supervised Object Detection through Progressive Domain Adaptation, CVPR 2018   Weakly Supervised Salient Object Detection Using Image Labels, AAAI 2018   Weakly Supervised Object Localization on grocery shelves using simple FCN and Synthetic Dataset, Arxiv 1803   Learning Semantic Segmentation with Diverse Supervision, WACV 2018   If some related works are missed, please kindly notice me by dxzhang@zju.edu.cn "
65,xiaoyufenfei/Real-Time-Semantic-Segmentation,131,https://github.com/xiaoyufenfei/Real-Time-Semantic-Segmentation,"Updated on Dec 10, 2019","Real-time network  for mobile devices awesome-image-classification Google Megvill Microsoft Research Others Papers for real-time semantic segmentation Some useful links & repo      README.md           Real-time network  for mobile devices awesome-image-classification Google Learning Transferable Architectures for Scalable Image Recognition NASNet MnasNet: Platform-Aware Neural Architecture Search for Mobile MnasNet MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications MobileNets MobileNetV2: Inverted Residuals and Linear Bottlenecks MobileNetV2 MobileNetV2-pytorch) Searching for Efficient Multi-Scale Architectures for Dense Image Prediction Megvill ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices ShuffleNet ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design ShuffleNet V2 Shufflenet-v2-Pytorch Microsoft Research Interleaved Group Convolutions for Deep Neural Networks IGCV IGCV2: Interleaved Structured Sparse Convolutional Neural Networks IGCV2 IGCV3: Interleaved Low-Rank Group Convolutions for Efficient Deep Neural Networks IGCV3 Accelerating Deep Neural Networks with Spatial Bottleneck Modules Others CondenseNet: An Efficient DenseNet using Learned Group Convolutions CondenseNet ChamNet: Towards Efficient Network Design through Platform-Aware Model AdaptationChamNet Papers for real-time semantic segmentation  A Comparative Study of Real-time Semantic Segmentation for Autonomous Driving Analysis of efficient CNN design techniques for semantic segmentation Real-time Semantic Image Segmentation via Spatial Sparsity   arxiv2017 ENet: A Deep Neural Network Architecture for Real-Time Semantic Segmentation ENet ICNet for Real-Time Semantic Segmentation on High-Resolution Images ICNet Speeding up Semantic Segmentation for Autonomous Driving SQNet Efficient ConvNet for Real-time Semantic Segmentation EConvNet ERFNet: Efficient Residual Factorized ConvNet for Real-time Semantic Segmentation ERFNet Efficient Dense Modules of Asymmetric Convolution for Real-Time Semantic SegmentationEDANet ESPNet: Efficient Spatial Pyramid of Dilated Convolutions for Semantic Segmentation ESPNet ESPNetv2: A Light-weight, Power Efficient, and General Purpose Convolutional Neural Network ESPNetv2 code Concentrated-Comprehensive Convolutions for lightweight semantic segmentation CCC-ERFNet BiSeNet: Bilateral Segmentation Network for Real-time Semantic Segmentation BiSeNet Light-Weight RefineNet for Real-Time Semantic SegmentationLight-Weight RefineNet code ShelfNet for Real-time Semantic Segmentation arxiv2018 code LadderNet: MULTI-PATH NETWORKS BASED ON U-NET FOR MEDICAL IMAGE SEGMENTATION LadderNet ShuffleSeg: REAL-TIME SEMANTIC SEGMENTATION NETWORK ShuffleSeg RTSeg: REAL-TIME SEMANTIC SEGMENTATION COMPARATIVE STUDY RTSeg ContextNet: Exploring Context and Detail for Semantic Segmentation in Real-time ContextNet CGNet: A Light-weight Context Guided Network for Semantic Segmentation arxiv2018 code Design of Real-time Semantic Segmentation Decoder for Automated Driving arxiv2019 DSNet: DSNet for Real-Time Driving Scene Semantic Segmentation DSNet Fast-SCNN: Fast Semantic Segmentation Network arxiv2019 An efficient solution for semantic segmentation: ShuffleNet V2 with atrous separable convolutions arxiv2019  Some useful links & repo   Awesome-model-compression-and-acceleration   awesome-model-compression-and-acceleration   real-time-network   ConvNets   awesome-computer-vision-models   fast_portrait_segmentation   Light_CNN   read--paper--list   awesome--computer--vision--models   Semantic-Segmentation-Paper-Record   Segmentation-Paper-Reading-Notes   CV-News   research-method   segmentation-paper-reading-notes   CV-Papers-Datasets   useful-computer-vision-phd-resources   AI_Projects   awesome-embedded-ai   Segmentation.X   SemanticSegmentation_DL   CV_resources   无人驾驶资源收集   无人驾驶相关论文速递   AI papers   Paper-Notes "
66,rstrudel/segmenter,356,https://github.com/rstrudel/segmenter,Updated 3 days ago,"Segmenter: Transformer for Semantic Segmentation Installation Model Zoo ADE20K Pascal Context Cityscapes Inference Train Logs Attention Maps Video Segmentation BibTex Acknowledgements      README.md           Segmenter: Transformer for Semantic Segmentation  Segmenter: Transformer for Semantic Segmentation by Robin Strudel*, Ricardo Garcia*, Ivan Laptev and Cordelia Schmid, ICCV 2021. *Equal Contribution Installation Define os environment variables pointing to your checkpoint and dataset directory, put in your .bashrc: export DATASET=/path/to/dataset/dir          Install PyTorch 1.9 then pip install . at the root of this repository. To download ADE20K, use the following command: python -m segm.scripts.prepare_ade20k $DATASET          Model Zoo We release models with a Vision Transformer backbone initialized from the improved ViT models. ADE20K Segmenter models with ViT backbone:   Name mIoU (SS/MS) # params Resolution FPS Download   Seg-T-Mask/16 38.1 / 38.8 7M 512x512 52.4 model config log   Seg-S-Mask/16 45.3 / 46.9 27M 512x512 34.8 model config log   Seg-B-Mask/16 48.5 / 50.0 106M 512x512 24.1 model config log   Seg-B/8 49.5 / 50.5 89M 512x512 4.2 model config log   Seg-L-Mask/16 51.8 / 53.6 334M 640x640 - model config log   Segmenter models with DeiT backbone:   Name mIoU (SS/MS) # params Resolution FPS Download   Seg-B†/16 47.1 / 48.1 87M 512x512 27.3 model config log   Seg-B†-Mask/16 48.7 / 50.1 106M 512x512 24.1 model config log   Pascal Context   Name mIoU (SS/MS) # params Resolution FPS Download   Seg-L-Mask/16 58.1 / 59.0 334M 480x480 - model config log   Cityscapes   Name mIoU (SS/MS) # params Resolution FPS Download   Seg-L-Mask/16 79.1 / 81.3 322M 768x768 - model config log   Inference Download one checkpoint with its configuration in a common folder, for example seg_tiny_mask. You can generate segmentation maps from your own data with: python -m segm.inference --model-path seg_tiny_mask/checkpoint.pth -i images/ -o segmaps/          To evaluate on ADE20K, run the command: # single-scale evaluation: python -m segm.eval.miou seg_tiny_mask/checkpoint.pth ade20k --singlescale # multi-scale evaluation: python -m segm.eval.miou seg_tiny_mask/checkpoint.pth ade20k --multiscale          Train Train Seg-T-Mask/16 on ADE20K on a single GPU: python -m segm.train --log-dir seg_tiny_mask --dataset ade20k \   --backbone vit_tiny_patch16_384 --decoder mask_transformer          To train Seg-B-Mask/16, simply set vit_base_patch16_384 as backbone and launch the above command using a minimum of 4 V100 GPUs (~12 minutes per epoch) and up to 8 V100 GPUs (~7 minutes per epoch). The code uses SLURM environment variables. Logs To plot the logs of your experiments, you can use python -m segm.utils.logs logs.yml          with logs.yml located in utils/ with the path to your experiments logs: root: /path/to/checkpoints/ logs:   seg-t: seg_tiny_mask/log.txt   seg-b: seg_base_mask/log.txt          Attention Maps To visualize the attention maps for Seg-T-Mask/16 encoder layer 0 and patch (0, 21), you can use: python -m segm.scripts.show_attn_map seg_tiny_mask/checkpoint.pth \ images/im0.jpg output_dir/ --layer-id 0 --x-patch 0 --y-patch 21 --enc          Different options are provided to select the generated attention maps:  --enc or --dec: Select encoder or decoder attention maps respectively. --patch or --cls: --patch generates attention maps for the patch with coordinates (x_patch, y_patch). --cls combined with --enc generates attention maps for the CLS token of the encoder. --cls combined with --dec generates maps for each class embedding of the decoder. --x-patch and --y-patch: Coordinates of the patch to draw attention maps from. This flag is ignored when --cls is used. --layer-id: Select the layer for which the attention maps are generated.  For example, to generate attention maps for the decoder class embeddings, you can use: python -m segm.scripts.show_attn_map seg_tiny_mask/checkpoint.pth \ images/im0.jpg output_dir/ --layer-id 0 --dec --cls          Attention maps for patch (0, 21) in Seg-L-Mask/16 encoder layers 1, 4, 8, 12 and 16:  Attention maps for the class embeddings in Seg-L-Mask/16 decoder layer 0:  Video Segmentation Zero shot video segmentation on DAVIS video dataset with Seg-B-Mask/16 model trained on ADE20K.         BibTex @article{strudel2021,   title={Segmenter: Transformer for Semantic Segmentation},   author={Strudel, Robin and Garcia, Ricardo and Laptev, Ivan and Schmid, Cordelia},   journal={arXiv preprint arXiv:2105.05633},   year={2021} }           Acknowledgements The Vision Transformer code is based on timm library and the semantic segmentation training and evaluation pipeline is using mmsegmentation. "
67,upul/Semantic_Segmentation,62,https://github.com/upul/Semantic_Segmentation,"Updated on Apr 3, 2019","Semantic Segmentation using a Fully Convolutional Neural Network Introduction How to Train the Model Network Architecture The KITTI dataset Training the Model Conclusion      README.md           Semantic Segmentation using a Fully Convolutional Neural Network Introduction This repository contains a set of python scripts to train and test semantic segmentation using a fully convolutional neural network. The semantic segmentation network is based on the paper described by Jonathan Long et al. How to Train the Model  Since the network uses VGG-16 weights, first, you have to download VGG-16 pre-trained weights from https://www.cs.toronto.edu/~frossard/vgg16/vgg16_weights.npz and save in the the pretrained_weights folder. Download KITTI dataset and save it in the data/data_road folder. Next, open a command window and type python fcn.py and hit the enter key.  Please note that training checkpointing will be saved to checkpoints/kitti folder and logs will be saved to graphs/kitti folder. So by using tensorboard --logdir=graphs/kitti command, you can start tensorboard to inspect the training process. Following images show sample output we obtained with the trained model.     Network Architecture We implement the FCN-8s model described in the paper by Jonathan Long et al. Following figure shows the architecture of the network. We generated this figure using TensorBoard.  Additionally, we would like to describe main functionalities of the python scripts of this repository in the following table.    Script Description     fcn.py This is the main script of the repository. The key methods of this script are:build, optimize and inference. The build method load pre-trained weights and build the network. The optimize method does the training and inference is used for testing with new images.   loss.py The script contains the loss function we optimize during the training.   helper.py This script contains some useful utility function for generating training and testing batches.   model_utils.py This script contains some useful utility functions to building fully convolutional network using VGG-16 pre-trained weights.    The KITTI dataset For training the semantic segmentation network, we used the KITTI dataset. The dataset consists of 289 training and 290 test images. It contains three different categories of road scenes:  uu - urban unmarked (98/100) um - urban marked (95/96) umm - urban multiple marked lanes (96/94)  Training the Model When it comes to training any deep learning algorithm, selecting suitable hyper-parameters play a big role. For this project, we carefully select following hyper-parameters    Parameter Value Description     Learning Rate 1e-5 We used Adam optimizer and normally 1e-3 or 1e-4 is the suggested learning rate. However, when we were experimenting with different learning rates we found out that 1e-5 works better than above values.   Number of epochs 25 The training dataset is not too big and it has only 289 training examples. Hence, we use a moderate number of epochs.   Batch Size 8 Based on the size of the training dataset, we selected batch size of 8 images.    The following image shows how the training loss changes when we train the model.  Conclusion In this project, we investigated how to use a fully convolutional neural network for semantic segmentation. We tested our model against KITTI dataset. The results indicate that our model is quite capable of separating road pixels form the rest. However, we would like to work on following additional ta to increase the accuracy of our model.  Data Augmentation: During our testing, we have found that our mode failed to label road surface when inadequate lighting in the environment. We think data augmentation can be used to generate more training examples with different lighting conditions. So additional data generated using data augmentation will help us to overcome the above-mentioned issue. "
68,martinkersner/train-DeepLab,171,https://github.com/martinkersner/train-DeepLab,"Updated on Feb 18, 2017","Train DeepLab for Semantic Image Segmentation Prerequisites Install DeepLab caffe Compile DenseCRF Strong annotations Dataset Data conversions Connect $DATASETS into $DEEPLAB Download necessary files for training Training with all classes Plotting training information Training with only 3 classes Evaluation Visual results Weak annotations Dataset Create subsets Training Evaluation Note FAQ      README.md           Train DeepLab for Semantic Image Segmentation Martin Kersner, m.kersner@gmail.com This repository contains scripts for training DeepLab for Semantic Image Segmentation using strongly and weakly annotated data. Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs and Weakly- and Semi-Supervised Learning of a DCNN for Semantic Image Segmentation papers describe training procedure using strongly and weakly annotated data, respectively. git clone --recursive https://github.com/martinkersner/train-DeepLab.git          In following tutorial we use couple of shell variables in order to reproduce the same results without any obtacles.  $DEEPLAB denotes the main directory where repository is checked out $DATASETS denotes path to directory where all necessary datasets are stored $LOGNAME denotes name of log file stored in $DEEPLAB/exper/voc12/log directory $DOWNLOADS denotes directory where downloaded files are stored  Prerequisites  matio  Install DeepLab caffe You should follow instructions for installation. However, if you have already fulfilled all necessary dependencies running following commands from code/ directory should do the job. cd $DEEPLAB/code cp Makefile.config.example Makefile.config # Adjust Makefile.config (for example, if using Anaconda Python, or if cuDNN is desired) make all make pycaffe make test # NOT mandatory make runtest # NOT mandatory          Compile DenseCRF Go to $DEEPLAB/code/densecrf directory, modify Makefile if necessary and run make command. Or you can run following commands in sequential order. cd $DEEPLAB/code/densecrf # Adjust Makefile if necessary make          Strong annotations In this part of tutorial we train DCNN for semantic image segmentation using PASCAL VOC dataset with all 21 classes and also with limited number of them. As a training data we use only strong annotations (pixel level labels). Dataset All necessary data for training are listed in $DEEPLAB/exper/voc12/list/original. Training scripts are prepared to employ either PASCAL VOC 2012 dataset or augmented PASCAL VOC dataset which contains more images. # augmented PASCAL VOC cd $DATASETS wget http://www.eecs.berkeley.edu/Research/Projects/CS/vision/grouping/semantic_contours/benchmark.tgz # 1.3 GB tar -zxvf benchmark.tgz mv benchmark_RELEASE VOC_aug  # original PASCAL VOC 2012 wget http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar # 2 GB tar -xvf VOCtrainval_11-May-2012.tar mv VOCdevkit/VOC2012 VOC2012_orig && rm -r VOCdevkit          Data conversions Unfortunately, ground truth labels within augmented PASCAL VOC dataset are distributed as Matlab data files, therefore we will have to convert them before we can start training itself. cd $DATASETS/VOC_aug/dataset mkdir cls_png cd $DEEPLAB ./mat2png.py $DATASETS/VOC_aug/dataset/cls $DATASETS/VOC_aug/dataset/cls_png          Caffe softmax loss function can accept only one-channel ground truth labels. However, those labels in original PASCAL VOC 2012 dataset are defined as RGB images. Thus, we have to reduce their dimensionality. cd $DATASETS/VOC2012_orig mkdir SegmentationClass_1D  cd $DEEPLAB ./convert_labels.py $DATASETS/VOC2012_orig/SegmentationClass/ \   $DATASETS/VOC2012_orig/ImageSets/Segmentation/trainval.txt \   $DATASETS/VOC2012_orig/SegmentationClass_1D/          At last, part of code which computes DenseCRF is able to work only with PPM image files, hence we have to perform another conversion. This step is necessary only if we want to use DenseCRF separately and as one of Caffe layers. cd $DEEPLAB  # augmented PASCAL VOC mkdir $DATASETS/VOC_aug/dataset/img_ppm ./jpg2ppm.sh $DATASETS/VOC_aug/dataset/img $DATASETS/VOC_aug/dataset/img_ppm  # original PASCAL VOC 2012 mkdir $DATASETS/VOC2012_orig/PPMImages ./jpg2ppm.sh $DATASETS/VOC2012_orig/JPEGImages $DATASETS/VOC2012_orig/PPMImages          Connect $DATASETS into $DEEPLAB Then we create symbolic links to training images and ground truth labels. mkdir -p $DEEPLAB/exper/voc12/data cd $DEEPLAB/exper/voc12/data  # augmented PASCAL VOC ln -s $DATASETS/VOC_aug/dataset/img images_aug ln -s $DATASETS/VOC_aug/dataset/cls_png labels_aug ln -s $DATASETS/VOC_aug/dataset/img_ppm images_aug_ppm  # original PASCAL VOC 2012 ln -s $DATASETS/VOC2012_orig/JPEGImages images_orig ln -s $DATASETS/VOC2012_orig/SegmentationClass_1D labels_orig ln -s $DATASETS/VOC2012_orig/PPMImages images_orig_ppm          Download necessary files for training Before the first training we have to download several files. Using the command below we download initialization model, definition its network and solver. It will also setup symbolic links in directories where those files are later expected during training. ./get_DeepLab_LargeFOV_voc12_data.sh          In order to easily switch between datasets we will modify image lists appropriately. ./prepare_voc12_data_lists.sh          Training with all classes run_pascal_strong.sh can go through 4 different phases (twice training, twice testing), but I wouldn't recommend to run testing phases using this script. Actually, they are currently disabled. At lines 27 through 30, any of phase can be enabled (value 1) or disabled (value 0). Finally, we can start training. ./run_pascal_strong.sh          Plotting training information Training script generates information which are printed to terminal and also stored in $DEEPLAB/exper/voc12/log/DeepLab-LargeFOV/ directory. For every printed iteration there are displayed loss and three different model evalutation metrics for currently employed batch. They denote pixel accuracy, average recall and average Jacard index, respectively. Even though those values are retrievd from training data, they possess important information about training and using the script below we can plot them as a graph. The script generates two graphs evaluation.png and loss.png. cd $DEEPLAB ./loss_from_log.py exper/voc12/log/DeepLab-LargeFOV/`ls -t exper/voc12/log/DeepLab-LargeFOV/ | head -n 1` # for the newest log #./loss_from_log.py exper/voc12/log/DeepLab-LargeFOV/$LOGNAME # specified log          Training with only 3 classes If we want to train with limited number of classes we have to modify ground truth labels and also list of images that can be exploited for training. In filter_images.py at line 17 are specified classes that we are interested in (defaultly bird, bottle and chair). # augmented PASCAL VOC mkdir -p $DATASETS/VOC_aug/dataset/cls_sub_png cd $DEEPLAB/exper/voc12/data/ ln -s $DATASETS/VOC_aug/dataset/cls_sub_png labels_sub_aug find exper/voc12/data/labels_aug/ -printf '%f\n' | sed 's/\.png//'  | tail -n +2 > all_aug_data.txt python filter_images.py $DATASETS/VOC_aug/dataset/cls_png/ $DATASETS/VOC_aug/dataset/cls_sub_png/ all_aug_data.txt sub_aug_data.txt  # original PASCAL VOC 2012 mkdir -p $DATASETS/VOC2012_orig/SegmentationClass_sub_1D cd $DEEPLAB/exper/voc12/data/ ln -s $DATASETS/VOC2012_orig/SegmentationClass_sub_1D labels_sub_orig find exper/voc12/data/labels_orig/ -printf '%f\n' | sed 's/\.png//'  | tail -n +2 > all_orig_data.txt python filter_images.py $DATASETS/VOC2012_orig/SegmentationClass_1D/ $DATASETS/VOC2012_orig/SegmentationClass_sub_1D/ all_orig_data.txt sub_orig_data.txt  ./filter_lists.sh          The number of classes that we plan to use is set at lines 13 and 14 in run_pascal_strong.sh. This number should be always higher by 1 than number of specified classes in filter_images.py script, because we also consider background as one of classes. After, we can proceed to training. ./run_pascal_strong.sh          We can also use the same script for plotting training information. Evaluation     phase 1 (24,000 iter., no CRF) phase 2 (12,000 iter., no CRF)     pixel accuracy 0.8315 0.8523   mean accuracy 0.6807 0.6987   mean IU 0.6725 0.6937   frequency weighted IU 0.8182 0.8439    Visual results Employed model was trained without CRF in phase 1 (24,000 iterations) and then in phase 2 (12,000 iterations), but results here exploited DENSE_CRF layer. Displayed images (bird: 2010_004994, bottle: 2007_000346, chair: 2008_000673) are part of validation dataset stored in $DEEPLAB/exper/voc12/list_subset/val.txt. Colors of segments differ from original ground truth labels because employed model was trained only for 3 classes + background.      Weak annotations In a case we don't possess enough training data, weakly annotated ground truth labels can be exploited using DeepLab. Dataset At first you should download SegmentationClassBboxRect_Visualization.zip and SegmentationClassBboxSeg_Visualization.zip from link https://ucla.app.box.com/s/laif889j7pk6dj04b0ou1apm2sgub9ga and run commands below to prepare data for use. cd $DOWNLOADS mv SegmentationClassBboxRect_Visualization.zip $DATASETS/VOC_aug/dataset/ mv SegmentationClassBboxSeg_Visualization.zip $DATASETS/VOC_aug/dataset/  cd $DATASETS/VOC_aug/dataset unzip SegmentationClassBboxRect_Visualization.zip unzip SegmentationClassBboxSeg_Visualization.zip  mv SegmentationClassBboxAug_Visualization/ SegClassBboxAug_RGB mv SegmentationClassBboxErode20CRFAug_Visualization/ SegClassBboxErode20CRFAug_RGB          Downloaded weak annotations were created using Matlab and because of that labels are sometimes stored with one channel and other times with three channels. Similarly to strong annotations we have to convert all labels to the same one channel format. In order to cope with it I recommend you to use Matlab script convert_weak_labels.m (if anybody knows how to perform the same conversion using python I would be really interested) which is stored in $DEEPLAB directory. Before running script you have to specify path to datasets on line 3. After script successfully finished we have to create symbolic links to be able to reach data during training. cd $DEEPLAB/exper/voc12/data ln -s $DATASETS/VOC_aug/dataset/SegClassBboxAug_1D/ labels_bbox ln -s $DATASETS/VOC_aug/dataset/SegClassBboxErode20CRFAug_1D/ labels_bboxcrf          Create subsets Training DeepLab using weak labels enables to employ datasets of different sizes. Following snippet creates those subsets of strong dataset and also necessary training lists with weak labels. cd $DEEPLAB ./create_weak_lists.sh  cd $DEEPLAB/exper/voc12/list head -n 200  train.txt > train200.txt head -n 500  train.txt > train500.txt head -n 750  train.txt > train750.txt head -n 1000 train.txt > train1000.txt  cp train_bboxcrf.txt trainval_bboxcrf.txt cp train_bbox.txt trainval_bbox.txt          Training Training using weak annotations is similar to exploiting strong annotations. The only difference is the name of script which should be run. ./run_pascal_weak.sh          Plotting is also same as for strong annotations. Evaluation 5000 weak annotations and 200 strong annotations     phase 1 (6,000 iter., no CRF) phase 2 (8,000 iter., no CRF)     pixel accuracy 0.8688 0.8671   mean accuracy 0.7415 0.750   mean IU 0.6324 0.6343   frequency weighted IU 0.7962 0.7951    Note Init models are modified VGG-16 networks with changed kernel size from 7x7 to 4x4 or 3x3. There are two models that can be employed for initialization: vgg16_128, vgg16_20M. The first fully connected layer of vgg16_128 has kernel size 4x4 and 4096 filters. It can be used for DeepLab basic model. In vgg16_20M, the first fully connected layer has kernel size 3x3 and 1024 filters. It can be used for DeepLab-LargeFOV. Currently training is focused on DeepLab-LargeFOV. FAQ At http://ccvl.stat.ucla.edu/deeplab_faq/ you can find frequently asked questions about DeepLab for semantic image segmentation. "
69,bfortuner/pytorch_tiramisu,278,https://github.com/bfortuner/pytorch_tiramisu,"Updated on Feb 2, 2019","One Hundred Layers Tiramisu Setup Dataset Architecture Authors' Results Our Results Training References and Links      README.md           One Hundred Layers Tiramisu PyTorch implementation of The One Hundred Layers Tiramisu: Fully Convolutional DenseNets for Semantic Segmentation. Tiramisu combines DensetNet and U-Net for high performance semantic segmentation. In this repository, we attempt to replicate the authors' results on the CamVid dataset. Setup Requires Anaconda for Python3 installed. conda create --name tiramisu python=3.6 source activate tiramisu conda install pytorch torchvision -c pytorch           The train.ipynb notebook shows a basic train/test workflow. Dataset Download  CamVid Website Download  Specs  Training: 367 frames Validation: 101 frames TestSet: 233 frames Dimensions: 360x480 Classes: 11 (+1 background)  Architecture Tiramisu adopts the UNet design with downsampling, bottleneck, and upsampling paths and skip connections. It replaces convolution and max pooling layers with Dense blocks from the DenseNet architecture. Dense blocks contain residual connections like in ResNet except they concatenate, rather than sum, prior feature maps.  Layers  FCDenseNet103  Authors' Results   Our Results FCDenseNet67 We trained for 670 epochs (224x224 crops) with 100 epochs fine-tuning (full-size images). The authors mention ""global accuracy"" of 90.8 for FC-DenseNet67 on Camvid, compared to our 86.8. If we exclude the 'background' class, accuracy increases to ~89%. We think the authors did this, but haven't confirmed.    Dataset Loss Accuracy     Validation .209 92.5   Testset .435 86.8     FCDenseNet103 We trained for 874 epochs with 50 epochs fine-tuning.    Dataset Loss Accuracy     Validation .178 92.8   Testset .441 86.6     Predictions  Training Hyperparameters  WeightInitialization = HeUniform Optimizer = RMSProp LR = .001 with exponential decay of 0.995 after each epoch Data Augmentation = Random Crops, Vertical Flips ValidationSet with early stopping based on IoU or MeanAccuracy with patience of 100 (50 during finetuning) WeightDecay = .0001 Finetune with full-size images, LR = .0001 Dropout = 0.2 BatchNorm ""we use current batch stats at training, validation, and test time""  References and Links  Project Thread Author's Implementation https://github.com/bamos/densenet.pytorch https://github.com/liuzhuang13/DenseNet "
70,Media-Smart/vedaseg,397,https://github.com/Media-Smart/vedaseg,"Updated on Nov 28, 2021","Introduction Features License Benchmark and model zoo Installation Requirements Install vedaseg Prepare data VOC data COCO data Folder structure Train Test Inference Deploy Contact      README.md           Introduction vedaseg is an open source semantic segmentation toolbox based on PyTorch. Features   Modular Design We decompose the semantic segmentation framework into different components. The flexible and extensible design make it easy to implement a customized semantic segmentation project by combining different modules like building Lego.   Support of several popular frameworks The toolbox supports several popular semantic segmentation frameworks out of the box, e.g. DeepLabv3+, DeepLabv3, U-Net, PSPNet, FPN, etc.   High efficiency Multi-GPU data parallelism & distributed training.   Multi-Class/Multi-Label segmentation We implement multi-class and multi-label segmentation(where a pixel can belong to multiple classes).   Acceleration and deployment Models can be accelerated and deployed with TensorRT.   License This project is released under the Apache 2.0 license. Benchmark and model zoo Note: All models are trained only on PASCAL VOC 2012 trainaug dataset and evaluated on PASCAL VOC 2012 val dataset.    Architecture backbone OS MS & Flip mIOU     DeepLabv3plus ResNet-101 16 True 79.46%   DeepLabv3plus ResNet-101 16 False 77.90%   DeepLabv3 ResNet-101 16 True 79.22%   DeepLabv3 ResNet101 16 False 77.08%   FPN ResNet-101 4 True 77.05%   FPN ResNet-101 4 False 75.64%   PSPNet ResNet-101 8 True 78.39%   PSPNet ResNet-101 8 False 77.30%   PSPNet ResNet_v1c-101 8 True 79.88%   PSPNet ResNet_v1c-101 8 False 78.85%   U-Net ResNet-101 1 True 74.58%   U-Net ResNet-101 1 False 72.59%    OS: Output stride used during evaluation. MS: Multi-scale inputs during evaluation. Flip: Adding horizontal flipped inputs during evaluation. ResNet_v1c: Modified stem from original ResNet, as shown in Figure 2(b) in this paper. Models above are available in the GoogleDrive. Installation Requirements  Linux Python 3.6+ PyTorch 1.4.0 or higher CUDA 9.0 or higher  We have tested the following versions of OS and softwares:  OS: Ubuntu 16.04.6 LTS CUDA: 10.2 PyTorch 1.4.0 Python 3.6.9  Install vedaseg  Create a conda virtual environment and activate it.  conda create -n vedaseg python=3.6.9 -y conda activate vedaseg           Install PyTorch and torchvision following the official instructions, e.g.,  conda install pytorch torchvision -c pytorch           Clone the vedaseg repository.  git clone https://github.com/Media-Smart/vedaseg.git cd vedaseg vedaseg_root=${PWD}           Install dependencies.  pip install -r requirements.txt          Prepare data VOC data Download Pascal VOC 2012 and Pascal VOC 2012 augmented (you can get details at Semantic Boundaries Dataset and Benchmark), resulting in 10,582 training images(trainaug), 1,449 validatation images. cd ${vedaseg_root} mkdir ${vedaseg_root}/data cd ${vedaseg_root}/data  wget http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar wget http://www.eecs.berkeley.edu/Research/Projects/CS/vision/grouping/semantic_contours/benchmark.tgz  tar xf VOCtrainval_11-May-2012.tar tar xf benchmark.tgz  python ../tools/encode_voc12_aug.py python ../tools/encode_voc12.py  mkdir VOCdevkit/VOC2012/EncodeSegmentationClass #cp benchmark_RELEASE/dataset/encode_cls/* VOCdevkit/VOC2012/EncodeSegmentationClass (cd benchmark_RELEASE/dataset/encode_cls; cp * ${vedaseg_root}/data/VOCdevkit/VOC2012/EncodeSegmentationClass) #cp VOCdevkit/VOC2012/EncodeSegmentationClassPart/* VOCdevkit/VOC2012/EncodeSegmentationClass (cd VOCdevkit/VOC2012/EncodeSegmentationClassPart; cp * ${vedaseg_root}/data/VOCdevkit/VOC2012/EncodeSegmentationClass)  comm -23 <(cat benchmark_RELEASE/dataset/{train,val}.txt VOCdevkit/VOC2012/ImageSets/Segmentation/train.txt | sort -u) <(cat VOCdevkit/VOC2012/ImageSets/Segmentation/val.txt | sort -u) > VOCdevkit/VOC2012/ImageSets/Segmentation/trainaug.txt          To avoid tedious operations, you could save the above linux commands as a shell file and execute it. COCO data Download the COCO-2017 dataset. cd ${vedaseg_root} mkdir ${vedaseg_root}/data cd ${vedaseg_root}/data mkdir COCO2017 && cd COCO2017 wget -c http://images.cocodataset.org/zips/train2017.zip unzip train2017.zip && rm train2017.zip wget -c http://images.cocodataset.org/zips/val2017.zip unzip val2017.zip &&  rm val2017.zip wget -c http://images.cocodataset.org/annotations/annotations_trainval2017.zip unzip annotations_trainval2017.zip && rm annotations_trainval2017.zip          Folder structure The folder structure should similar as following: data ├── COCO2017 │   ├── annotations │   │   ├── instances_train2017.json │   │   ├── instances_val2017.json │   ├── train2017 │   ├── val2017 │── VOCdevkit │   │   ├── VOC2012 │   │   │   ├── JPEGImages │   │   │   ├── SegmentationClass │   │   │   ├── ImageSets │   │   │   │   ├── Segmentation │   │   │   │   │   ├── trainaug.txt │   │   │   │   │   ├── val.txt            Train  Config  Modify configuration files in configs/ according to your needs(e.g. configs/voc_unet.py). The major configuration difference between single-label and multi-label training lies in: nclasses, multi_label, metricsand criterion. You can take configs/coco_multilabel_unet.py as a reference. Currently, multi-label training is only supported in COCO data format.  Ditributed training  # train pspnet using GPUs with gpu_id 0, 1, 2, 3 ./tools/dist_train.sh configs/voc_pspnet.py ""0, 1, 2, 3""           Non-distributed training  python tools/train.py configs/voc_unet.py          Snapshots and logs by default will be generated at ${vedaseg_root}/workdir/name_of_config_file(you can specify workdir in config files). Test  Config  Modify configuration as you wish(e.g. configs/voc_unet.py).  Ditributed testing  # test pspnet using GPUs with gpu_id 0, 1, 2, 3 ./tools/dist_test.sh configs/voc_pspnet.py path/to/checkpoint.pth ""0, 1, 2, 3""           Non-distributed testing  python tools/test.py configs/voc_unet.py path/to/checkpoint.pth          Inference  Config  Modify configuration as you wish(e.g. configs/voc_unet.py).  Run  # visualize the results in a new window python tools/inference.py configs/voc_unet.py checkpoint_path image_file_path --show  # save the visualization results in folder which named with image prefix, default under folder './result/' python tools/inference.py configs/voc_unet.py checkpoint_path image_file_path --out folder_name          Deploy  Convert to ONNX  Firstly, install volksdep following the official instructions. Then, run the following code to convert PyTorch to ONNX. The input shape format is CxHxW. If you need the ONNX model with dynamic input shape, please add --dynamic_shape in the end. python tools/torch2onnx.py configs/voc_unet.py weight_path out_path --dummy_input_shape 3,513,513 --opset_version 11          Here are some known issues:  Currently PSPNet model is not supported because of the unsupported operation AdaptiveAvgPool2d. Default ONNX opset version is 9 and PyTorch Upsample operation is only supported with specified size, nearest mode and align_corners being None. If bilinear mode and align_corners are wanted, please add --opset_version 11 when using torch2onnx.py.   Inference SDK  Firstly, install flexinfer and see the example for details. Contact This repository is currently maintained by Yuxin Zou (@YuxinZou), Tianhe Wang(@DarthThomas), Hongxiang Cai (@hxcai), Yichao Xiong (@mileistone). "
71,hellochick/semantic-segmentation-tensorflow,83,https://github.com/hellochick/semantic-segmentation-tensorflow,"Updated on Apr 2, 2018","semantic-segmentation-tensorflow Models ...to be continue Install Inference Arg list Import module in your code: Results ade20k cityscapes Citation      README.md           semantic-segmentation-tensorflow This is a Tensorflow implementation of semantic segmentation models on MIT ADE20K scene parsing dataset and Cityscapes dataset  We re-produce the inference phase of several models, including PSPNet, FCN, and ICNet by transforming the released pre-trained weights into tensorflow format, and apply on handcraft models. Also, we refer to ENet from freg856 github. Still working on task integrated. Models  PSPNet FCN ENet ICNet  ...to be continue Install Get corresponding transformed pre-trained weights, and put into model directory:    FCN PSPNet ICNet     Google drive Google drive Google drive    Inference Run following command: python inference.py --img-path /Path/To/Image --dataset Model_Type           Arg list --model - choose from ""icnet""/""pspnet""/""fcn""/""enet""           Import module in your code: from model import FCN8s, PSPNet50, ICNet, ENet  model = PSPNet50() # or another model  model.read_input(img_path)  # read image data from path  sess = tf.Session(config=config) init = tf.global_variables_initializer() sess.run(init)  model.load(model_path, sess)  # load pretrained model preds = model.forward(sess) # Get prediction          Results ade20k    Input Image PSPNet FCN                cityscapes    Input Image ICNet ENet           Citation @inproceedings{zhao2017pspnet,   author = {Hengshuang Zhao and             Jianping Shi and             Xiaojuan Qi and             Xiaogang Wang and             Jiaya Jia},   title = {Pyramid Scene Parsing Network},   booktitle = {Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},   year = {2017} }           Scene Parsing through ADE20K Dataset. B. Zhou, H. Zhao, X. Puig, S. Fidler, A. Barriuso and A. Torralba. Computer Vision and Pattern Recognition (CVPR), 2017. (http://people.csail.mit.edu/bzhou/publication/scene-parse-camera-ready.pdf) @inproceedings{zhou2017scene,     title={Scene Parsing through ADE20K Dataset},     author={Zhou, Bolei and Zhao, Hang and Puig, Xavier and Fidler, Sanja and Barriuso, Adela and Torralba, Antonio},     booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},     year={2017} }           Semantic Understanding of Scenes through ADE20K Dataset. B. Zhou, H. Zhao, X. Puig, S. Fidler, A. Barriuso and A. Torralba. arXiv:1608.05442. (https://arxiv.org/pdf/1608.05442.pdf) @article{zhou2016semantic,   title={Semantic understanding of scenes through the ade20k dataset},   author={Zhou, Bolei and Zhao, Hang and Puig, Xavier and Fidler, Sanja and Barriuso, Adela and Torralba, Antonio},   journal={arXiv preprint arXiv:1608.05442},   year={2016} } "
72,twke18/Adaptive_Affinity_Fields,251,https://github.com/twke18/Adaptive_Affinity_Fields,"Updated on Sep 20, 2018","Adaptive Affinity Fields for Semantic Segmentation Prerequisites Required Python Packages Data Preparation ImageNet Pre-Trained Models Training Inference Benchmarking Citation License      README.md           Adaptive Affinity Fields for Semantic Segmentation By Tsung-Wei Ke*, Jyh-Jing Hwang*, Ziwei Liu, and Stella X. Yu (* equal contribution)  Semantic segmentation has made much progress with increasingly powerful pixel-wise classifiers and incorporating structural priors via Conditional Random Fields (CRF) or Generative Adversarial Networks (GAN). We propose a simpler alternative that learns to verify the spatial structure of segmentation during training only. Unlike existing approaches that enforce semantic labels on individual pixels and match labels between neighbouring pixels, we propose the concept of Adaptive Affinity Fields (AAF) to capture and match the semantic relations between neighbouring pixels in the label space. We use adversarial learning to select the optimal affinity field size for each semantic category. It is formulated as a minimax problem, optimizing our segmentation neural network in a best worst-case learning scenario. AAF is versatile for representing structures as a collection of pixel-centric relations, easier to train than GAN and more efficient than CRF without run-time inference. Our extensive evaluations on PASCAL VOC 2012, Cityscapes, and GTA5 datasets demonstrate its above-par segmentation performance and robust generalization across domains. AAF is published in ECCV 2018, see our paper for more details.  Multi-GPU SyncBatchNorm has been released!  Prerequisites  Linux Python2.7 or Python3 (>=3.5) Cuda 8.0 and Cudnn 6  Required Python Packages  tensorflow 1.4 (for versions >= 1.6 might cause OOM error) numpy scipy tqdm PIL opencv  Data Preparation  PASCAL VOC 2012 Cityscapes  ImageNet Pre-Trained Models Download ResNet101.v1 from Tensorflow-Slim. Training  Baseline Models:  python pyscripts/train/train.py            Baseline Models (Multi-GPUs):  python pyscripts/train/train_mgpu.py            Affinity  python pyscripts/train/train_affinity.py            Affinity (Multi-GPUs)  python pyscripts/train/train_affinity_mgpu.py            AAF  python pyscripts/train/train_aaf.py            AAF (Multi-GPUs)  python pyscripts/train/train_aaf_mgpu.py           Inference  Single-Scale Input only  python pyscripts/inference/inference.py            Multi-Scale Inputs and Left-Right Flipping (opencv is required)  python pyscripts/inference/inference_msc.py           Benchmarking  mIoU  python pyscripts/benchmark/benchmark_by_mIoU.py            instance-wise mIoU  python pyscripts/benchmark/benchmark_by_instance.py           See our bash script examples for the corresponding input arguments. Citation If you find this code useful for your research, please consider citing our paper Adaptive Affinity Fields for Semantic Segmentation. @inproceedings{aaf2018,  author = {Ke, Tsung-Wei and Hwang, Jyh-Jing and Liu, Ziwei and Yu, Stella X.},  title = {Adaptive Affinity Fields for Semantic Segmentation},  booktitle = {European Conference on Computer Vision (ECCV)},  month = {September},  year = {2018} }           License AAF is released under the MIT License (refer to the LICENSE file for details). "
73,MVIG-SJTU/pointSIFT,577,https://github.com/MVIG-SJTU/pointSIFT,"Updated on Jul 16, 2019","PointSIFT: A SIFT-like Network Module for 3D Point Cloud Semantic Segmentation Introduction Installation Usage Training and evaluating on ScanNet Citation      README.md           PointSIFT: A SIFT-like Network Module for 3D Point Cloud Semantic Segmentation Created by Mingyang Jiang, Yiran Wu, Tianqi Zhao, Zelin Zhao, Cewu Lu (corresponding author). Introduction PointSIFT is a semantic segmentation framework for 3D point clouds. It is based on a simple module which extract featrues from neighbor points in eight directions. For more details, please refer to our arxiv paper. PointSIFT is freely available for free non-commercial use, and may be redistributed under these conditions. For commercial queries, contact Cewu Lu.  Installation In our experiment, All the codes are tested in Python3.5(If you use Python 2.7, please add some system paths), CUDA 8.0 and CUDNN 5.1.  Install TensorFlow (We use v1.4.1). Install other python libraries like h5py Compile TF operator (Similar to PointNet++).  Firstly, you should find Tensorflow include path and library paths.      import tensorflow as tf     # include path     print(tf.sysconfig.get_include())     # library path     print(tf.sysconfig.get_lib())          Then, change the path in all the complie file, like tf_utils/tf_ops/sampling/tf_sampling_compile.sh Finally, compile the source file, we use tf_sampling as example.     cd tf_utils/tf_ops/sampling     chmod +x tf_sampling_compile.sh     ./tf_sampling_compile.sh          Usage If you want use our model in your own project. After compiling the TF operator, you can import it easily. Here shows a simple case.(we take batch_size * num_point * input_dim as input and get batch_size * num_point * output_dim as output) import tensorflow as tf # import our module from tf_utils.pointSIFT_util import pointSIFT_module # input coordinates xyz = tf.tf.placeholder(tf.float32, shape=(batch_size, num_point, 3)) # input features point_feature = tf.tf.placeholder(tf.float32, shape=(batch_size, num_point, input_dim) # setting phases is_training = tf.placeholder(dtype=tf.bool, shape=()) # setting searching radius (0.1 as an example) radius = 0.1 _, out_feature, _ = pointSIFT_module(xyz, point_feature, radius, output_dim, is_training)          Training and evaluating on ScanNet  All the data can be download from here. They are the same as PointNet++. Train the data:  python train_and_eval_scannet.py          If you have multiple GPU: CUDA_VISIBLE_DEVICES=0,1,2,3 python train_and_eval_scannet.py --gpu_num=4          Citation Please cite the paper in your publications if it helps your research: @misc{1807.00652, Author = {Mingyang Jiang and Yiran Wu and Tianqi Zhao and Zelin Zhao and Cewu Lu}, Title = {PointSIFT: A SIFT-like Network Module for 3D Point Cloud Semantic Segmentation}, Year = {2018}, Eprint = {arXiv:1807.00652}, } "
74,WillBrennan/SemanticSegmentation,86,https://github.com/WillBrennan/SemanticSegmentation,"Updated on Jul 26, 2021","Semantic Segmentation Overview Getting Started Pre-Trained Segmentation Projects Skin Segmentation Pizza Topping Segmentation Cat and Bird Segmentation Training New Projects      readme.md           Semantic Segmentation Overview This project started as a replacement to the Skin Detection project that used traditional computer vision techniques. This project implements two models,  FCNResNet101 from torchvision for accurate segmentation BiSeNetV2 for real-time segmentation  These models are trained with masks from labelme annotations. As labelme annotations allow for multiple categories per a pixel we use multi-label semantic segmentation. Both the accurate and real-time models are in the pretrained directory. Getting Started The pretrained models are stored in the repo with git-lfs, when you clone make sure you've pulled the files by calling, git lfs pull          or by downloading them from github directly. This project uses conda to manage its enviroment; once conda is installed we create the enviroment and activate it, conda env create -f enviroment.yml conda activate semantic_segmentation          . On windows; powershell needs to be initialised and the execution policy needs to be modified. conda init powershell Set-ExecutionPolicy -ExecutionPolicy RemoteSigned -Scope CurrentUser          Pre-Trained Segmentation Projects This project comes bundled with several pretrained models, which can be found in the pretrained directory. To infer segmentation masks on your images run evaluate_images. # to display the output python evaluate_images.py --images ~/Pictures/ --model pretrained/model_segmentation_skin_30.pth --model-type FCNResNet101 --display # to save the output python evaluate_images.py --images ~/Pictures/ --model pretrained/model_segmentation_skin_30.pth --model-type FCNResNet101 --save          To run the real-time models change the --model-type, # to display the output python evaluate_images.py --images ~/Pictures/ --model pretrained/model_segmentation_realtime_skin_30.pth --model-type BiSeNetV2 --display # to save the output python evaluate_images.py --images ~/Pictures/ --model pretrained/model_segmentation_realtime_skin_30.pth --model-type BiSeNetV2 --save          Skin Segmentation This model was trained with a custom dataset of 150 images taken from COCO where skin segmentation annotations were added. This includes a wide variety of skin colours and lighting conditions making it more robust than the Skin Detection project. This model detects,  skin   Pizza Topping Segmentation This was trained with a custom dataset of 89 images taken from COCO where pizza topping annotations were added. There's very few images for each type of topping so this model performs very badly and needs quite a few more images to behave well!  'chilli', 'ham', 'jalapenos', 'mozzarella', 'mushrooms', 'olive', 'pepperoni', 'pineapple', 'salad', 'tomato'   Cat and Bird Segmentation Annotated images of birds and cats were taken from COCO using the extract_from_coco script and then trained on.  cat, birds   Training New Projects To train a new project you can either create new labelme annotations on your images, to launch labelme run, labelme          and start annotating your images! You'll need a couple of hundred. Alternatively if your category is already in COCO you can run the conversion tool to create labelme annotations from them. python extract_from_coco.py --images ~/datasets/coco/val2017 --annotations ~/datasets/coco/annotations/instances_val2017.json --output ~/datasets/my_cat_images_val --categories cat          Once you've got a directory of labelme annotations you can check how the images will be shown to the model during training by running, python check_dataset.py --dataset ~/datasets/my_cat_images_val # to show our dataset with training augmentation python check_dataset.py --dataset ~/datasets/my_cat_images_val --use-augmentation          . If your happy with the images and how they'll appear in training then train the model using, python train.py --train ~/datasets/my_cat_images_train --val ~/datasets/my_cat_images_val --model-tag segmentation_cat --model-type FCNResNet101          . This may take some time depending on how many images you have. Tensorboard logs are available in the logs directory. To run your trained model on a directory of images run # to display the output python evaluate_images.py --images ~/Pictures/my_cat_imgs --model models/model_segmentation_cat_30.pth --model-type FCNResNet101 --display # to save the output python evaluate_images.py --images ~/Pictures/my_cat_imgs --model models/model_segmentation_cat_30.pth --model-type FCNResNet101 --save "
75,sacmehta/ESPNet,492,https://github.com/sacmehta/ESPNet,"Updated on Apr 6, 2020","ESPNet: Efficient Spatial Pyramid of Dilated Convolutions for Semantic Segmentation Sample results Structure of this repository Performance on the CityScape dataset Performance on the CamVid dataset Pre-requisite Citation FAQs Assertion error with class labels (t >= 0 && t < n_classes).      README.md           ESPNet: Efficient Spatial Pyramid of Dilated Convolutions for Semantic Segmentation This repository contains the source code of our paper, ESPNet (accepted for publication in ECCV'18). Sample results Check our project page for more qualitative results (videos). Click on the below sample image to view the segmentation results on YouTube.    Structure of this repository This repository is organized as:  train This directory contains the source code for trainig the ESPNet-C and ESPNet models. test This directory contains the source code for evaluating our model on RGB Images. pretrained This directory contains the pre-trained models on the CityScape dataset  encoder This directory contains the pretrained ESPNet-C models decoder This directory contains the pretrained ESPNet models    Performance on the CityScape dataset Our model ESPNet achives an class-wise mIOU of 60.336 and category-wise mIOU of 82.178 on the CityScapes test dataset and runs at  112 fps on the NVIDIA TitanX (30 fps faster than ENet) 9 FPS on TX2 With the same number of parameters as ENet, our model is 2% more accurate  Performance on the CamVid dataset Our model achieves an mIOU of 55.64 on the CamVid test set. We used the dataset splits (train/val/test) provided here. We trained the models at a resolution of 480x360. For comparison  with other models, see SegNet paper. Note: We did not use the 3.5K dataset for training which was used in the SegNet paper.    Model mIOU Class avg.     ENet 51.3 68.3   SegNet 55.6 65.2   ESPNet 55.64 68.30    Pre-requisite To run this code, you need to have following libraries:  OpenCV - We tested our code with version > 3.0. PyTorch - We tested with v0.3.0 Python - We tested our code with Pythonv3. If you are using Python v2, please feel free to make necessary changes to the code.  We recommend to use Anaconda. We have tested our code on Ubuntu 16.04. Citation If ESPNet is useful for your research, then please cite our paper. @inproceedings{mehta2018espnet,   title={ESPNet: Efficient Spatial Pyramid of Dilated Convolutions for Semantic Segmentation},   author={Sachin Mehta, Mohammad Rastegari, Anat Caspi, Linda Shapiro, and Hannaneh Hajishirzi},   booktitle={ECCV},   year={2018} }           FAQs Assertion error with class labels (t >= 0 && t < n_classes). If you are getting an assertion error with class labels, then please check the number of class labels defined in the label images. You can do this as: import cv2 import numpy as np labelImg = cv2.imread(<label_filename.png>, 0) unique_val_arr = np.unique(labelImg) print(unique_val_arr)           The values inside unique_val_arr should be between 0 and total number of classes in the dataset. If this is not the case, then pre-process your label images. For example, if the label iamge contains 255 as a value, then you can ignore these values by mapping it to an undefined or background class as: labelImg[labelImg == 255] = <undefined class id> "
76,Tramac/Lightweight-Segmentation,282,https://github.com/Tramac/Lightweight-Segmentation,"Updated on Sep 8, 2020","Lightweight Model for Real-Time Semantic Segmentation Usage Train Evaluation Result Support To Do References      README.md           Lightweight Model for Real-Time Semantic Segmentation    This project aims at providing the popular lightweight model implementations for real-time semantic segmentation. Usage  Train  Single GPU training  python train.py --model mobilenet --dataset citys --lr 0.01 --epochs 240            Multi-GPU training  # for example, train mobilenet with 4 GPUs: export NGPUS=4 python -m torch.distributed.launch --nproc_per_node=$NGPUS train.py --model mobilenet --dataset citys --lr 0.01 --epochs 240           Evaluation  Single GPU evaluating  python eval.py --model mobilenet_small --dataset citys            Multi-GPU evaluating  # for example, evaluate mobilenet with 4 GPUs: export NGPUS=4 python -m torch.distributed.launch --nproc_per_node=$NGPUS eval.py --model mobilenet --dataset citys           Result  Cityscapes  Where: crop_size=768, lr=0.01, epochs=80.    Backbone OHEM Params(M) FLOPs(G) CPU(fps) GPU(fps) mIoU/pixACC Model     mobilenet ✘ 5.31 4.48 0.81 77.11 0.463/0.901 GoogleDrive,BaiduCloud(ybsg)   mobilenet ✓ 5.31 4.48 0.81 75.35 0.526/0.909 GoogleDrive,BaiduCloud(u2y2)   mobilenetv2 ✓ 4.88 4.04 0.49 49.40 0.613/0.930 GoogleDrive,BaiduCloud(q2g5)   mobilenetv3_small ✓ 1.02 1.64 2.59 104.56 0.529/0.908 GoogleDrive,BaiduCloud(e7no)   mobilenetv3_large ✓ 2.68 4.59 1.39 79.43 0.584/0.916 GoogleDrive,BaiduCloud(i60c)   shufflenet ✓ 6.89 5.68 0.57 43.79 0.493/0.901 GoogleDrive,BaiduCloud(6fjh)   shufflenetv2 ✓ 5.24 4.33 0.72 57.71 0.528/0.914 GoogleDrive,BaiduCloud(7pi5)   igcv3 ✓ 4.86 4.04 0.34 29.70 0.573/0.923 GoogleDrive,BaiduCloud(qe4f)   efficientnet-b0 ✓ 6.63 2.60 0.33 30.15 0.492/0.903 GoogleDrive,BaiduCloud(phuy)     Improve     Model batch_size epochs crop_size init_weight optimizer mIoU/pixACC     mobilenetv3_small 4 80 768 kaiming_uniform SGD 0.529/0.908   mobilenetv3_small 4 160 768 kaiming_uniform SGD 0.587/0.918   mobilenetv3_small 8 160 768 kaiming_uniform SGD 0.553/0/913   mobilenetv3_small 4 80 1024 kaiming_uniform SGD 0.557/0.914   mobilenetv3_small 4 80 768 xavier_uniform SGD 0.550/0.911   mobilenetv3_small 4 80 768 kaiming_uniform Adam 0.549/0.911   mobilenetv3_small 8 160 1024 xavier_uniform SGD 0.612/0.920    Support  MobileNet MobileNetV2 MobileNetV3 ShuffleNet ShuffleNetV2 IGCV3 EfficientNet  To Do   improve performance  optimize memory  check efficientnet  replace nn.SyncBatchNorm by nn.BatchNorm.convert_sync_batchnorm  check find_unused_parameters in nn.parallel.DistributedDataParallel  References  maskrcnn-benchmark mobilenetv3-segmentation awesome-semantic-segmentation-pytorch "
77,hoya012/semantic-segmentation-tutorial-pytorch,113,https://github.com/hoya012/semantic-segmentation-tutorial-pytorch,"Updated on Nov 16, 2021","Semantic Segmentation Tutorial using PyTorch 0. Experimental Setup 0-1. Prepare Library 0-2. Download dataset (MiniCity from CityScapes) 0-3. Dataset Simple EDA (Exploratory Data Analysis) - Class Distribution, Sample Distribution benchmark class from 0 to 18 class, count labeled pixels deeplab v3 baseline test set result 1. Training Baseline Model 1-1. Loss Functions 1-2. Normalization Layer 1-3. Additional Augmentation Tricks 2. Inference 2-1. Multi-Scale Infernece (Test Time Augmentation) 2-2. Calculate Metric using Validation Set 3. Final Result 4. Reference      README.md           Semantic Segmentation Tutorial using PyTorch Semantic Segmentation Tutorial using PyTorch. Based on 2020 ECCV VIPriors Challange Start Code, implements semantic segmentation codebase and add some tricks. Editer: Hoseong Lee (hoya012) 0. Experimental Setup 0-1. Prepare Library pip install -r requirements.txt          0-2. Download dataset (MiniCity from CityScapes) We will use MiniCity Dataset from Cityscapes. This dataset is used for 2020 ECCV VIPriors Challenge.  workshop page: https://vipriors.github.io/challenges/ challenge link: https://competitions.codalab.org/competitions/23712 dataset download(google drive)  move dataset into minicity folder.    0-3. Dataset Simple EDA (Exploratory Data Analysis) - Class Distribution, Sample Distribution benchmark class         CityscapesClass('road', 7, 0, 'flat', 1, False, False, (128, 64, 128)),         CityscapesClass('sidewalk', 8, 1, 'flat', 1, False, False, (244, 35, 232)),         CityscapesClass('building', 11, 2, 'construction', 2, False, False, (70, 70, 70)),         CityscapesClass('wall', 12, 3, 'construction', 2, False, False, (102, 102, 156)),         CityscapesClass('fence', 13, 4, 'construction', 2, False, False, (190, 153, 153)),         CityscapesClass('pole', 17, 5, 'object', 3, False, False, (153, 153, 153)),         CityscapesClass('traffic light', 19, 6, 'object', 3, False, False, (250, 170, 30)),         CityscapesClass('traffic sign', 20, 7, 'object', 3, False, False, (220, 220, 0)),         CityscapesClass('vegetation', 21, 8, 'nature', 4, False, False, (107, 142, 35)),         CityscapesClass('terrain', 22, 9, 'nature', 4, False, False, (152, 251, 152)),         CityscapesClass('sky', 23, 10, 'sky', 5, False, False, (70, 130, 180)),         CityscapesClass('person', 24, 11, 'human', 6, True, False, (220, 20, 60)),         CityscapesClass('rider', 25, 12, 'human', 6, True, False, (255, 0, 0)),         CityscapesClass('car', 26, 13, 'vehicle', 7, True, False, (0, 0, 142)),         CityscapesClass('truck', 27, 14, 'vehicle', 7, True, False, (0, 0, 70)),         CityscapesClass('bus', 28, 15, 'vehicle', 7, True, False, (0, 60, 100)),         CityscapesClass('train', 31, 16, 'vehicle', 7, True, False, (0, 80, 100)),         CityscapesClass('motorcycle', 32, 17, 'vehicle', 7, True, False, (0, 0, 230)),         CityscapesClass('bicycle', 33, 18, 'vehicle', 7, True, False, (119, 11, 32)),          from 0 to 18 class, count labeled pixels  deeplab v3 baseline test set result  Dataset has severe Class-Imbalance problem.  IoU of minor class is very low. (wall, fence, bus, train)    classes          IoU      nIoU -------------------------------- road          : 0.963      nan sidewalk      : 0.762      nan building      : 0.856      nan wall          : 0.120      nan fence         : 0.334      nan pole          : 0.488      nan traffic light : 0.563      nan traffic sign  : 0.631      nan vegetation    : 0.884      nan terrain       : 0.538      nan sky           : 0.901      nan person        : 0.732    0.529 rider         : 0.374    0.296 car           : 0.897    0.822 truck         : 0.444    0.218 bus           : 0.244    0.116 train         : 0.033    0.006 motorcycle    : 0.492    0.240 bicycle       : 0.638    0.439 -------------------------------- Score Average : 0.573    0.333 --------------------------------          1. Training Baseline Model   I use DeepLabV3 from torchvision.  ResNet-50 Backbone, ResNet-101 Backbone    I use 4 RTX 2080 Ti GPUs. (11GB x 4)   If you have just 1 GPU or small GPU Memory, please use smaller batch size (<= 8)   python baseline.py --save_path baseline_run_deeplabv3_resnet50 --crop_size 576 1152 --batch_size 8;          python baseline.py --save_path baseline_run_deeplabv3_resnet101 --model DeepLabv3_resnet101 --train_size 512 1024 --test_size 512 1024 --crop_size 384 768 --batch_size 8;          1-1. Loss Functions  I tried 3 loss functions.  Cross-Entropy Loss Class-Weighted Cross Entropy Loss Focal Loss   You can choose loss function using --loss argument.  I recommend default (ce) or Class-Weighted CE loss. Focal loss didn'y work well in my codebase.    # Cross Entropy Loss python baseline.py --save_path baseline_run_deeplabv3_resnet50 --crop_size 576 1152 --batch_size 8;          # Weighted Cross Entropy Loss python baseline.py --save_path baseline_run_deeplabv3_resnet50_wce --crop_size 576 1152 --batch_size 8 --loss weighted_ce;          # Focal Loss python baseline.py --save_path baseline_run_deeplabv3_resnet50_focal --crop_size 576 1152 --batch_size 8 --loss focal --focal_gamma 2.0;          1-2. Normalization Layer   I tried 4 normalization layer.  Batch Normalization (BN) Instance Normalization (IN) Group Normalization (GN) Evolving Normalization (EvoNorm)    You can choose normalization layer using --norm argument.  I recommend BN.    # Batch Normalization python baseline.py --save_path baseline_run_deeplabv3_resnet50 --crop_size 576 1152 --batch_size 8;          # Instance Normalization python baseline.py --save_path baseline_run_deeplabv3_resnet50_instancenorm --crop_size 576 1152 --batch_size 8 --norm instance;          # Group Normalization python baseline.py --save_path baseline_run_deeplabv3_resnet50_groupnorm --crop_size 576 1152 --batch_size 8 --norm group;          # Evolving Normalization python baseline.py --save_path baseline_run_deeplabv3_resnet50_evonorm --crop_size 576 1152 --batch_size 8 --norm evo;          1-3. Additional Augmentation Tricks   Propose 2 data augmentation techniques (CutMix, copyblob)   CutMix Augmentation   Based on Original CutMix, bring idea to Semantic Segmentation.    CopyBlob Augmentation   To tackle Class-Imbalance, use CopyBlob augmentation with visual inductive prior.  Wall must be located on the sidewalk Fence must be located on the sidewalk Bus must be located on the Road Train must be located on the Road      # CutMix Augmentation python baseline.py --save_path baseline_run_deeplabv3_resnet50_cutmix --crop_size 576 1152 --batch_size 8 --cutmix;          # CopyBlob Augmentation python baseline.py --save_path baseline_run_deeplabv3_resnet50_copyblob --crop_size 576 1152 --batch_size 8 --copyblob;          2. Inference  After training, we can evaluate using trained models.  I recommend same value for train_size and test_size.    python baseline.py --save_path baseline_run_deeplabv3_resnet50 --batch_size 4 --predict;          2-1. Multi-Scale Infernece (Test Time Augmentation)  I use [0.5, 0.75, 1.0, 1.25, 1.5, 1.75, 2.0, 2.2] scales for Multi-Scale Inference. Additionaly, use H-Flip.  Must use single batch (batch_size=1)    # Multi-Scale Inference python baseline.py --save_path baseline_run_deeplabv3_resnet50 --batch_size 1 --predict --mst;          2-2. Calculate Metric using Validation Set  We can calculate metric and save results into results.txt.  ex) My final validation set result    python evaluate.py --results baseline_run_deeplabv3_resnet50/results_val --batch_size 1 --predict --mst;          3. Final Result   My final single model result is 0.6069831962012341  Achieve 5th place on the leaderboard. But, didn't submit short-paper, so my score is not official score.   If i use bigger model and bigger backbone, performance will be improved.. maybe.. If i use ensemble various models, performance will be improved! Leader board can be found in Codalab Challenge Page  4. Reference  vipriors-challange-toolkit torchvision deeplab v3 model Focal Loss Class Weighted CE Loss EvoNorm CutMix Augmentation "
78,hfslyc/AdvSemiSeg,420,https://github.com/hfslyc/AdvSemiSeg,"Updated on Apr 21, 2021","Adversarial Learning for Semi-supervised Semantic Segmentation Prerequisite Installation Testing on VOC2012 validation set with pretrained models Example visualization results Training on VOC2012 Changelog      README.md           Adversarial Learning for Semi-supervised Semantic Segmentation This repo is the pytorch implementation of the following paper: Adversarial Learning for Semi-supervised Semantic Segmentation Wei-Chih Hung, Yi-Hsuan Tsai, Yan-Ting Liou, Yen-Yu Lin, and Ming-Hsuan Yang Proceedings of the British Machine Vision Conference (BMVC), 2018. Contact: Wei-Chih Hung (whung8 at ucmerced dot edu)  The code are heavily borrowed from a pytorch DeepLab implementation (Link). The baseline model is DeepLabv2-Resnet101 without multiscale training and CRF post processing, which yields meanIOU 73.6% on the VOC2012 validation set. Please cite our paper if you find it useful for your research. @inproceedings{Hung_semiseg_2018,   author = {W.-C. Hung and Y.-H. Tsai and Y.-T. Liou and Y.-Y. Lin and M.-H. Yang},   booktitle = {Proceedings of the British Machine Vision Conference (BMVC)},   title = {Adversarial Learning for Semi-supervised Semantic Segmentation},   year = {2018} }           Prerequisite  CUDA/CUDNN pytorch >= 0.2 (We only support 0.4 for evaluation. Will migrate the code to 0.4 soon.) python-opencv >=3.4.0 (3.3 will cause extra GPU memory on multithread data loader)  Installation  Clone this repo  git clone https://github.com/hfslyc/AdvSemiSeg.git           Place VOC2012 dataset in AdvSemiSeg/dataset/VOC2012. For training, you will need the augmented labels (Download). The folder structure should be like:  AdvSemiSeg/dataset/VOC2012/JPEGImages                           /SegmentationClassAug           Testing on VOC2012 validation set with pretrained models python evaluate_voc.py --pretrained-model semi0.125 --save-dir results           It will download the pretrained model with 1/8 training data and evaluate on the VOC2012 val set. The colorized images will be saved in results/ and the detailed class IOU will be saved in results/result.txt. The mean IOU should be around 68.8%.  Available --pretrained-model options: semi0.125, semi0.25, semi0.5 , advFull.  Example visualization results  Training on VOC2012 python train.py --snapshot-dir snapshots \                 --partial-data 0.125 \                 --num-steps 20000 \                 --lambda-adv-pred 0.01 \                 --lambda-semi 0.1 --semi-start 5000 --mask-T 0.2           The parameters correspond to those in Table 5 of the paper. To evaluate trained model, execute the following: python evaluate_voc.py --restore-from snapshots/VOC_20000.pth \                        --save-dir results           Changelog  07/24/2018: Update BMVC results "
79,shelhamer/fcn.berkeleyvision.org,3.1k,https://github.com/shelhamer/fcn.berkeleyvision.org,"Updated on Sep 27, 2021","Fully Convolutional Networks for Semantic Segmentation Frequently Asked Questions      README.md           Fully Convolutional Networks for Semantic Segmentation This is the reference implementation of the models and code for the fully convolutional networks (FCNs) in the PAMI FCN and CVPR FCN papers: Fully Convolutional Models for Semantic Segmentation Evan Shelhamer*, Jonathan Long*, Trevor Darrell PAMI 2016 arXiv:1605.06211  Fully Convolutional Models for Semantic Segmentation Jonathan Long*, Evan Shelhamer*, Trevor Darrell CVPR 2015 arXiv:1411.4038           Note that this is a work in progress and the final, reference version is coming soon. Please ask Caffe and FCN usage questions on the caffe-users mailing list. Refer to these slides for a summary of the approach. These models are compatible with BVLC/caffe:master. Compatibility has held since master@8c66fa5 with the merge of PRs #3613 and #3570. The code and models here are available under the same license as Caffe (BSD-2) and the Caffe-bundled models (that is, unrestricted use; see the BVLC model license). PASCAL VOC models: trained online with high momentum for a ~5 point boost in mean intersection-over-union over the original models. These models are trained using extra data from Hariharan et al., but excluding SBD val. FCN-32s is fine-tuned from the ILSVRC-trained VGG-16 model, and the finer strides are then fine-tuned in turn. The ""at-once"" FCN-8s is fine-tuned from VGG-16 all-at-once by scaling the skip connections to better condition optimization.  FCN-32s PASCAL: single stream, 32 pixel prediction stride net, scoring 63.6 mIU on seg11valid FCN-16s PASCAL: two stream, 16 pixel prediction stride net, scoring 65.0 mIU on seg11valid FCN-8s PASCAL: three stream, 8 pixel prediction stride net, scoring 65.5 mIU on seg11valid and 67.2 mIU on seg12test FCN-8s PASCAL at-once: all-at-once, three stream, 8 pixel prediction stride net, scoring 65.4 mIU on seg11valid  FCN-AlexNet PASCAL: AlexNet (CaffeNet) architecture, single stream, 32 pixel prediction stride net, scoring 48.0 mIU on seg11valid. Unlike the FCN-32/16/8s models, this network is trained with gradient accumulation, normalized loss, and standard momentum. (Note: when both FCN-32s/FCN-VGG16 and FCN-AlexNet are trained in this same way FCN-VGG16 is far better; see Table 1 of the paper.) To reproduce the validation scores, use the seg11valid split defined by the paper in footnote 7. Since SBD train and PASCAL VOC 2011 segval intersect, we only evaluate on the non-intersecting set for validation purposes. NYUDv2 models: trained online with high momentum on color, depth, and HHA features (from Gupta et al. https://github.com/s-gupta/rcnn-depth). These models demonstrate FCNs for multi-modal input.  FCN-32s NYUDv2 Color: single stream, 32 pixel prediction stride net on color/BGR input FCN-32s NYUDv2 HHA: single stream, 32 pixel prediction stride net on HHA input FCN-32s NYUDv2 Early Color-Depth: single stream, 32 pixel prediction stride net on early fusion of color and (log) depth for 4-channel input FCN-32s NYUDv2 Late Color-HHA: single stream, 32 pixel prediction stride net by late fusion of FCN-32s NYUDv2 Color and FCN-32s NYUDv2 HHA  SIFT Flow models: trained online with high momentum for joint semantic class and geometric class segmentation. These models demonstrate FCNs for multi-task output.  FCN-32s SIFT Flow: single stream stream, 32 pixel prediction stride net FCN-16s SIFT Flow: two stream, 16 pixel prediction stride net FCN-8s SIFT Flow: three stream, 8 pixel prediction stride net  Note: in this release, the evaluation of the semantic classes is not quite right at the moment due to an issue with missing classes. This will be corrected soon. The evaluation of the geometric classes is fine. PASCAL-Context models: trained online with high momentum on an object and scene labeling of PASCAL VOC.  FCN-32s PASCAL-Context: single stream, 32 pixel prediction stride net FCN-16s PASCAL-Context: two stream, 16 pixel prediction stride net FCN-8s PASCAL-Context: three stream, 8 pixel prediction stride net  Frequently Asked Questions Is learning the interpolation necessary? In our original experiments the interpolation layers were initialized to bilinear kernels and then learned. In follow-up experiments, and this reference implementation, the bilinear kernels are fixed. There is no significant difference in accuracy in our experiments, and fixing these parameters gives a slight speed-up. Note that in our networks there is only one interpolation kernel per output class, and results may differ for higher-dimensional and non-linear interpolation, for which learning may help further. Why pad the input?: The 100 pixel input padding guarantees that the network output can be aligned to the input for any input size in the given datasets, for instance PASCAL VOC. The alignment is handled automatically by net specification and the crop layer. It is possible, though less convenient, to calculate the exact offsets necessary and do away with this amount of padding. Why are all the outputs/gradients/parameters zero?: This is almost universally due to not initializing the weights as needed. To reproduce our FCN training, or train your own FCNs, it is crucial to transplant the weights from the corresponding ILSVRC net such as VGG16. The included surgery.transplant() method can help with this. What about FCN-GoogLeNet?: a reference FCN-GoogLeNet for PASCAL VOC is coming soon. "
80,1044197988/Semantic-segmentation-of-remote-sensing-images,171,https://github.com/1044197988/Semantic-segmentation-of-remote-sensing-images,"Updated on Apr 28, 2020","Semantic-segmentation-of-remote-sensing-image   基于深度学习关于遥感影像的语义分割 Unet、FPN模型及嵌入相关模块后的结果： 准确率对比： Iou对比： Loss对比： 参数对比： 不同全色波段融合方法的结果： Unet++模型结果： 混淆矩阵百分比： 混淆矩阵像素数： 相关统计： 结果图的拼接痕迹问题： 代码运行： CRF后处理: GDAL 提示      README.md           Semantic-segmentation-of-remote-sensing-image   基于深度学习关于遥感影像的语义分割 首先看一下数据集，包含原始影像与标签，实际的分辨率很大，这个只是缩略图。 影像数据是Landsat8卫星的，用五四三波段进行合成，并利用GS方法进行全色第八波段的融合。(Envi软件处理) 标签是通过矢量图层以ArcGIS软件来处理生成的。 此代码库可在Tensorflow下keras环境运行，在Tensorflow1.12及Tensorflow2.0测试运行，代码更改后，更适合于Tensorflow2.0    Unet、FPN模型及嵌入相关模块后的结果： 通过fit_generator运行，所以生成器需要自己编写，FCN8S与Segnet均为序列式模型与Keras的Model类有些不同，可以调用更多的方法。 展示一下Unet模型及FPN模型在此数据集上的结果，结果比Segnet与FCN好太多，所以就在这里不对比Segnet与FCN了。 其中Unet未经过预训练，其他集成的模块都经过了Imagenet预训练，并且测试都是通过划分数据来进行测试的，train75%，val25%。 准确率对比：  Iou对比：  Loss对比：  参数对比： 在相同的硬件条件下运行：  不同全色波段融合方法的结果： 在目录里可以看到，这个影响不大。 Unet++模型结果： 混淆矩阵百分比：  混淆矩阵像素数：  相关统计： 当中1，2，3，4，5，15分别代表预测后的像素值，每个像素值代表一类    结果图的拼接痕迹问题： 我认为这个很大概率上跟模型的拟合能力、泛化能力有关，所以这个问题不用考虑，只要能够训练到所有的像素就可以了。 Unet等相关模型预测的图，“拼接痕迹”很小。 代码运行： 弄好数据集后，需要切割，切割的话这个可以参考一下生成数据并增强.py,更改相关参数即可。 然后通过里面的Segnet的训练程序启动即可，需要修改参数。 这个Segnet、FCN8S是用序列式类来实现的模型，所以预测的话是跟Model类有一点不相同，就是可以调用predict_classes的方法。 Model类文件里提供了使用Model类的模型的预测方法。 以上所有代码只是提供参考,训练其他数据的话很多参数需要自己修改，甚至生成器也需要更改，如果想对数据类别进行加权或使用样本权重，主要记得在生成器中做修改就可以了。 CRF后处理: 模型预测完毕后，可以使用CRF进行后处理，CRF.py文件提供了相关代码参考，但未必保证结果可靠。 GDAL 如果自己做的图包含多个波段(往往大于4个)，Opencv或PIL就不太顶用了，这时候GDAL就派上用场了 例如我有一个十波段图像，用此函数读取后为numpy数组类,shape为[h,w,10] from osgeo import gdal import numpy as np  def load_img(path):     dataset = gdal.Open(path)     im_width = dataset.RasterXSize     im_height = dataset.RasterYSize     im_data = dataset.ReadAsArray(0,0,im_width,im_height)     im_data = im_data.transpose((1,2,0)) #此步保证矩阵为channel_last模式     return im_data          提示 我的该贡献库中，提供了一些分割的模型及相关指标与损失的代码 1044197988-TF.Keras-Commonly-used-models "
81,oyam/Semantic-Segmentation-using-Adversarial-Networks,96,https://github.com/oyam/Semantic-Segmentation-using-Adversarial-Networks,"Updated on Jul 25, 2017",Semantic Segmentation using Adversarial Networks Requirements Differences Caution      README.md           Semantic Segmentation using Adversarial Networks Requirements Chainer (1.23.0) Differences Use of FCN-VGG16 instead of Dilated8 as Segmentor. Caution I have not tuned Discriminator's learning rate yet. 
82,yaq007/Autofocus-Layer,179,https://github.com/yaq007/Autofocus-Layer,"Updated on May 13, 2019","Autofocus Layer for Semantic Segmentation Introduction Citation Data Supported Models Performance Environment Installation Quick Start Training Evaluation Testing Case 1 Case 2 Contact      README.md           Autofocus Layer for Semantic Segmentation Introduction This is a PyTorch implementation of the autofocus convolutional layer proposed for semantic segmentation with the objective of enhancing the capabilities of neural networks for multi-scale processing. Autofocus layers adaptively change the size of the effective receptive field based on the processed context to generate more powerful features. The proposed autofocus layer can be easily integrated into existing networks to improve a model's representational power. Here we apply the autofocus convolutional layer to deep neural networks for 3D semantic segmentation. We run experiments on the Brain Tumor Image Segmentation dataset (BRATS2015) as an example to show how the models work. In addition, we also implement a series of deep learning based models used for 3D Semantic Segmentation. The details of all the models implemented here can be found in our paper: Autofocus Layer for Semantic Segmentation.  Figure 1. An autofocus convolutional layer with four candidate dilation rates. (a) The attention model. (b) A weighted summation of activations from parallel dilated convolutions. (c) An example of attention maps for a small (r^1) and large (r^2) dilation rate. The first row is the input and the segmentation result of AFN6. Citation If you find the code or the models implemented here are useful, please cite our paper: Autofocus Layer for Semantic Segmentation. Y. Qin, K. Kamnitsas, S. Ancha, J. Nanavati, G. Cottrell, A. Criminisi, A. Nori, MICCAI 2018. Data You can download the full dataset with training and testing images from https://www.smir.ch/BRATS/Start2015. To run all the models here, you need to do a series of data pre-processing to the input images.  Provide a mask including the Region of Interest (RoI) as one of the input image. For example, in the BRATS dataset, the region outside the brain should be masked out with the provided mask. The intensity of the data within the RoI must be normalized to be zero-mean, unit-variance. For the BRATS dataset, each image must be normalized independently other than doing the normalization with the mean and variance of the whole training dataset. Make sure the ground-truth labels for training and testing represent the background with zero. For example, we have four different classes in the BRATS dataset, then the number of classes in this dataset will be 5, including the background ([--num_classes 5]) and number zero will be used to represent the background. When you use the training code for your own data, please change the data path correspondingly.  We provide the example codes for data preprocessing, including converting the data format, generating the masks and normalizing the input image. The corresponding text file is also provided to show the directory where the image are saved. You can create your own text file to save the image data path and change the corresponding code in the python scripts. The data normalization code is mainly derived from DeepMedic. A small subset of the BRATS dataset (after all the above data pre-processing) is provided here to run the preset examples. Supported Models Please refer ""Autofocus Layer for Semantic Segmentation"" for the details of all the supported models.  Basic Model: half pathway of DeepMedic with the last 6 layers with dilation rates equal 2. ASPP-c: adding an ASPP module on top of Basic model (parallel features are merged via concatenation). ASPP-s: adding an ASPP module on top of Basic model (parallel features are merged via summation). AFN1-6: with the last 1-6 dilated convolutional layers replaced by our proposed aufofocus convolutional layer.  Performance The performance reported here is an average over three runs on the 54 images from BRATS 2015 dataset. All the trained models can be downloaded here.  Table 1: Dice scores shown in format mean (standard deviation). Environment The code is developed under the follwing configurations.  1-3 GPUs with at least 12G GPU memories. You can choose the number of GPUs used via [--num_gpus NUM_GPUS]. PyTorch 0.3.0 or higher is required to run the codes. Nibabel is used here for loading the NIFTI images. SimpleITK is used for saving the output into images.  Installation git clone https://github.com/yaq007/Autofocus-Layer.git conda install pytorch torchvision -c pytorch pip install nibabel pip install SimpleITK          Quick Start First, you need to download the provided subset of BRATS dataset and all the trained models. Please run chmod +x download_sub_dataset.sh ./download_sub_daset.sh  chmod +x download_models.sh ./download_models.sh          Then you can run the following script to choose a model and do the testing. Here we use AFN1 as an example. python test.py --num_gpus 1 --id AFN1 --test_epoch 390          You can change the number of used GPUs via [--num_gpus NUM_GPUS] and choose the tested model that you want via [--id MODEL_ID]. Make sure the test epoch is included in the downloaded directory ""saved_models"". You can check all the input arguments via python test.py -h. Training In the provided subset of dataset, we also provided 20 example images for training. You can start training via: python train.py --num_gpus NUM_GPUS --id MODEL_ID          For the models like ""Basic"", you may only need one gpu to run the experiments. For the models like ""AFN6"", you may need to increase the number of GPUs to be 2 or 3. This depends on the GPU memory that you are using. Please check all the input arguments via python train.py -h. Evaluation You can evaluate a series of models saved after different epochs for one network via. python val.py --num_gpus NUM_GPUS --id MODEL_ID          Please make sure that you have already provided a validation list in order to load the validation images. You can specify the steps of epochs that you want to evaluate. Please check all the input arguments via python val.py -h. Testing Case 1 If you have labels for test data and want to see the accuracy (e.g., Dice score for BRATS dataset), you can use the following two testing codes:  test.py The input of the network are small image segments as in the training stage. test_full.py The input of the network is a full image rather than a smaller image segment.  There are small differences of these two different testing methods due to the padding in the convolutions. For the performance that we report above, we use the test.py to get all the results. To test, you can simply run python test.py/test_full.py --num_gpus NUM_GPUS --id MODEL_ID --test_epoch NUM_OF_TEST_EPOCH          You can increase the number of GPUs to speed up the evaluation process. You can also use --visualize action to save the prediction as an output image. Case 2 If you do not have ground truth for test data, you should use test_online.py to do the testing and save the output. For the BRATS dataset, you can simply run the following script to generate the predicted images and submit them to the online evaluation server. python test_online.py --num_gpus NUM_GPUS --id MODEL_ID --test_epoch NUM_OF_TEST_EPOCH --visualize          Note! If you want to run test_online.py on the provided sample testing images, you need to change the directory of data when loading images. Contact If you have any problems when using our codes or models, please feel free to contact me via e-mail: yaq007@eng.ucsd.edu. "
83,VisualComputingInstitute/3d-semantic-segmentation,89,https://github.com/VisualComputingInstitute/3d-semantic-segmentation,"Updated on Oct 19, 2018","Exploring Spatial Context for 3D Semantic Segmentation of Point Clouds Introduction Citation Installation Usage Reproducing the scores of our paper for stanford indoor 3d Downloading the data set Producing numpy files from the original dataset Downsampling for training Training configuration scripts Evaluating on full scale point clouds VKitti instructions Trained models for downloading License      README.md           Exploring Spatial Context for 3D Semantic Segmentation of Point Clouds Created by Francis Engelmann, Theodora Kontogianni, Alexander Hermans, Jonas Schult and Bastian Leibe from RWTH Aachen University.  Introduction This work is based on our paper Exploring Spatial Context for 3D Semantic Segmentation of Point Clouds, which appeared at the IEEE International Conference on Computer Vision (ICCV) 2017, 3DRMS Workshop. You can also check our project page for further details. Deep learning approaches have made tremendous progress in the field of semantic segmentation over the past few years. However, most current approaches operate in the 2D image space. Direct semantic segmentation of unstructured 3D point clouds is still an open research problem. The recently proposed PointNet architecture presents an interesting step ahead in that it can operate on unstructured point clouds, achieving decent segmentation results. However, it subdivides the input points into a grid of blocks and processes each such block individually. In this paper, we investigate the question how such an architecture can be extended to incorporate larger-scale spatial context. We build upon PointNet and propose two extensions that enlarge the receptive field over the 3D scene. We evaluate the proposed strategies on challenging indoor and outdoor datasets and show improved results in both scenarios. In this repository, we release code for training and testing various pointcloud semantic segmentation networks on arbitrary datasets. Citation If you find our work useful in your research, please consider citing: @inproceedings{3dsemseg_ICCVW17,   author    = {Francis Engelmann and                Theodora Kontogianni and                Alexander Hermans and                Bastian Leibe},   title     = {Exploring Spatial Context for 3D Semantic Segmentation of Point Clouds},   booktitle = {{IEEE} International Conference on Computer Vision, 3DRMS Workshop, {ICCV}},   year      = {2017} }           Installation Install TensorFlow. The code has been tested with Python 3.6 and TensorFlow 1.8. Usage In order to get more representative blocks, it is encouraged to uniformly downsample the original point clouds. This is done via the following script: python tools/downsample.py --data_dir path/to/dataset --cell_size 0.03           This statement will produce pointclouds where each point will be representative for its 3cm x 3cm x 3cm neighborhood. To train/test a model for semantic segmentation on pointclouds, you need to run: python run.py --config path/to/config/file.yaml           Detailed instruction of the structure for the yaml config file can be found in the wiki. Additionally, some example configuration files are given in the folder experiments. Note that the final evaluation is done on the full sized point clouds using k-nn interpolation. Reproducing the scores of our paper for stanford indoor 3d Downloading the data set First of all, Stanford Large-Scale 3D Indoor Spaces Dataset has to be downloaded. Follow the instructions here. The aligned version 1.2 is used for our results. Producing numpy files from the original dataset Our pipeline cannot handle the original file type of s3dis. So, we need to convert it to npy files. Note that Area_5/hallway_6 has to be fixed manually due to format inconsistencies. The following script has to be run from the tools directory: python prepare_s3dis.py --input_dir path/to/dataset --output_dir path/to/output           Downsampling for training Before training, we downsampled the pointclouds. python tools/downsample.py --data_dir path/to/dataset --cell_size 0.03           Training configuration scripts Configuration files for all experiments are located in experiments/iccvw_paper_2017/*. For example, they can be launched as follows: python run.py --config experiments/iccvw_paper_2017/s3dis_mscu/s3dis_mscu_area_1.yaml           The above script will run our multi scale consolidation unit network on stanford indoor 3d with test area 1. Evaluating on full scale point clouds Reported scores on the dataset are based on the full scale pointclouds. In order to do so, we need to load the trained model and set the TEST flag. Replace modus: TRAIN_VAL with     modus: TEST     model_path: 'path/to/trained/model/model_ckpts'          which is located in the log directory specified for training. VKitti instructions  Coming soon...  Trained models for downloading  Coming soon...  License Our code is released under MIT License (see LICENSE file for details). "
84,Megvii-BaseDetection/DynamicRouting,358,https://github.com/Megvii-BaseDetection/DynamicRouting,"Updated on Jun 9, 2021","DynamicRouting Requirement Installation Usage Dataset Pretrained Model Training Evaluation Performance Cityscapes val set To do Acknowledgement Citation      README.md           DynamicRouting This project provides an implementation for ""Learning Dynamic Routing for Semantic Segmentation"" (CVPR2020 Oral) on PyTorch. For the reason that experiments in the paper were conducted using internal framework, this project reimplements them on dl_lib and reports detailed comparisons below. Some parts of code in dl_lib are based on detectron2.  Requirement  Python >= 3.6  python3 --version   PyTorch >= 1.3  pip3 install torch torchvision   OpenCV  pip3 install opencv-python   GCC >= 4.9  gcc --version    Installation Make sure that your get at least one gpu when compiling. Run:  git clone https://github.com/yanwei-li/DynamicRouting.git cd DynamicRouting sudo python3 setup.py build develop  Usage Dataset We use Cityscapes dataset for training and validation. Please refer to datasets/README.md or dataset structure in detectron2 for more details.  Cityscapes Download  Pretrained Model We give ImageNet pretained models:  Layer16-Fix GoogleDrive Layer33-Fix GoogleDrive  Training For example, if you want to train Dynamic Network with Layer16 backbone:  Train from scratch cd playground/Dynamic/Seg.Layer16 dl_train --num-gpus 4           Use ImageNet pretrain cd playground/Dynamic/Seg.Layer16.ImageNet dl_train --num-gpus 4 MODEL.WEIGHTS /path/to/your/save_dir/ckpt.pth            NOTE: Please set FIX_SIZE_FOR_FLOPS to [768,768] and [1024,2048] for training and evaluation, respectively. Evaluation You can evaluate the trained or downloaded model:  Evaluate the trained model dl_test --num-gpus 8           Evaluate the downloaded model: dl_test --num-gpus 8 MODEL.WEIGHTS /path/to/your/save_dir/ckpt.pth            NOTE: If your machine does not support such setting, please change settings in config.py to a suitable value. Performance Cityscapes val set Without ImageNet Pretrain:    Methods Backbone Iter/K mIoU (paper) GFLOPs (paper) mIoU (ours) GFLOPs (ours) Model     Dynamic-A Layer16 186 72.8 44.9 73.9 52.5 GoogleDrive   Dynamic-B Layer16 186 73.8 58.7 74.3 58.9 GoogleDrive   Dynamic-C Layer16 186 74.6 66.6 74.8 59.8 GoogleDrive   Dynamic-Raw Layer16 186 76.1 119.5 76.7 114.9 GoogleDrive   Dynamic-Raw Layer16 558 78.3 113.3 78.1 114.2 GoogleDrive    With ImageNet Pretrain:    Methods Backbone Iter/K mIoU (paper) GFLOPs (paper) mIoU (ours) GFLOPs (ours) Model     Dynamic-Raw Layer16 186 78.6 119.4 78.8 117.8 GoogleDrive   Dynamic-Raw Layer33 186 79.2 242.3 79.4 243.1 GoogleDrive    To do   Faster inference speed  Support more vision tasks   Object detection  Instance segmentation  Panoptic segmentation    Acknowledgement  Detectron2 DARTS  Citation Consider cite the Dynamic Routing in your publications if it helps your research. @inproceedings{li2020learning,     title = {Learning Dynamic Routing for Semantic Segmentation},     author = {Yanwei Li, Lin Song, Yukang Chen, Zeming Li, Xiangyu Zhang, Xingang Wang, Jian Sun},     booktitle = {IEEE Conference on Computer Vision and Pattern Recognition},     year = {2020} }           Consider cite this project in your publications if it helps your research. @misc{DynamicRouting,     author = {Yanwei Li},     title = {DynamicRouting},     howpublished = {\url{https://github.com/yanwei-li/DynamicRouting}},     year ={2020} } "
85,valeoai/ZS3,147,https://github.com/valeoai/ZS3,"Updated on Feb 17, 2020","Zero-Shot Semantic Segmentation Paper Abstract Code Pre-requisites Installation Datasets Pascal-VOC 2012 Pascal-Context Training Pascal-VOC Pascal-Context Testing Acknowledgements License      README.md           Zero-Shot Semantic Segmentation Paper  Zero-Shot Semantic Segmentation Maxime Bucher, Tuan-Hung Vu , Matthieu Cord, Patrick Pérez valeo.ai, France Neural Information Processing Systems (NeurIPS) 2019 If you find this code useful for your research, please cite our paper: @inproceedings{bucher2019zero,   title={Zero-Shot Semantic Segmentation},   author={Bucher, Maxime and Vu, Tuan-Hung and Cord, Mathieu and P{\'e}rez, Patrick},   booktitle={NeurIPS},   year={2019} }           Abstract Semantic segmentation models are limited in their ability to scale to large numbers of object classes. In this paper, we introduce the new task of zero-shot semantic segmentation: learning pixel-wise classifiers for never-seen object categories with zero training examples. To this end, we present a novel architecture, ZS3Net, combining a deep visual  segmentation model with an approach to generate visual representations from semantic word embeddings. By this way, ZS3Net addresses pixel classification tasks where both seen and unseen categories are faced at test time (so called ""generalized"" zero-shot classification). Performance is further improved by a self-training step that relies on automatic pseudo-labeling of pixels from unseen classes. On the two standard segmentation datasets, Pascal-VOC and Pascal-Context, we propose zero-shot benchmarks and set competitive baselines. For complex scenes as ones in the Pascal-Context dataset, we extend our approach by using a graph-context encoding to fully leverage spatial context priors coming from class-wise segmentation maps. Code Pre-requisites  Python 3.6 Pytorch >= 1.0 or higher CUDA 9.0 or higher  Installation  Clone the repo:  $ git clone https://github.com/valeoai/ZS3           Install this repository and the dependencies using pip:  $ pip install -e ZS3          With this, you can edit the ZS3 code on the fly and import function and classes of ZS3 in other project as well.  Optional. To uninstall this package, run:  $ pip uninstall ZS3          You can take a look at the Dockerfile if you are uncertain about steps to install this project. Datasets Pascal-VOC 2012   Pascal-VOC 2012: Please follow the instructions here to download images and semantic segmentation annotations.   Semantic Boundaries Dataset: Please follow the instructions here to download images and semantic segmentation annotations. Use this train set, which excludes overlap with Pascal-VOC validation set.   The Pascal-VOC and SBD datasets directory should have this structure: ZS3/data/VOC2012/    % Pascal VOC and SBD datasets root ZS3/data/VOC2012/ImageSets/Segmentation/     % Pascal VOC splits ZS3/data/VOC2012/JPEGImages/     % Pascal VOC images ZS3/data/VOC2012/SegmentationClass/      % Pascal VOC segmentation maps ZS3/data/VOC2012/benchmark_RELEASE/dataset/img      % SBD images ZS3/data/VOC2012/benchmark_RELEASE/dataset/cls      % SBD segmentation maps ZS3/data/VOC2012/benchmark_RELEASE/dataset/train_noval.txt       % SBD train set          Pascal-Context   Pascal-VOC 2010: Please follow the instructions here to download images.   Pascal-Context: Please follow the instructions here to download segmentation annotations.   The Pascal-Context dataset directory should have this structure: ZS3/data/context/    % Pascal context dataset root ZS3/data/context/train.txt     % Pascal context train split ZS3/data/context/val.txt     % Pascal context val split ZS3/data/context/full_annotations/trainval/     % Pascal context segmentation maps ZS3/data/context/full_annotations/labels.txt     % Pascal context 459 classes ZS3/data/context/classes-59.txt     % Pascal context 59 classes ZS3/data/context/VOCdevkit/VOC2010/JPEGImages     % Pascal VOC images          Training Pascal-VOC Follow steps below to train your model:  Train deeplabv3+ using Pascal VOC dataset and ResNet as backbone, pretrained on imagenet (weights here):  python train_pascal.py            Main options  imagenet_pretrained_path: Path to ImageNet pretrained weights. exp_path: Path to saved logs and weights folder. checkname: Name of the saved logs and weights folder. unseen_classes_idx: List of idx of unseen classes.    Trained deeplabv3+ weights  2 unseen classes 4 unseen classes 6 unseen classes 8 unseen classes 10 unseen classes     Train GMMN and finetune the last classification layer of the trained deeplabv3+ model:  python train_pascal_GMMN.py            Main options  imagenet_pretrained_path: Path to ImageNet pretrained weights. resume: Path to deeplabv3+ weights. exp_path: Path to saved logs and weights folder. checkname: Name of the saved logs and weights folder. seen_classes_idx_metric: List of idx of seen classes. unseen_classes_idx_metric: List of idx of unseen classes.    Final deeplabv3+ and GMMN weights  2 unseen classes 4 unseen classes 6 unseen classes 8 unseen classes 10 unseen classes    Pascal-Context Follow steps below to train your model:  Train deeplabv3+ using Pascal Context dataset and ResNet as backbone, pretrained on imagenet (weights here):  python train_context.py            Main options  imagenet_pretrained_path: Path to ImageNet pretrained weights. exp_path: Path to saved logs and weights folder. checkname: Name of the saved logs and weights folder. unseen_classes_idx: List of idx of unseen classes.    Trained deeplabv3+ weights  2 unseen classes 4 unseen classes 6 unseen classes 8 unseen classes 10 unseen classes     Train GMMN and finetune the last classification layer of the trained deeplabv3+ model:  python train_context_GMMN.py            Main options  imagenet_pretrained_path: Path to ImageNet pretrained weights. resume: Path to deeplabv3+ weights. exp_path: Path to saved logs and weights folder. checkname: Name of the saved logs and weights folder. seen_classes_idx_metric: List of idx of seen classes. unseen_classes_idx_metric: List of idx of unseen classes.    Final deeplabv3+ and GMMN weights  2 unseen classes 4 unseen classes 6 unseen classes 8 unseen classes 10 unseen classes    (2 bis). Train GMMN with graph context and finetune the last classification layer of the trained deeplabv3+ model: python train_context_GMMN_GCNcontext.py            Main options  imagenet_pretrained_path: Path to ImageNet pretrained weights. resume: Path to deeplabv3+ weights. exp_path: Path to saved logs and weights folder. checkname: Name of the saved logs and weights folder. seen_classes_idx_metric: List of idx of seen classes. unseen_classes_idx_metric: List of idx of unseen classes.    Final deeplabv3+ and GMMN with graph context weights  2 unseen classes 4 unseen classes 6 unseen classes 8 unseen classes 10 unseen classes    Testing python eval_pascal.py          python eval_context.py           Main options  resume: Path to deeplabv3+ and GMMN weights. seen_classes_idx_metric: List of idx of seen classes. unseen_classes_idx_metric: List of idx of unseen classes.    Acknowledgements  This codebase is heavily borrowed from pytorch-deeplab-xception. Special thanks for @gabrieldemarmiesse for his work in enhancing, cleaning and formatting this repository for release.  License ZS3Net is released under the Apache 2.0 license. "
86,NikolasEnt/Road-Semantic-Segmentation,73,https://github.com/NikolasEnt/Road-Semantic-Segmentation,"Updated on Dec 28, 2017","Semantic Segmentation Udacity Self-Driving Car Engineer Nanodegree. Project: Vehicle Detection and Tracking Results Content of this repo Architecture Setup Augmentation Cityscapes Setup Frameworks and Packages Dataset Start Implement Run Submission How to write a README      README.md           Semantic Segmentation Udacity Self-Driving Car Engineer Nanodegree. Project: Vehicle Detection and Tracking This Project is the twelfth task of the Udacity Self-Driving Car Nanodegree program. The main goal of the project is to train an artificial neural network for semantic segmentation of a video from a front-facing camera on a car in order to mark road pixels using Tensorflow. Results KITTI Road segmentation (main task of the project):  Cityscapes multiclass segmentation (optional task):  Content of this repo  Segmentation.ipynb - Jupyter notebook with the main code for the project helper.py - python program for images pre- and  post- processing. runs - directory with processed images cityscapes.ipynb - Jupyter notebook with some visualization and preprocessing of the Cityscape dataset. Please, see the notebook for correct dataset directories placing. Segmentation_cityscapes.ipynb - Jupyter notebook with the main code for the Cityscape dataset. helper_cityscapes.py - python program for images pre- and  post- processing for the Cityscape dataset.  Note: The repository does not contain any training images. You have to download the image datasetsplace them in appropriate directories on your own. Architecture A Fully Convolutional Network (FCN-8 Architecture developed at Berkeley, see paper ) was applied for the project. It uses VGG16 pretrained on ImageNet as an encoder. Decoder is used to upsample features, extracted by the VGG16 model, to the original image size. The decoder is based on transposed convolution layers. The goal is to assign each pixel of the input image to the appropriate class (road, backgroung, etc). So, it is a classification problem, that is why, cross entropy loss was applied. Setup Hyperparameters were chosen by the try-and-error process. Adam optimizer was used as a well-established optimizer. Weights were initialized by a random normal initializer. Some benefits of L2 weights regularization were observed, therefore, it was applied in order to reduce grainy edges of masks. Augmentation Resized input images were also treated by random contrast and brightness augmentation (as linear function of the input image). It helps to produce reasonable predictions in difficult light conditions. def bc_img(img, s = 1.0, m = 0.0):     img = img.astype(np.int)     img = img * s + m     img[img > 255] = 255     img[img < 0] = 0     img = img.astype(np.uint8)     return img             Deep shadows and contrast variations are not a problem because of rich augmentation on the training stage. Cityscapes Two classes (roads and cars) were chosen from the Cityscapes dataset for the optional task. The classes are unbalanced (roads are prevalent), so, a weighted loss function was involved (see Segmentation_cityscapes.ipynb for details). Interestingly, RMSProp optimizer performed better for the imageset. Unfortunately, accord to the Cityscapes dataset licence I can not publish all produced images, however, there are some of them.  It correctly do not label a cyclist as a car, but mark small partly occluded cars.  It does not have problems with recognizing a cobbled street as a road.  And the ANN is able to mark cars in different projections. References:  KITTI dataset Cityscapes dataset FCN ANN  _____________________ Udacity Readme.md ____________________ Setup Frameworks and Packages Make sure you have the following is installed:  Python 3 TensorFlow NumPy SciPy  Dataset Download the Kitti Road dataset from here.  Extract the dataset in the data folder.  This will create the folder data_road with all the training a test images. Start Implement Implement the code in the main.py module indicated by the ""TODO"" comments. The comments indicated with ""OPTIONAL"" tag are not required to complete. Run Run the following command to run the project: python main.py           Note If running this in Jupyter Notebook system messages, such as those regarding test status, may appear in the terminal rather than the notebook. Submission  Ensure you've passed all the unit tests. Ensure you pass all points on the rubric. Submit the following in a zip file.   helper.py main.py project_tests.py Newest inference images from runs folder  How to write a README A well written README file can enhance your project and portfolio.  Develop your abilities to create professional README files by completing this free course. "
87,ArtyZe/yolo_segmentation,160,https://github.com/ArtyZe/yolo_segmentation,"Updated on Jan 6, 2021","yolo_segmentation [The Commond to Run My Project] [Pretrain weights file and cfg file] [How to Train with Your Own Dataset ?] Colorful Original Image: Lable Image: Steps to train you own dataset:      README.md           yolo_segmentation   The code is to get segmentation image by darknet In the process of my project, I have referenced nithi89/unet_darknet in some points and nithilan has give me many important advices, thanks to him, and if you have interest you can visit his homepage. This is my third version, I added dilation convolutional, and now it has not so perfect result, but i think it's good enough for me. I will continue to update afterwards, please stay tuned. [The Commond to Run My Project] Compile: make -j8           Train: ./darknet segmenter train cfg/maskyolo.data cfg/instance_segment.cfg [pretrain weights file I gave to you]           Test: ./darknet segmenter test cfg/maskyolo.data cfg/instance_segment.cfg [weights file] [image path]           Merge two images: python Merge.py           And you will get the mask image named final.png Test image: Merge them together image:  Output image:(for orig)   Merge them together image:(not so good, 1. more epochs; 2. deeper or more complex backbone)  [Pretrain weights file and cfg file]  https://www.dropbox.com/sh/9wrevnyzwfv8hg7/AAA1MJElri9aROsjaPTxO5KCa?dl=0 https://pan.baidu.com/s/15gcrXGzb-fY2vGdl4KlLqg password: bk01  [How to Train with Your Own Dataset ?] The Way is so easy, you only need three files: original colorful image;  label image(pixel value is 0, 1, 2, 3 if you have 3 classes + background);  train.list.           For example with cityscape dataset: Colorful Original Image:  Lable Image: I only have one class so the label image, as 0 is background and others are multi classes. If you have 2 classes, the label image pixel value should be 012 and so on:  Steps to train you own dataset:   1. prepare train images and label images like above images      I have added below function call in my code according to my pictures, you have to change it according to your image name      #######################################################     find_replace(labelpath, ""_leftImg8bit.png"", ""_gtFine_labelIds.png"", labelpath);     #######################################################    2. put label images and original images together    3. generate the train.list file just like:      /home/user/Desktop/YOLO_train/leftImg8bit/train/aachen_resize/jena_000012_000019_leftImg8bit.png    4. start train      ./darknet segmenter train [data_file path] cfg/segment.cfg [pretrain weights file I gave to you]           If you want to see my Result Video, I have put it in: https://pan.baidu.com/s/1uJwFYLHEQ9DGFZ8RkGuagg, and the password is: ic3q If you want to get how to change the code, see https://github.com/ArtyZe/yolo_segmentation/blob/master/Train_Details.md What I did to change Yolo for image segmentation, I have written a blog in: https://blog.csdn.net/Artyze/article/details/82721147 After I will do some work in semantic segmentation with yolo. If you want to do something with Yolo with me, contact me with E-mail: Gaoyang917528@163.com. "
88,AdivarekarBhumit/ID-Card-Segmentation,76,https://github.com/AdivarekarBhumit/ID-Card-Segmentation,"Updated on Aug 26, 2020","ID-Card-Segmentation U-Net Architecture Our Result's Requirements Dataset Train Model Test Model Benchmarks IoU Loss Binary Accuracy Val IoU Loss Val Binary Loss Citation      README.md           ID-Card-Segmentation Segmentation of ID Cards using U-Net U-Net Architecture  Our Result's   Requirements  Tensorflow-GPU 1.12 Keras 2.1 OpenCV 3.4.5 Numpy 1.16  Dataset  Download Dataset  python dataset/download_dataset.py            Combine To single npy file (First Download the dataset)  python dataset/stack_npy.py           Train Model  Start Training  python model/train.py           Training data in 100 epochs. This data was trained on google colab Test Model python test_model.py           Benchmarks  IoU Loss  Binary Accuracy    Val IoU Loss  Val Binary Loss   Citation Please cite this paper, if using midv dataset, link for dataset provided in paper @article{DBLP:journals/corr/abs-1807-05786,   author    = {Vladimir V. Arlazarov and                Konstantin Bulatov and                Timofey S. Chernov and                Vladimir L. Arlazarov},   title     = {{MIDV-500:} {A} Dataset for Identity Documents Analysis and Recognition                on Mobile Devices in Video Stream},   journal   = {CoRR},   volume    = {abs/1807.05786},   year      = {2018},   url       = {http://arxiv.org/abs/1807.05786},   archivePrefix = {arXiv},   eprint    = {1807.05786},   timestamp = {Mon, 13 Aug 2018 16:46:35 +0200},   biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1807-05786},   bibsource = {dblp computer science bibliography, https://dblp.org} } "
89,Golbstein/Keras-segmentation-deeplab-v3.1,171,https://github.com/Golbstein/Keras-segmentation-deeplab-v3.1,"Updated on Mar 7, 2020","Keras DeepLab V3.1+ Features New Features That Are Not Included In The Paper Results Dependencies      README.md           Keras DeepLab V3.1+ DeepLab V3+ for Semantic Image Segmentation With Subpixel Upsampling Layer Implementation in Keras Added Tensorflow 2 support - Nov 2019 DeepLab is a state-of-art deep learning model for semantic image segmentation. Original DeepLabV3 can be reviewed here: DeepLab Paper with the original model implementation. Features  Conditional Random Fields (CRF) implementation as post-processing step to aquire better contour that is correlated with nearby pixels and their color. See here: Fully-Connected CRF Custom image generator for semantic segmentation with large augmentation capabilities.  New Features That Are Not Included In The Paper  Keras Subpixel (Pixel-Shuffle layer) from: Keras-Subpixel for efficient upsampling and more accurate segmentation ICNR Initializer for subpixel layer (removing checkerboard artifact) ICNR Comparisson of the original Deeplab model with my Deeplab+subpixel+CRF Fast training - transfer learning from paper's proposed model to a better model within ~1 hour with 1-1080Ti GPU Jaccard (mIOU) monitoring during training process for multi-class segmentation tasks. Adaptive pixel weights.  Results I've compared the segmentation visual results and the IOU score between paper's model and mine, as well as the outcome of applying CRF as a post-processing step. Below depicted few examples:     And the IOU score amid the classes:  I didn't receive a significant improvement of the IOU scores, perhaps due to low number of epochs. However I believe this method can eventually outperform the original model for a bigger dataset and more epochs. Dependencies  Python 3.6 Keras>2.2.x pydensecrf tensorflow > 1.11 "
